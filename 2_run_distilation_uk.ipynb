{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d2e02-792c-44ac-8d8c-b2e63f91292d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager,store).\n",
      "Your token has been saved to C:\\Users\\Zhenya\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfFolder\n",
    "login(token='', add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765d97b6-fbe9-4855-a0ac-835a21c23c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    IterableDataset,\n",
    "    IterableDatasetDict,\n",
    "    concatenate_datasets,\n",
    "    interleave_datasets,\n",
    "    load_dataset,\n",
    ")\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AddedToken,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    WhisperTokenizerFast,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer, EnglishTextNormalizer\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.34.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=2.14.6\", \"To fix: `pip install --upgrade datasets`\")\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926b7b8b-ed85-49ec-81c0-9ade2a150550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from run_distillation import (ModelArguments, DataTrainingArguments, DistillationTrainingArguments,\n",
    "    DataCollatorSpeechSeq2SeqWithPadding, log_metric, log_pred, convert_dataset_str_to_list,\n",
    "    load_multiple_datasets, get_layers_to_supervise, sorted_checkpoints, rotate_checkpoints,\n",
    "    get_last_checkpoint, get_parameter_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe07f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "# sampling_rate = 16_000\n",
    "# 3. Load dataset\n",
    "all_train_datasets_list = []\n",
    "all_eval_datasets_list = []\n",
    "\n",
    "# Mozila UK\n",
    "\n",
    "def prepare_mozilla_uk_dataset(folder_path, split):\n",
    "    split_dataset = datasets.load_from_disk(f'{folder_path}/{split}',) \n",
    "    split_dataset = split_dataset.rename_column(\"sentence\", \"text\")\n",
    "    split_dataset = split_dataset.remove_columns(\n",
    "        set(split_dataset.features.keys()) - {\"audio\", \"text\", \"whisper_transcript\"}\n",
    "        )\n",
    "    return split_dataset\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    print(split)\n",
    "    all_train_datasets_list.append(\n",
    "        prepare_mozilla_uk_dataset('dataset_saved/labeled', split)\n",
    "        )\n",
    "    \n",
    "for split in ['test']:\n",
    "    print(split)\n",
    "    all_eval_datasets_list.append(\n",
    "        prepare_mozilla_uk_dataset('dataset_saved/labeled', split)\n",
    "        )\n",
    "    \n",
    "# Place for next dataset    \n",
    "\n",
    "raw_datasets['train'] = concatenate_datasets(all_train_datasets_list)\n",
    "raw_datasets['eval'] = concatenate_datasets(all_eval_datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364ef027-253e-4b56-a685-0915746b5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parse input arguments\n",
    "# We keep distinct sets of args, for cleaner separation of model/data/training related args\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, DistillationTrainingArguments))\n",
    "\n",
    "list_args = [\n",
    "    '--model_name_or_path=./models_dir/student_moz_uk',\n",
    "    # '--teacher_model_name_or_path=openai/whisper-medium',\n",
    "    '--teacher_model_name_or_path=./local_whisper_medium',\n",
    "    '--eval_steps=500',\n",
    "    '--save_steps=500',\n",
    "    '--warmup_steps=250',\n",
    "    '--learning_rate=0.00001',\n",
    "    '--lr_scheduler_type=constant_with_warmup',\n",
    "    '--logging_steps=25',\n",
    "    '--save_total_limit=3',\n",
    "    '--max_steps=10000',\n",
    "    # '--wer_threshold=None',\n",
    "    '--per_device_train_batch_size=24',\n",
    "    '--per_device_eval_batch_size=24',\n",
    "    '--dataloader_num_workers=4',\n",
    "    '--preprocessing_num_workers=4',\n",
    "    '--ddp_timeout=7200',\n",
    "    '--dtype=float16',\n",
    "    '--do_train=True',\n",
    "    '--do_eval=True',\n",
    "    '--gradient_checkpointing=True',\n",
    "    '--streaming=False',\n",
    "    '--cache_dir=./model_cache/',\n",
    "\n",
    "    '--overwrite_output_dir=True',\n",
    "    '--output_dir=./result_distiling_3',\n",
    "    '--freeze_encoder=True',\n",
    "    '--language=uk',\n",
    "    # '--=',\n",
    "]\n",
    "\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(list_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173692f2-afac-4f39-ac97-97e7a4c1b590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzekamrozek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\distiling_whisper_local\\wandb\\run-20240416_181148-okut9fs8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zekamrozek/distil-whisper/runs/okut9fs8' target=\"_blank\">genial-pine-25</a></strong> to <a href='https://wandb.ai/zekamrozek/distil-whisper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zekamrozek/distil-whisper' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zekamrozek/distil-whisper/runs/okut9fs8' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper/runs/okut9fs8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 18:11:57 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/16/2024 18:11:57 - INFO - __main__ - Training/evaluation parameters DistillationTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=7200,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "dtype=float16,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=500.0,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_encoder=True,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "kl_weight=1.0,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./result_distiling_3\\runs\\Apr16_18-11-44_DESKTOP-9H5C6TS,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=25,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=constant_with_warmup,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10000,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./result_distiling_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=24,\n",
      "per_device_train_batch_size=24,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./result_distiling_3,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "temperature=2.0,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=250,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. Initialize the accelerator\n",
    "# We will let the accelerator handle device placement for us in this example\n",
    "# We simply have to specify the training precision and any trackers being used\n",
    "# We'll use the same dtype arguments as our JAX/Flax training script and convert\n",
    "# it to accelerate format\n",
    "# The teacher model can safely be cast to the dtype of training since we don't\n",
    "# update the params\n",
    "if training_args.dtype == \"float16\":\n",
    "    mixed_precision = \"fp16\"\n",
    "    teacher_dtype = torch.float16\n",
    "elif training_args.dtype == \"bfloat16\":\n",
    "    mixed_precision = \"bf16\"\n",
    "    teacher_dtype = torch.bfloat16\n",
    "else:\n",
    "    mixed_precision = \"no\"\n",
    "    teacher_dtype = torch.float32\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "    mixed_precision=mixed_precision,\n",
    "    log_with=training_args.report_to,\n",
    "    project_dir=training_args.output_dir,\n",
    ")\n",
    "\n",
    "accelerator.init_trackers(project_name=data_args.wandb_project)\n",
    "\n",
    "# 3. Set-up basic logging\n",
    "# Create one log on every process with the configuration for debugging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "# Log a small summary on each proces\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
    "    f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "\n",
    "# Set the verbosity to info of the Transformers logger (on main process only)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef778b1-6b8e-4869-9d5e-826742fef631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5335a96-b4d0-4198-827f-cba2ef4d94ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if training_args.push_to_hub:\n",
    "        # Retrieve of infer repo_name\n",
    "        repo_name = training_args.hub_model_id\n",
    "        if repo_name is None:\n",
    "            repo_name = Path(training_args.output_dir).absolute().name\n",
    "        # Create repo and retrieve repo_id\n",
    "        repo_id = create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n",
    "        # Clone repo locally\n",
    "        repo = Repository(training_args.output_dir, clone_from=repo_id, token=training_args.hub_token)\n",
    "\n",
    "        with open(os.path.join(training_args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "            if \"wandb\" not in gitignore:\n",
    "                gitignore.write(\"wandb\\n\")\n",
    "    elif training_args.output_dir is not None:\n",
    "        os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eecf625-6143-4d42-ae37-7968aeef3664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models_dir/student_moz_uk\\config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"./local_whisper_medium\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 2,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 24,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading configuration file ./models_dir/student_moz_uk\\preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file tokenizer.json\n",
      "loading file merges.txt\n",
      "loading file normalizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Load pretrained model, tokenizer, and feature extractor\n",
    "config = WhisperConfig.from_pretrained(\n",
    "    (model_args.config_name if model_args.config_name else model_args.model_name_or_path),\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    ")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "    (model_args.feature_extractor_name if model_args.feature_extractor_name else model_args.model_name_or_path),\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    ")\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\n",
    "    (model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path),\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    ")\n",
    "\n",
    "# override timestamp tokens until tokenizer issues are fixed in transformers\n",
    "timestamps = [AddedToken(\"<|%.2f|>\" % (i * 0.02), lstrip=False, rstrip=False) for i in range(1500 + 1)]\n",
    "tokenizer.add_tokens(timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94fae2c-324c-462f-bf2a-0703cedb9fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./local_whisper_medium\\config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-medium\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 24,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 24,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file ./local_whisper_medium\\model.safetensors\n",
      "Instantiating WhisperForConditionalGeneration model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"max_length\": 448,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ]\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at ./local_whisper_medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./local_whisper_medium\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"alignment_heads\": [\n",
      "    [\n",
      "      13,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      16,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      20,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      23,\n",
      "      4\n",
      "    ]\n",
      "  ],\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ]\n",
      "  ],\n",
      "  \"is_multilingual\": true,\n",
      "  \"lang_to_id\": {\n",
      "    \"<|af|>\": 50327,\n",
      "    \"<|am|>\": 50334,\n",
      "    \"<|ar|>\": 50272,\n",
      "    \"<|as|>\": 50350,\n",
      "    \"<|az|>\": 50304,\n",
      "    \"<|ba|>\": 50355,\n",
      "    \"<|be|>\": 50330,\n",
      "    \"<|bg|>\": 50292,\n",
      "    \"<|bn|>\": 50302,\n",
      "    \"<|bo|>\": 50347,\n",
      "    \"<|br|>\": 50309,\n",
      "    \"<|bs|>\": 50315,\n",
      "    \"<|ca|>\": 50270,\n",
      "    \"<|cs|>\": 50283,\n",
      "    \"<|cy|>\": 50297,\n",
      "    \"<|da|>\": 50285,\n",
      "    \"<|de|>\": 50261,\n",
      "    \"<|el|>\": 50281,\n",
      "    \"<|en|>\": 50259,\n",
      "    \"<|es|>\": 50262,\n",
      "    \"<|et|>\": 50307,\n",
      "    \"<|eu|>\": 50310,\n",
      "    \"<|fa|>\": 50300,\n",
      "    \"<|fi|>\": 50277,\n",
      "    \"<|fo|>\": 50338,\n",
      "    \"<|fr|>\": 50265,\n",
      "    \"<|gl|>\": 50319,\n",
      "    \"<|gu|>\": 50333,\n",
      "    \"<|haw|>\": 50352,\n",
      "    \"<|ha|>\": 50354,\n",
      "    \"<|he|>\": 50279,\n",
      "    \"<|hi|>\": 50276,\n",
      "    \"<|hr|>\": 50291,\n",
      "    \"<|ht|>\": 50339,\n",
      "    \"<|hu|>\": 50286,\n",
      "    \"<|hy|>\": 50312,\n",
      "    \"<|id|>\": 50275,\n",
      "    \"<|is|>\": 50311,\n",
      "    \"<|it|>\": 50274,\n",
      "    \"<|ja|>\": 50266,\n",
      "    \"<|jw|>\": 50356,\n",
      "    \"<|ka|>\": 50329,\n",
      "    \"<|kk|>\": 50316,\n",
      "    \"<|km|>\": 50323,\n",
      "    \"<|kn|>\": 50306,\n",
      "    \"<|ko|>\": 50264,\n",
      "    \"<|la|>\": 50294,\n",
      "    \"<|lb|>\": 50345,\n",
      "    \"<|ln|>\": 50353,\n",
      "    \"<|lo|>\": 50336,\n",
      "    \"<|lt|>\": 50293,\n",
      "    \"<|lv|>\": 50301,\n",
      "    \"<|mg|>\": 50349,\n",
      "    \"<|mi|>\": 50295,\n",
      "    \"<|mk|>\": 50308,\n",
      "    \"<|ml|>\": 50296,\n",
      "    \"<|mn|>\": 50314,\n",
      "    \"<|mr|>\": 50320,\n",
      "    \"<|ms|>\": 50282,\n",
      "    \"<|mt|>\": 50343,\n",
      "    \"<|my|>\": 50346,\n",
      "    \"<|ne|>\": 50313,\n",
      "    \"<|nl|>\": 50271,\n",
      "    \"<|nn|>\": 50342,\n",
      "    \"<|no|>\": 50288,\n",
      "    \"<|oc|>\": 50328,\n",
      "    \"<|pa|>\": 50321,\n",
      "    \"<|pl|>\": 50269,\n",
      "    \"<|ps|>\": 50340,\n",
      "    \"<|pt|>\": 50267,\n",
      "    \"<|ro|>\": 50284,\n",
      "    \"<|ru|>\": 50263,\n",
      "    \"<|sa|>\": 50344,\n",
      "    \"<|sd|>\": 50332,\n",
      "    \"<|si|>\": 50322,\n",
      "    \"<|sk|>\": 50298,\n",
      "    \"<|sl|>\": 50305,\n",
      "    \"<|sn|>\": 50324,\n",
      "    \"<|so|>\": 50326,\n",
      "    \"<|sq|>\": 50317,\n",
      "    \"<|sr|>\": 50303,\n",
      "    \"<|su|>\": 50357,\n",
      "    \"<|sv|>\": 50273,\n",
      "    \"<|sw|>\": 50318,\n",
      "    \"<|ta|>\": 50287,\n",
      "    \"<|te|>\": 50299,\n",
      "    \"<|tg|>\": 50331,\n",
      "    \"<|th|>\": 50289,\n",
      "    \"<|tk|>\": 50341,\n",
      "    \"<|tl|>\": 50348,\n",
      "    \"<|tr|>\": 50268,\n",
      "    \"<|tt|>\": 50351,\n",
      "    \"<|uk|>\": 50280,\n",
      "    \"<|ur|>\": 50290,\n",
      "    \"<|uz|>\": 50337,\n",
      "    \"<|vi|>\": 50278,\n",
      "    \"<|yi|>\": 50335,\n",
      "    \"<|yo|>\": 50325,\n",
      "    \"<|zh|>\": 50260\n",
      "  },\n",
      "  \"max_initial_timestamp_index\": 50,\n",
      "  \"max_length\": 448,\n",
      "  \"no_timestamps_token_id\": 50363,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"prev_sot_token_id\": 50361,\n",
      "  \"return_timestamps\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"task_to_id\": {\n",
      "    \"transcribe\": 50359,\n",
      "    \"translate\": 50358\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_args.teacher_model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    token=model_args.token,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=teacher_dtype,\n",
    ")\n",
    "\n",
    "# teacher_model.save_pretrained('./local_whisper_medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb611d22-0835-4147-8e74-5b653cff12dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file ./models_dir/student_moz_uk\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"max_length\": 448,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ]\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at ./models_dir/student_moz_uk.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./models_dir/student_moz_uk\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"alignment_heads\": [\n",
      "    [\n",
      "      13,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      16,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      20,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      23,\n",
      "      4\n",
      "    ]\n",
      "  ],\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ]\n",
      "  ],\n",
      "  \"is_multilingual\": true,\n",
      "  \"lang_to_id\": {\n",
      "    \"<|af|>\": 50327,\n",
      "    \"<|am|>\": 50334,\n",
      "    \"<|ar|>\": 50272,\n",
      "    \"<|as|>\": 50350,\n",
      "    \"<|az|>\": 50304,\n",
      "    \"<|ba|>\": 50355,\n",
      "    \"<|be|>\": 50330,\n",
      "    \"<|bg|>\": 50292,\n",
      "    \"<|bn|>\": 50302,\n",
      "    \"<|bo|>\": 50347,\n",
      "    \"<|br|>\": 50309,\n",
      "    \"<|bs|>\": 50315,\n",
      "    \"<|ca|>\": 50270,\n",
      "    \"<|cs|>\": 50283,\n",
      "    \"<|cy|>\": 50297,\n",
      "    \"<|da|>\": 50285,\n",
      "    \"<|de|>\": 50261,\n",
      "    \"<|el|>\": 50281,\n",
      "    \"<|en|>\": 50259,\n",
      "    \"<|es|>\": 50262,\n",
      "    \"<|et|>\": 50307,\n",
      "    \"<|eu|>\": 50310,\n",
      "    \"<|fa|>\": 50300,\n",
      "    \"<|fi|>\": 50277,\n",
      "    \"<|fo|>\": 50338,\n",
      "    \"<|fr|>\": 50265,\n",
      "    \"<|gl|>\": 50319,\n",
      "    \"<|gu|>\": 50333,\n",
      "    \"<|haw|>\": 50352,\n",
      "    \"<|ha|>\": 50354,\n",
      "    \"<|he|>\": 50279,\n",
      "    \"<|hi|>\": 50276,\n",
      "    \"<|hr|>\": 50291,\n",
      "    \"<|ht|>\": 50339,\n",
      "    \"<|hu|>\": 50286,\n",
      "    \"<|hy|>\": 50312,\n",
      "    \"<|id|>\": 50275,\n",
      "    \"<|is|>\": 50311,\n",
      "    \"<|it|>\": 50274,\n",
      "    \"<|ja|>\": 50266,\n",
      "    \"<|jw|>\": 50356,\n",
      "    \"<|ka|>\": 50329,\n",
      "    \"<|kk|>\": 50316,\n",
      "    \"<|km|>\": 50323,\n",
      "    \"<|kn|>\": 50306,\n",
      "    \"<|ko|>\": 50264,\n",
      "    \"<|la|>\": 50294,\n",
      "    \"<|lb|>\": 50345,\n",
      "    \"<|ln|>\": 50353,\n",
      "    \"<|lo|>\": 50336,\n",
      "    \"<|lt|>\": 50293,\n",
      "    \"<|lv|>\": 50301,\n",
      "    \"<|mg|>\": 50349,\n",
      "    \"<|mi|>\": 50295,\n",
      "    \"<|mk|>\": 50308,\n",
      "    \"<|ml|>\": 50296,\n",
      "    \"<|mn|>\": 50314,\n",
      "    \"<|mr|>\": 50320,\n",
      "    \"<|ms|>\": 50282,\n",
      "    \"<|mt|>\": 50343,\n",
      "    \"<|my|>\": 50346,\n",
      "    \"<|ne|>\": 50313,\n",
      "    \"<|nl|>\": 50271,\n",
      "    \"<|nn|>\": 50342,\n",
      "    \"<|no|>\": 50288,\n",
      "    \"<|oc|>\": 50328,\n",
      "    \"<|pa|>\": 50321,\n",
      "    \"<|pl|>\": 50269,\n",
      "    \"<|ps|>\": 50340,\n",
      "    \"<|pt|>\": 50267,\n",
      "    \"<|ro|>\": 50284,\n",
      "    \"<|ru|>\": 50263,\n",
      "    \"<|sa|>\": 50344,\n",
      "    \"<|sd|>\": 50332,\n",
      "    \"<|si|>\": 50322,\n",
      "    \"<|sk|>\": 50298,\n",
      "    \"<|sl|>\": 50305,\n",
      "    \"<|sn|>\": 50324,\n",
      "    \"<|so|>\": 50326,\n",
      "    \"<|sq|>\": 50317,\n",
      "    \"<|sr|>\": 50303,\n",
      "    \"<|su|>\": 50357,\n",
      "    \"<|sv|>\": 50273,\n",
      "    \"<|sw|>\": 50318,\n",
      "    \"<|ta|>\": 50287,\n",
      "    \"<|te|>\": 50299,\n",
      "    \"<|tg|>\": 50331,\n",
      "    \"<|th|>\": 50289,\n",
      "    \"<|tk|>\": 50341,\n",
      "    \"<|tl|>\": 50348,\n",
      "    \"<|tr|>\": 50268,\n",
      "    \"<|tt|>\": 50351,\n",
      "    \"<|uk|>\": 50280,\n",
      "    \"<|ur|>\": 50290,\n",
      "    \"<|uz|>\": 50337,\n",
      "    \"<|vi|>\": 50278,\n",
      "    \"<|yi|>\": 50335,\n",
      "    \"<|yo|>\": 50325,\n",
      "    \"<|zh|>\": 50260\n",
      "  },\n",
      "  \"max_initial_timestamp_index\": 50,\n",
      "  \"max_length\": 448,\n",
      "  \"no_timestamps_token_id\": 50363,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"prev_sot_token_id\": 50361,\n",
      "  \"return_timestamps\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"task_to_id\": {\n",
      "    \"transcribe\": 50359,\n",
      "    \"translate\": 50358\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    subfolder=model_args.subfolder,\n",
    "    token=model_args.token,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e0f764-abca-497e-a0c0-b5a54d823a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if student_model.config.decoder_start_token_id is None or teacher_model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\n",
    "        f\"Make sure that `config.decoder_start_token_id` is correctly defined for both the \"\n",
    "        f\"student and teacher model. Got {student_model.config.decoder_start_token_id} for the \"\n",
    "        f\"student and {teacher_model.config.decoder_start_token_id} for the teacher.\"\n",
    "    )\n",
    "\n",
    "share_hidden_states = training_args.freeze_encoder and student_model.config.d_model == teacher_model.config.d_model\n",
    "\n",
    "# enable gradient checkpointing if necessary\n",
    "if training_args.gradient_checkpointing:\n",
    "    student_model.gradient_checkpointing_enable()\n",
    "\n",
    "# freeze student encoder if necessary\n",
    "if training_args.freeze_encoder:\n",
    "    student_model.freeze_encoder()\n",
    "    student_model.model.encoder.gradient_checkpointing = False\n",
    "\n",
    "# if share_hidden_states:\n",
    "#     # tie the weights for the teacher encoder if we're freezing the student and it's the same as the teacher\n",
    "#     teacher_model.model.encoder = student_model.model.encoder\n",
    "\n",
    "is_multilingual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "429b70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extractor saved in ./result_distiling_3\\preprocessor_config.json\n",
      "tokenizer config file saved in ./result_distiling_3\\tokenizer_config.json\n",
      "Special tokens file saved in ./result_distiling_3\\special_tokens_map.json\n",
      "Configuration saved in ./result_distiling_3\\config.json\n",
      "Configuration saved in ./result_distiling_3\\generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# 8. Create a single speech processor - make sure all processes wait until data is saved\n",
    "if accelerator.is_main_process:\n",
    "    feature_extractor.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    # save the config and generation config as well\n",
    "    config.save_pretrained(training_args.output_dir)\n",
    "    student_model.generation_config.save_pretrained(training_args.output_dir)\n",
    "\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fec3a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./result_distiling_3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a33683d-4976-45ab-a540-c7681eb9774b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./result_distiling_3\\preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file tokenizer.json\n",
      "loading file merges.txt\n",
      "loading file normalizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410b5932-8f58-47af-b4de-f13737e8d700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 9. Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,\n",
    "# so we just need to set the correct target sampling rate.\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "raw_datasets = raw_datasets.cast_column(\n",
    "    data_args.audio_column_name,\n",
    "    datasets.features.Audio(sampling_rate=sampling_rate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "589600c8-9014-41ca-8a2a-fc7516d51122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Preprocessing the datasets: we need to read the audio files as arrays and tokenize the targets.\n",
    "# 10.1: Define the pre-processing constants\n",
    "max_input_length = int(data_args.max_duration_in_seconds * sampling_rate)\n",
    "min_input_length = int(data_args.min_duration_in_seconds * sampling_rate)\n",
    "max_label_length = (\n",
    "    data_args.max_label_length if data_args.max_label_length is not None else student_model.config.max_length\n",
    ")\n",
    "\n",
    "timestamp_probability = data_args.timestamp_probability\n",
    "condition_on_prev_probability = data_args.condition_on_prev_probability\n",
    "return_timestamps = data_args.return_timestamps if timestamp_probability > 0 else False\n",
    "\n",
    "timestamp_ids = tokenizer.timestamp_ids()\n",
    "timestamp_begin = tokenizer.all_special_ids[-1]\n",
    "timestamp_position = 3 if is_multilingual else 1\n",
    "\n",
    "decoder_start_token_id = student_model.config.decoder_start_token_id  # <|startoftranscript|>\n",
    "decoder_prev_token_id = tokenizer.all_special_ids[-3]  # <|startofprev|>\n",
    "decoder_eot_token_id = tokenizer.eos_token_id\n",
    "\n",
    "language = data_args.language\n",
    "task = data_args.task\n",
    "\n",
    "num_workers = data_args.preprocessing_num_workers\n",
    "dataloader_num_workers = training_args.dataloader_num_workers\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "normalizer = (\n",
    "    BasicTextNormalizer() if language is not None else EnglishTextNormalizer(tokenizer.english_spelling_normalizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b6dae6c-60dc-44fe-ba50-75f9d296bc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wer_threshold = 10#data_args.wer_threshold\n",
    "\n",
    "\n",
    "# 10.3: filter training data based on WER threshold -> this is KEY to good distillation performance\n",
    "def is_wer_in_range(ground_truth, whisper_transcript):\n",
    "    norm_ground_truth = normalizer(ground_truth)\n",
    "    if (\n",
    "        isinstance(whisper_transcript, str)\n",
    "        and whisper_transcript.startswith(\"[\")\n",
    "        and whisper_transcript.endswith(\"]\")\n",
    "    ):\n",
    "        whisper_transcript = re.findall(r\"\\d+\", whisper_transcript)\n",
    "        whisper_transcript = [int(token) for token in whisper_transcript]\n",
    "    if isinstance(whisper_transcript, list):\n",
    "        whisper_transcript = tokenizer.decode(whisper_transcript, skip_special_tokens=True)\n",
    "    if len(norm_ground_truth) > 0 and whisper_transcript is not None:\n",
    "        norm_whisper_transcript = normalizer(whisper_transcript)\n",
    "        wer = 100 * metric.compute(predictions=[norm_whisper_transcript], references=[norm_ground_truth])\n",
    "        return wer < wer_threshold\n",
    "    else:\n",
    "        # filter automatically since we can't know the WER\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee37ac3-514c-40c7-ae03-eeb89774b113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33585 9791\n",
      "train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfc2f28242e4b5da6e7e4e765b5248b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by wer:   0%|          | 0/33585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c815544e9e849d7b8fd9f7fe9889d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by wer:   0%|          | 0/9791 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11439 3196\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'].num_rows, raw_datasets['eval'].num_rows)\n",
    "\n",
    "for split_for_wer_filter in [\n",
    "    'train', 'eval'\n",
    "]:\n",
    "    print(split_for_wer_filter\n",
    "    )\n",
    "    filter_by_wer_threshold = partial(\n",
    "        raw_datasets[split_for_wer_filter].filter,\n",
    "        function=is_wer_in_range,\n",
    "        input_columns=[\"text\", \"whisper_transcript\"],\n",
    "    )\n",
    "\n",
    "    if wer_threshold is not None:\n",
    "        raw_datasets[split_for_wer_filter] = filter_by_wer_threshold(num_proc=1, desc=\"filtering train dataset by wer\")\n",
    "    \n",
    "print(raw_datasets['train'].num_rows, raw_datasets['eval'].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ac6073-4540-4556-aeaa-69d9fcac072a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10.4: pre-process training/evaluation datasets\n",
    "def has_timestamp_tokens(input_str):\n",
    "    \"\"\"\n",
    "    Identify whether the input string contains timestamp tokens, of the form <|0.00|>, by searching for\n",
    "    pairs of left and right-angle brackets.\n",
    "    \"\"\"\n",
    "    return bool(re.search(\"\\<[^\\>]*\\>\", input_str))\n",
    "\n",
    "def prepare_train_dataset(batch):\n",
    "    \"\"\"\n",
    "    Pre-process the raw dataset in a three stage process:\n",
    "        1. Convert the audio arrays to log-mel spectrogram inputs\n",
    "        2. Possibly filter the timestamp tokens from the token ids (depending on the timestamp probability)\n",
    "        3. Possibly add prompt tokens if conditioning on previous text (depending on the conditioning probability)\n",
    "    TODO(SG): see whether we can 'pack' the audio inputs closer to 30 second chunks\n",
    "    \"\"\"\n",
    "    # process audio input\n",
    "    audio = [sample[\"array\"] for sample in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(audio, sampling_rate=sampling_rate)\n",
    "    batch[\"input_features\"] = inputs.input_features\n",
    "    batch[\"input_length\"] = [len(sample) for sample in audio]\n",
    "\n",
    "    # process text targets - for training these are the Whisper-generated pseudo-labels\n",
    "    input_str_batched = batch[\"whisper_transcript\"]\n",
    "\n",
    "    all_token_ids = []\n",
    "    all_token_ids_unprompted = []\n",
    "    for input_str in input_str_batched:\n",
    "        if isinstance(input_str, list):\n",
    "            # pseudo-labelled transcriptions have been retained as token ids (`decode_token_ids=False`)\n",
    "            token_ids = input_str\n",
    "        elif input_str[0].startswith(\"[\") and input_str[0].endswith(\"]\"):\n",
    "            token_ids = re.findall(r\"\\d+\", input_str)\n",
    "            token_ids = [int(token) for token in token_ids]\n",
    "        else:\n",
    "            token_ids = None\n",
    "\n",
    "        if token_ids is not None:\n",
    "            # remove the EOT tokens to get the 'true' token length\n",
    "            token_ids = [token for token in token_ids if token != decoder_eot_token_id]\n",
    "            token_ids = token_ids + [decoder_eot_token_id]\n",
    "            # check whether we have timestamps in the PLs and filter if required\n",
    "            has_timestamps = len(set(token_ids) & set(timestamp_ids)) > 0\n",
    "            if has_timestamps:\n",
    "                # sample from binomial distribution to get probability of training on timestamps\n",
    "                predict_timestamps = bool(np.random.binomial(1, timestamp_probability))\n",
    "                if not predict_timestamps:\n",
    "                    # filter timestamps and insert the <|notimestamps|> task token\n",
    "                    token_ids = [token for token in token_ids if token < timestamp_begin]\n",
    "                    token_ids.insert(timestamp_position, timestamp_begin)\n",
    "        else:\n",
    "            # pseudo-labelled transcriptions have been decoded to text (`decode_token_ids=True`)\n",
    "            has_timestamps = has_timestamp_tokens(input_str)\n",
    "\n",
    "            if has_timestamps:\n",
    "                predict_timestamps = bool(np.random.binomial(1, timestamp_probability))\n",
    "                if not predict_timestamps:\n",
    "                    # filter timestamp token ids if not part of the prediction task\n",
    "                    input_str = tokenizer._filter_timestamp_ids(input_str)\n",
    "            else:\n",
    "                predict_timestamps = False\n",
    "\n",
    "            tokenizer.set_prefix_tokens(language=language, task=task, predict_timestamps=predict_timestamps)\n",
    "            token_ids = tokenizer(input_str).input_ids\n",
    "\n",
    "        all_token_ids_unprompted.append(token_ids)\n",
    "        # check whether to condition on previous text - we do this with probability condition_on_prev_probability\n",
    "        condition_on_prev = bool(np.random.binomial(1, condition_on_prev_probability))\n",
    "        if condition_on_prev and len(all_token_ids_unprompted) > 1:\n",
    "            # prompt ids are the penultimate token ids in the batch\n",
    "            prompt_ids = all_token_ids_unprompted[-2]\n",
    "            # strip timestamp tokens from prompt\n",
    "            prompt_ids = [token for token in prompt_ids if token < timestamp_begin]\n",
    "            if len(prompt_ids) > 0:\n",
    "                # remove the standard task tokens and add the special <|startofprev|> token\n",
    "                prompt_ids = [decoder_prev_token_id] + prompt_ids[timestamp_position:-1]\n",
    "            if len(prompt_ids + token_ids) < max_label_length:\n",
    "                token_ids = prompt_ids + token_ids\n",
    "        all_token_ids.append(token_ids)\n",
    "\n",
    "    batch[\"labels\"] = all_token_ids\n",
    "    return batch\n",
    "\n",
    "def prepare_eval_dataset(batch):\n",
    "    # process audio input\n",
    "    sample = batch[\"audio\"]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    batch[\"input_length\"] = len(sample[\"array\"])\n",
    "\n",
    "    # process targets - for evaluation these are the ground-truth transcriptions\n",
    "    input_str = batch[\"text\"]\n",
    "    batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c72be60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c7616d2a6d4b06996a0c97d1424b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess train dataset:   0%|          | 0/11439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aeee0ae8eb491888f8472389b1246e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess eval dataset:   0%|          | 0/3196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_datasets = DatasetDict()\n",
    "raw_datasets_train_features = list(raw_datasets[\"train\"].features.keys())\n",
    "\n",
    "map_fn_train = partial(\n",
    "    raw_datasets['train'].map,\n",
    "    function=prepare_train_dataset,\n",
    "    remove_columns=raw_datasets_train_features,\n",
    "    batched=True,\n",
    "    batch_size=max(training_args.per_device_train_batch_size // 4, 4),  # TODO(SG) make data prep bs configurable\n",
    ")\n",
    "vectorized_datasets[\"train\"] = map_fn_train(num_proc=1, desc=\"preprocess train dataset\")\n",
    "\n",
    "\n",
    "raw_datasets_eval_features = list(raw_datasets['eval'].features.keys())\n",
    "map_fn_eval = partial(\n",
    "    raw_datasets['eval'].map, function=prepare_eval_dataset, remove_columns=raw_datasets_eval_features\n",
    ")\n",
    "\n",
    "vectorized_datasets['eval'] = map_fn_eval(num_proc=1, desc=\"preprocess eval dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed788686-1409-4a04-a868-23200ed145db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f510392bdbe4fc088474f1248b68255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by audio length:   0%|          | 0/11439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcb45147d8d4562b08ee19a9657a3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by audio length:   0%|          | 0/3196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13248228a96d4900baad6757efee8228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset:   0%|          | 0/11439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf3c07399ef44e2952fa8f6785288b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset:   0%|          | 0/3196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10.5: Filter training data with inputs longer than `max_input_length`\n",
    "def is_audio_in_length_range(length):\n",
    "    return min_input_length < length < max_input_length\n",
    "\n",
    "filter_by_audio_fn = partial(\n",
    "    vectorized_datasets.filter, function=is_audio_in_length_range, input_columns=[\"input_length\"]\n",
    ")\n",
    "vectorized_datasets = filter_by_audio_fn(num_proc=1, desc=\"filtering train dataset by audio length\")\n",
    "\n",
    "# 10.6: Filter training data with labels longer than `max_label_length`\n",
    "def is_labels_in_length_range(labels):\n",
    "    return 0 < len(labels) <= max_label_length\n",
    "\n",
    "filter_by_labels_fn = partial(\n",
    "    vectorized_datasets.filter, function=is_labels_in_length_range, input_columns=[\"labels\"]\n",
    ")\n",
    "vectorized_datasets = filter_by_labels_fn(num_proc=1, desc=\"filtering train dataset\")\n",
    "\n",
    "# Pre-processing complete!\n",
    "# For large datasets it is advised to run the preprocessing on a\n",
    "# single machine first with `--preprocessing_only` since there will mostly likely\n",
    "# be a timeout when running the script in distributed mode.\n",
    "# In a second step, `--preprocessing_only` can then be set to `False` to load the\n",
    "# cached dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c7beaf6-5e5a-423c-8d83-a7a30cbac35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 18:26:39 - INFO - __main__ - Data preprocessing finished. Files cached at {'train': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\train\\\\cache-986c887d0bd1f317.arrow'}, {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\train\\\\cache-cef3de1d3b821f5b.arrow'}], 'eval': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\test\\\\cache-905c92fa89753543.arrow'}, {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\test\\\\cache-d290041596bd97bc.arrow'}]}.\n"
     ]
    }
   ],
   "source": [
    "cache = {k: v.cache_files for k, v in vectorized_datasets.items()}\n",
    "logger.info(f\"Data preprocessing finished. Files cached at {cache}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbdb2f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\train\\\\cache-5098ea3880ff3c68.arrow'},\n",
       "  {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\train\\\\cache-72585aa289e41103.arrow'}],\n",
       " 'eval': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\test\\\\cache-e7d01e48949f8d93.arrow'},\n",
       "  {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\test\\\\cache-2e2173af0f196642.arrow'}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache\n",
    "{'train': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\train\\\\cache-5098ea3880ff3c68.arrow'},\n",
    "  {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\train\\\\cache-72585aa289e41103.arrow'}],\n",
    " 'eval': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\test\\\\cache-e7d01e48949f8d93.arrow'},\n",
    "  {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled\\\\test\\\\cache-2e2173af0f196642.arrow'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "614234ed-402c-4847-8406-fc2f1e4b8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define Evaluation Metrics\n",
    "def compute_metrics(preds, labels):\n",
    "    # replace padded labels by the padding token\n",
    "    for idx in range(len(labels)):\n",
    "        labels[idx][labels[idx] == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True, decode_with_timestamps=return_timestamps)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # normalize everything and re-compute the WER\n",
    "    norm_pred_str = [normalizer(pred) for pred in pred_str]\n",
    "    norm_label_str = [normalizer(label) for label in label_str]\n",
    "    # for logging, we need the pred/labels to match the norm_pred/norm_labels, so discard any filtered samples here\n",
    "    pred_str = [pred_str[i] for i in range(len(norm_pred_str)) if len(norm_label_str[i]) > 0]\n",
    "    label_str = [label_str[i] for i in range(len(norm_label_str)) if len(norm_label_str[i]) > 0]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero normalized references:\n",
    "    norm_pred_str = [norm_pred_str[i] for i in range(len(norm_pred_str)) if len(norm_label_str[i]) > 0]\n",
    "    norm_label_str = [norm_label_str[i] for i in range(len(norm_label_str)) if len(norm_label_str[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=norm_pred_str, references=norm_label_str)\n",
    "    return {\"wer\": wer, \"wer_ortho\": wer_ortho}, pred_str, label_str, norm_pred_str, norm_label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1854cb9b-8362-43c8-b824-a67e12386493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 18:26:39 - INFO - __main__ - max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# 12. Define Training Schedule\n",
    "# Store some constants\n",
    "per_device_train_batch_size = int(training_args.per_device_train_batch_size)\n",
    "train_batch_size = per_device_train_batch_size * accelerator.num_processes\n",
    "gradient_accumulation_steps = int(training_args.gradient_accumulation_steps)\n",
    "per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
    "\n",
    "if not data_args.streaming and training_args.max_steps < 0:\n",
    "    num_epochs = int(training_args.num_train_epochs)\n",
    "    steps_per_epoch = len(vectorized_datasets[\"train\"]) // (train_batch_size * gradient_accumulation_steps)\n",
    "    total_train_steps = steps_per_epoch * num_epochs\n",
    "elif training_args.max_steps > 0:\n",
    "    logger.info(\"max_steps is given, it will override any value given in num_train_epochs\")\n",
    "    total_train_steps = int(training_args.max_steps)\n",
    "    # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n",
    "    num_epochs = sys.maxsize\n",
    "    steps_per_epoch = total_train_steps\n",
    "else:\n",
    "    raise ValueError(\"max_steps must be specified when training with a streaming (iterable) dataset\")\n",
    "\n",
    "if training_args.eval_steps is None:\n",
    "    logger.info(\n",
    "        f\"eval_steps is not set, evaluating at the end of {'each epoch' if not data_args.streaming else 'training'}\"\n",
    "    )\n",
    "    eval_steps = steps_per_epoch\n",
    "else:\n",
    "    eval_steps = training_args.eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6f8ecd3-10b4-4f70-a8cc-44ac875ecc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Define optimizer, LR scheduler, collator\n",
    "decay_parameters = get_parameter_names(\n",
    "    student_model,\n",
    "    [nn.LayerNorm],\n",
    "    forbidden_module=[student_model.model.encoder] if training_args.freeze_encoder else None,\n",
    ")\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [param for name, param in student_model.named_parameters() if name in decay_parameters],\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [param for name, param in student_model.named_parameters() if name not in decay_parameters],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=optimizer_grouped_parameters,\n",
    "    lr=training_args.learning_rate,\n",
    "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "    eps=training_args.adam_epsilon,\n",
    ")\n",
    "\n",
    "# LR scheduler gets stepped by `num_processes` each time -> account for this in warmup / total steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=training_args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=total_train_steps * accelerator.num_processes,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=decoder_start_token_id,\n",
    "    decoder_prev_token_id=decoder_prev_token_id,\n",
    "    input_padding=\"longest\",\n",
    "    target_padding=\"max_length\",\n",
    "    max_target_length=max_label_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c79087ec-db1e-47c6-939f-87384fb39fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Define generation arguments - we need to do this before we wrap the models in DDP\n",
    "# so that we can still access the configs\n",
    "num_beams = (\n",
    "    training_args.generation_num_beams\n",
    "    if training_args.generation_num_beams is not None\n",
    "    else getattr(student_model.generation_config, \"num_beams\", 1)\n",
    ")\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"max_length\": max_label_length,\n",
    "    \"num_beams\": num_beams,\n",
    "    \"return_timestamps\": return_timestamps,\n",
    "}\n",
    "if hasattr(teacher_model.generation_config, \"is_multilingual\") and teacher_model.generation_config.is_multilingual:\n",
    "    # forcing the language and task tokens helps multilingual models in their generations\n",
    "    gen_kwargs.update(\n",
    "        {\n",
    "            \"language\": data_args.language,\n",
    "            \"task\": data_args.task,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91f94ce5-d385-486f-a0dd-cf7898e4c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Prepare everything with accelerate\n",
    "student_model, teacher_model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "    student_model, teacher_model, optimizer, lr_scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8027d324-ace5-4ca4-a886-2cbce2c42d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kl_divergence(target_distribution, log_predicted_distribution, labels):\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"none\")\n",
    "    divergence = kl_loss(log_predicted_distribution, target_distribution)\n",
    "    # ignore padded tokens from divergence, i.e. where labels are not set to -100\n",
    "    padding_mask = labels >= 0\n",
    "    padding_mask = padding_mask.unsqueeze(-1)\n",
    "    divergence = divergence * padding_mask\n",
    "    # take the average over the mini-batch\n",
    "    divergence = divergence.sum() / padding_mask.sum()\n",
    "    return divergence\n",
    "\n",
    "# Define gradient update step fn\n",
    "def train_step(\n",
    "    batch,\n",
    "    temperature=2.0,\n",
    "):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    student_outputs = student_model(**batch)\n",
    "    with torch.no_grad():\n",
    "        if share_hidden_states:\n",
    "            # if the student and teacher share the same frozen encoder then we don't have to recompute the\n",
    "            # encoder hidden-states for the teacher model, we can just re-use from the student\n",
    "            encoder_outputs = BaseModelOutput(student_outputs.encoder_last_hidden_state)\n",
    "            teacher_outputs = teacher_model(encoder_outputs=encoder_outputs, labels=batch[\"labels\"])\n",
    "        else:\n",
    "            # do the full forward pass for the teacher model (encoder + decoder)\n",
    "            teacher_outputs = teacher_model(**batch)\n",
    "\n",
    "    # CE (data) loss\n",
    "    ce_loss = student_outputs.loss\n",
    "    # rescale distribution by temperature to ensure gradients scale correctly\n",
    "    teacher_distribution = nn.functional.softmax(teacher_outputs.logits / temperature, dim=-1)\n",
    "    # log softmax of student predictions for numerical stability\n",
    "    student_distribution = nn.functional.log_softmax(student_outputs.logits / temperature, dim=-1)\n",
    "    # KL-divergence loss (scaled by temperature)\n",
    "    kl_loss = kl_divergence(teacher_distribution, student_distribution, batch[\"labels\"]) * temperature**2\n",
    "\n",
    "    # use Distil-Whisper formulation (fix weight of CE loss and tune KL weight)\n",
    "    loss = 0.8 * ce_loss + training_args.kl_weight * kl_loss\n",
    "    metrics = {\"loss\": loss, \"ce_loss\": ce_loss, \"kl_loss\": kl_loss}\n",
    "    return loss, metrics\n",
    "\n",
    "# Define eval fn\n",
    "def eval_step(batch):\n",
    "    student_model.eval()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student_model(**batch)\n",
    "        if share_hidden_states:\n",
    "            encoder_outputs = BaseModelOutput(student_outputs.encoder_last_hidden_state)\n",
    "            teacher_outputs = teacher_model(encoder_outputs=encoder_outputs, labels=batch[\"labels\"])\n",
    "        else:\n",
    "            teacher_outputs = teacher_model(**batch)\n",
    "\n",
    "    # CE (data) loss\n",
    "    ce_loss = student_outputs.loss\n",
    "\n",
    "    # log softmax / softmax for numerical stability\n",
    "    student_distribution = nn.functional.log_softmax(student_outputs.logits, dim=-1)\n",
    "    teacher_distribution = nn.functional.softmax(teacher_outputs.logits, dim=-1)\n",
    "    # temperature is always 1 for eval\n",
    "    kl_loss = kl_divergence(teacher_distribution, student_distribution, batch[\"labels\"])\n",
    "\n",
    "    # use Distil-Whisper formulation (fix weight of CE loss and tune KL weight)\n",
    "    loss = 0.8 * ce_loss + training_args.kl_weight * kl_loss\n",
    "    metrics = {\"loss\": loss, \"ce_loss\": ce_loss, \"kl_loss\": kl_loss}\n",
    "    return metrics\n",
    "\n",
    "def generate_step(batch):\n",
    "    student_model.eval()\n",
    "    output_ids = accelerator.unwrap_model(student_model).generate(batch[\"input_features\"], **gen_kwargs)\n",
    "    output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e07bf3c-27b4-4f0a-a856-8da6cdb3a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 18:26:46 - INFO - __main__ - ***** Running training *****\n",
      "04/16/2024 18:26:46 - INFO - __main__ -   Num examples = 240000\n",
      "04/16/2024 18:26:46 - INFO - __main__ -   Instantaneous batch size per device = 24\n",
      "04/16/2024 18:26:46 - INFO - __main__ -   Gradient accumulation steps = 1\n",
      "04/16/2024 18:26:46 - INFO - __main__ -   Total train batch size (w. parallel & distributed) = 24\n",
      "04/16/2024 18:26:46 - INFO - __main__ -   Total optimization steps = 10000\n",
      "Train steps ... :   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {total_train_steps * train_batch_size * gradient_accumulation_steps}\")\n",
    "logger.info(\"  Instantaneous batch size per device =\" f\" {training_args.per_device_train_batch_size}\")\n",
    "logger.info(\"  Gradient accumulation steps =\" f\" {gradient_accumulation_steps}\")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel & distributed) = {train_batch_size * gradient_accumulation_steps}\"\n",
    ")\n",
    "logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
    "\n",
    "# ======================== Training ================================\n",
    "train_time = 0\n",
    "train_start = time.time()\n",
    "steps_trained_progress_bar = tqdm(\n",
    "    range(total_train_steps), desc=\"Train steps ... \", position=0, disable=not accelerator.is_local_main_process\n",
    ")\n",
    "continue_training = True\n",
    "epochs_trained = 0\n",
    "cur_step = 0\n",
    "\n",
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "\n",
    "if checkpoint is not None:\n",
    "    accelerator.load_state(checkpoint)\n",
    "    # Find num steps and epoch from saved state string pattern\n",
    "    pattern = r\"checkpoint-(\\d+)-epoch-(\\d+)\"\n",
    "    match = re.search(pattern, checkpoint)\n",
    "    cur_step = int(match.group(1))\n",
    "    epochs_trained = int(match.group(2))\n",
    "\n",
    "    logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "    logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "    logger.info(f\"  Continuing training from global step {cur_step}\")\n",
    "\n",
    "    steps_trained_progress_bar.update(cur_step)\n",
    "\n",
    "    for epoch in range(0, epochs_trained):\n",
    "        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(training_args.seed)\n",
    "\n",
    "    if not data_args.streaming and training_args.max_steps < 0:\n",
    "        # we know exactly the number of steps per epoch, so can skip through the required number of batches\n",
    "        resume_step = (cur_step - epochs_trained * steps_per_epoch) * gradient_accumulation_steps\n",
    "    else:\n",
    "        # Currently we don't know how many steps we've taken in the current epoch\n",
    "        # So we just shuffle the dataset one extra time and start from a fresh epoch\n",
    "        # This is \"good enough\" for our purposes but not fully correct\n",
    "        resume_step = None\n",
    "        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(training_args.seed)\n",
    "else:\n",
    "    resume_step = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e02eb65-bc78-4408-8c30-3dc561d7d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "f:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Train steps ... :   0%|          | 25/10000 [01:02<4:03:11,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (25 / 10000 | Loss: 37.562034606933594, Learning Rate: 6.800000000000001e-07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   0%|          | 50/10000 [01:39<4:05:28,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (50 / 10000 | Loss: 14.571999549865723, Learning Rate: 1.6800000000000002e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   1%|          | 75/10000 [02:16<4:07:14,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (75 / 10000 | Loss: 10.80471420288086, Learning Rate: 2.68e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   1%|          | 100/10000 [02:53<4:07:40,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (100 / 10000 | Loss: 9.0494966506958, Learning Rate: 3.6800000000000003e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   1%|         | 125/10000 [03:30<3:55:37,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (125 / 10000 | Loss: 7.6973185539245605, Learning Rate: 4.680000000000001e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   2%|         | 150/10000 [04:06<4:10:32,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (150 / 10000 | Loss: 7.172237396240234, Learning Rate: 5.68e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   2%|         | 175/10000 [04:45<4:09:45,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (175 / 10000 | Loss: 7.0225629806518555, Learning Rate: 6.680000000000001e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   2%|         | 200/10000 [05:23<4:08:43,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (200 / 10000 | Loss: 6.685459613800049, Learning Rate: 7.680000000000001e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   2%|         | 225/10000 [06:01<4:05:27,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (225 / 10000 | Loss: 6.407696723937988, Learning Rate: 8.68e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   2%|         | 250/10000 [06:38<4:04:53,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (250 / 10000 | Loss: 6.125951290130615, Learning Rate: 9.68e-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   3%|         | 275/10000 [07:16<4:05:28,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (275 / 10000 | Loss: 6.078726768493652, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   3%|         | 300/10000 [07:54<4:03:51,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (300 / 10000 | Loss: 5.442155838012695, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   3%|         | 325/10000 [08:31<4:03:32,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (325 / 10000 | Loss: 5.7024970054626465, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   4%|         | 350/10000 [09:09<4:02:37,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (350 / 10000 | Loss: 5.596182823181152, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   4%|         | 375/10000 [09:47<4:02:04,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (375 / 10000 | Loss: 5.39168119430542, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   4%|         | 400/10000 [10:25<4:01:28,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (400 / 10000 | Loss: 5.0774736404418945, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   4%|         | 425/10000 [11:02<4:00:57,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (425 / 10000 | Loss: 4.949780464172363, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   4%|         | 450/10000 [11:41<4:04:40,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (450 / 10000 | Loss: 5.225059509277344, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   5%|         | 475/10000 [12:19<4:01:13,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (475 / 10000 | Loss: 4.858271598815918, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   5%|         | 500/10000 [13:17<3:44:17,  1.42s/it] 04/16/2024 18:40:03 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-500-epoch-1\n",
      "04/16/2024 18:40:03 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (500 / 10000 | Loss: 4.963423252105713, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 18:40:04 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-500-epoch-1\\model.safetensors\n",
      "04/16/2024 18:40:04 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 18:40:06 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-500-epoch-1\\model_1.safetensors\n",
      "04/16/2024 18:40:09 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-500-epoch-1\\optimizer.bin\n",
      "04/16/2024 18:40:09 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-500-epoch-1\\scheduler.bin\n",
      "04/16/2024 18:40:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-500-epoch-1\\sampler.bin\n",
      "04/16/2024 18:40:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-500-epoch-1\\sampler_1.bin\n",
      "04/16/2024 18:40:09 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-500-epoch-1\\scaler.pt\n",
      "04/16/2024 18:40:09 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-500-epoch-1\\random_states_0.pkl\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:23<00:00,  1.52s/it]\n",
      "Train steps ... :   5%|         | 500/10000 [16:46<3:44:17,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (500 / 10000 | Eval Loss: 14.736096382141113 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   5%|         | 525/10000 [17:23<4:05:45,  1.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (525 / 10000 | Loss: 4.1686859130859375, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   6%|         | 550/10000 [18:02<4:03:15,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (550 / 10000 | Loss: 4.6322479248046875, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   6%|         | 575/10000 [18:41<4:02:20,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (575 / 10000 | Loss: 3.7231669425964355, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   6%|         | 600/10000 [19:19<4:01:43,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (600 / 10000 | Loss: 4.337860107421875, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   6%|         | 625/10000 [19:58<4:00:46,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (625 / 10000 | Loss: 4.07745361328125, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   6%|         | 650/10000 [20:36<4:00:29,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (650 / 10000 | Loss: 4.3676042556762695, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   7%|         | 675/10000 [21:15<3:59:52,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (675 / 10000 | Loss: 3.7001819610595703, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   7%|         | 700/10000 [21:53<3:56:13,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (700 / 10000 | Loss: 3.7746777534484863, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   7%|         | 725/10000 [22:31<3:55:24,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (725 / 10000 | Loss: 4.081659317016602, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   8%|         | 750/10000 [23:09<3:54:51,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (750 / 10000 | Loss: 3.519364833831787, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   8%|         | 775/10000 [23:47<3:54:18,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (775 / 10000 | Loss: 3.7338523864746094, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   8%|         | 800/10000 [24:25<3:53:43,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (800 / 10000 | Loss: 3.1825969219207764, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   8%|         | 825/10000 [25:03<3:52:47,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (825 / 10000 | Loss: 3.842989444732666, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   8%|         | 850/10000 [25:41<3:52:14,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (850 / 10000 | Loss: 2.8071818351745605, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   9%|         | 875/10000 [26:20<3:51:27,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (875 / 10000 | Loss: 3.540456771850586, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   9%|         | 900/10000 [26:59<3:55:03,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (900 / 10000 | Loss: 3.104435443878174, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :   9%|         | 925/10000 [27:38<3:54:33,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (925 / 10000 | Loss: 3.5860342979431152, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  10%|         | 950/10000 [28:16<3:50:07,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (950 / 10000 | Loss: 2.800153970718384, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  10%|         | 975/10000 [29:14<3:33:05,  1.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (975 / 10000 | Loss: 2.6036558151245117, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  10%|         | 1000/10000 [29:50<3:35:03,  1.43s/it]04/16/2024 18:56:36 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-1000-epoch-2\n",
      "04/16/2024 18:56:36 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1000 / 10000 | Loss: 2.5946388244628906, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 18:56:37 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-1000-epoch-2\\model.safetensors\n",
      "04/16/2024 18:56:37 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 18:56:39 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-1000-epoch-2\\model_1.safetensors\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-1000-epoch-2\\optimizer.bin\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-1000-epoch-2\\scheduler.bin\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-1000-epoch-2\\sampler.bin\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-1000-epoch-2\\sampler_1.bin\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-1000-epoch-2\\sampler_2.bin\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-1000-epoch-2\\sampler_3.bin\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-1000-epoch-2\\scaler.pt\n",
      "04/16/2024 18:56:40 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-1000-epoch-2\\random_states_0.pkl\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:25<00:00,  1.53s/it]\n",
      "Train steps ... :  10%|         | 1000/10000 [33:19<3:35:03,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (1000 / 10000 | Eval Loss: 13.890497207641602 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  10%|         | 1025/10000 [33:58<4:30:49,  1.81s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1025 / 10000 | Loss: 2.2812557220458984, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  10%|         | 1050/10000 [34:38<3:50:58,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1050 / 10000 | Loss: 2.751598358154297, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  11%|         | 1075/10000 [35:16<3:47:51,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1075 / 10000 | Loss: 2.860492706298828, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  11%|         | 1100/10000 [35:55<3:47:36,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1100 / 10000 | Loss: 3.2308192253112793, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  11%|        | 1125/10000 [36:33<3:46:31,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1125 / 10000 | Loss: 2.6588611602783203, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  12%|        | 1150/10000 [37:11<3:45:56,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1150 / 10000 | Loss: 2.5941519737243652, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  12%|        | 1175/10000 [37:50<3:45:22,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1175 / 10000 | Loss: 2.444899559020996, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  12%|        | 1200/10000 [38:28<3:45:15,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1200 / 10000 | Loss: 2.348555088043213, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  12%|        | 1225/10000 [39:07<3:44:06,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1225 / 10000 | Loss: 2.6210384368896484, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  12%|        | 1250/10000 [39:45<3:43:35,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1250 / 10000 | Loss: 2.5350139141082764, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  13%|        | 1275/10000 [40:23<3:42:45,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1275 / 10000 | Loss: 2.417076587677002, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  13%|        | 1300/10000 [41:02<3:42:18,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1300 / 10000 | Loss: 2.1333045959472656, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  13%|        | 1325/10000 [41:40<3:41:40,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1325 / 10000 | Loss: 2.2749218940734863, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  14%|        | 1350/10000 [42:18<3:41:01,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1350 / 10000 | Loss: 2.122131109237671, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  14%|        | 1375/10000 [42:57<3:40:39,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1375 / 10000 | Loss: 2.4274301528930664, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  14%|        | 1400/10000 [43:35<3:39:59,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1400 / 10000 | Loss: 2.0960047245025635, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  14%|        | 1425/10000 [44:13<3:39:21,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1425 / 10000 | Loss: 2.235675096511841, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  14%|        | 1450/10000 [45:17<3:22:46,  1.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1450 / 10000 | Loss: 1.9691576957702637, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  15%|        | 1475/10000 [45:53<3:23:45,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1475 / 10000 | Loss: 2.1694982051849365, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  15%|        | 1500/10000 [46:29<3:35:17,  1.52s/it]04/16/2024 19:13:16 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-1500-epoch-3\n",
      "04/16/2024 19:13:16 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1500 / 10000 | Loss: 2.12319016456604, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 19:13:17 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-1500-epoch-3\\model.safetensors\n",
      "04/16/2024 19:13:17 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 19:13:21 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-1500-epoch-3\\model_1.safetensors\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-1500-epoch-3\\optimizer.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-1500-epoch-3\\scheduler.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-1500-epoch-3\\sampler.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-1500-epoch-3\\sampler_1.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-1500-epoch-3\\sampler_2.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-1500-epoch-3\\sampler_3.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-1500-epoch-3\\sampler_4.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-1500-epoch-3\\sampler_5.bin\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-1500-epoch-3\\scaler.pt\n",
      "04/16/2024 19:13:34 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-1500-epoch-3\\random_states_0.pkl\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:28<00:00,  1.56s/it]\n",
      "Train steps ... :  15%|        | 1500/10000 [50:17<3:35:17,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (1500 / 10000 | Eval Loss: 14.21540641784668 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  15%|        | 1525/10000 [50:53<3:25:36,  1.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1525 / 10000 | Loss: 1.794851303100586, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  16%|        | 1550/10000 [51:31<3:40:57,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1550 / 10000 | Loss: 1.8724091053009033, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  16%|        | 1575/10000 [52:10<3:39:34,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1575 / 10000 | Loss: 1.7663676738739014, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  16%|        | 1600/10000 [52:49<3:38:39,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1600 / 10000 | Loss: 1.776869773864746, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  16%|        | 1625/10000 [53:27<3:32:29,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1625 / 10000 | Loss: 1.6645762920379639, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  16%|        | 1650/10000 [54:06<3:31:57,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1650 / 10000 | Loss: 1.7806965112686157, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  17%|        | 1675/10000 [54:44<3:31:33,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1675 / 10000 | Loss: 1.8890093564987183, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  17%|        | 1700/10000 [55:22<3:30:51,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1700 / 10000 | Loss: 2.0756988525390625, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  17%|        | 1725/10000 [56:00<3:30:11,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1725 / 10000 | Loss: 1.9030003547668457, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  18%|        | 1750/10000 [56:38<3:29:37,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1750 / 10000 | Loss: 1.801339864730835, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  18%|        | 1775/10000 [57:16<3:29:06,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1775 / 10000 | Loss: 1.7519099712371826, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  18%|        | 1800/10000 [57:54<3:28:30,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1800 / 10000 | Loss: 1.5567169189453125, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  18%|        | 1825/10000 [58:32<3:27:39,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1825 / 10000 | Loss: 1.490767478942871, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  18%|        | 1850/10000 [59:10<3:27:05,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1850 / 10000 | Loss: 1.653874158859253, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  19%|        | 1875/10000 [59:49<3:26:25,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1875 / 10000 | Loss: 1.715132236480713, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  19%|        | 1900/10000 [1:00:27<3:25:34,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1900 / 10000 | Loss: 1.464047908782959, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  19%|        | 1925/10000 [1:01:25<3:13:58,  1.44s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1925 / 10000 | Loss: 1.507411003112793, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  20%|        | 1950/10000 [1:02:01<3:13:21,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1950 / 10000 | Loss: 1.4761583805084229, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  20%|        | 1975/10000 [1:02:38<3:26:51,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (1975 / 10000 | Loss: 1.305785894393921, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  20%|        | 2000/10000 [1:03:17<3:25:51,  1.54s/it]04/16/2024 19:30:03 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-2000-epoch-4\n",
      "04/16/2024 19:30:03 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2000 / 10000 | Loss: 1.5221607685089111, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 19:30:04 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-2000-epoch-4\\model.safetensors\n",
      "04/16/2024 19:30:04 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 19:30:06 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-2000-epoch-4\\model_1.safetensors\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-2000-epoch-4\\optimizer.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-2000-epoch-4\\scheduler.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler_1.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler_2.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler_3.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler_4.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler_5.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler_6.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-2000-epoch-4\\sampler_7.bin\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-2000-epoch-4\\scaler.pt\n",
      "04/16/2024 19:30:19 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-2000-epoch-4\\random_states_0.pkl\n",
      "04/16/2024 19:30:19 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-500-epoch-1] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:26<00:00,  1.54s/it]\n",
      "Train steps ... :  20%|        | 2000/10000 [1:06:59<3:25:51,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (2000 / 10000 | Eval Loss: 14.146730422973633 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  20%|        | 2025/10000 [1:07:35<3:13:25,  1.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2025 / 10000 | Loss: 1.5378689765930176, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  20%|        | 2050/10000 [1:08:12<3:18:24,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2050 / 10000 | Loss: 1.2470369338989258, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  21%|        | 2075/10000 [1:08:49<3:17:34,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2075 / 10000 | Loss: 1.437402606010437, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  21%|        | 2100/10000 [1:09:27<3:16:50,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2100 / 10000 | Loss: 1.5322054624557495, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  21%|       | 2125/10000 [1:10:04<3:16:10,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2125 / 10000 | Loss: 1.655663251876831, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  22%|       | 2150/10000 [1:10:41<3:15:38,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2150 / 10000 | Loss: 1.5854521989822388, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  22%|       | 2175/10000 [1:11:19<3:15:12,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2175 / 10000 | Loss: 1.4650038480758667, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  22%|       | 2200/10000 [1:11:56<3:14:32,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2200 / 10000 | Loss: 1.4120019674301147, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  22%|       | 2225/10000 [1:12:34<3:13:44,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2225 / 10000 | Loss: 1.4223930835723877, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  22%|       | 2250/10000 [1:13:11<3:13:03,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2250 / 10000 | Loss: 1.5009827613830566, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  23%|       | 2275/10000 [1:13:48<3:12:29,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2275 / 10000 | Loss: 1.3275995254516602, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  23%|       | 2300/10000 [1:14:26<3:12:05,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2300 / 10000 | Loss: 1.4047420024871826, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  23%|       | 2325/10000 [1:15:03<3:11:26,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2325 / 10000 | Loss: 1.2628144025802612, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  24%|       | 2350/10000 [1:15:41<3:10:47,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2350 / 10000 | Loss: 1.4573674201965332, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  24%|       | 2375/10000 [1:16:18<3:10:13,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2375 / 10000 | Loss: 1.3696706295013428, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  24%|       | 2400/10000 [1:17:16<3:05:18,  1.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2400 / 10000 | Loss: 1.2688615322113037, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  24%|       | 2425/10000 [1:17:52<3:01:56,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2425 / 10000 | Loss: 1.1484566926956177, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  24%|       | 2450/10000 [1:18:29<3:09:47,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2450 / 10000 | Loss: 1.2219007015228271, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  25%|       | 2475/10000 [1:19:07<3:11:35,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2475 / 10000 | Loss: 1.087132215499878, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  25%|       | 2500/10000 [1:19:45<3:10:51,  1.53s/it]04/16/2024 19:46:31 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-2500-epoch-5\n",
      "04/16/2024 19:46:31 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2500 / 10000 | Loss: 1.0255718231201172, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 19:46:32 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-2500-epoch-5\\model.safetensors\n",
      "04/16/2024 19:46:32 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 19:46:37 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-2500-epoch-5\\model_1.safetensors\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-2500-epoch-5\\optimizer.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-2500-epoch-5\\scheduler.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_1.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_2.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_3.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_4.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_5.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_6.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_7.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_8.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-2500-epoch-5\\sampler_9.bin\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-2500-epoch-5\\scaler.pt\n",
      "04/16/2024 19:46:51 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-2500-epoch-5\\random_states_0.pkl\n",
      "04/16/2024 19:46:51 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-1000-epoch-2] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:23<00:00,  1.52s/it]\n",
      "Train steps ... :  25%|       | 2500/10000 [1:23:28<3:10:51,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (2500 / 10000 | Eval Loss: 14.9074068069458 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  25%|       | 2525/10000 [1:24:04<3:01:12,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2525 / 10000 | Loss: 1.279223084449768, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  26%|       | 2550/10000 [1:24:42<3:09:26,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2550 / 10000 | Loss: 1.2829458713531494, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  26%|       | 2575/10000 [1:25:20<3:08:45,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2575 / 10000 | Loss: 1.0913742780685425, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  26%|       | 2600/10000 [1:25:58<3:08:02,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2600 / 10000 | Loss: 1.2561845779418945, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  26%|       | 2625/10000 [1:26:36<3:07:18,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2625 / 10000 | Loss: 1.3727985620498657, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  26%|       | 2650/10000 [1:27:15<3:06:42,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2650 / 10000 | Loss: 1.1092615127563477, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  27%|       | 2675/10000 [1:27:53<3:06:10,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2675 / 10000 | Loss: 1.2260215282440186, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  27%|       | 2700/10000 [1:28:31<3:05:30,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2700 / 10000 | Loss: 1.227380394935608, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  27%|       | 2725/10000 [1:29:09<3:04:58,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2725 / 10000 | Loss: 1.2677221298217773, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  28%|       | 2750/10000 [1:29:47<3:04:17,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2750 / 10000 | Loss: 1.2037323713302612, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  28%|       | 2775/10000 [1:30:25<3:03:25,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2775 / 10000 | Loss: 1.1574794054031372, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  28%|       | 2800/10000 [1:31:03<3:02:54,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2800 / 10000 | Loss: 1.192481517791748, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  28%|       | 2825/10000 [1:31:41<3:02:07,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2825 / 10000 | Loss: 1.2595548629760742, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  28%|       | 2850/10000 [1:32:19<3:01:31,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2850 / 10000 | Loss: 1.0622498989105225, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  29%|       | 2875/10000 [1:33:18<2:58:44,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2875 / 10000 | Loss: 0.9955295324325562, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  29%|       | 2900/10000 [1:33:54<2:50:28,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2900 / 10000 | Loss: 0.9700402021408081, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  29%|       | 2925/10000 [1:34:30<2:50:12,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2925 / 10000 | Loss: 1.0251113176345825, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  30%|       | 2950/10000 [1:35:08<2:59:38,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2950 / 10000 | Loss: 1.0812050104141235, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  30%|       | 2975/10000 [1:35:46<2:58:48,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (2975 / 10000 | Loss: 1.224587082862854, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  30%|       | 3000/10000 [1:36:24<2:58:14,  1.53s/it]04/16/2024 20:03:11 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-3000-epoch-6\n",
      "04/16/2024 20:03:11 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3000 / 10000 | Loss: 1.1605541706085205, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 20:03:12 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-3000-epoch-6\\model.safetensors\n",
      "04/16/2024 20:03:12 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 20:03:16 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-3000-epoch-6\\model_1.safetensors\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-3000-epoch-6\\optimizer.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-3000-epoch-6\\scheduler.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_1.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_2.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_3.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_4.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_5.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_6.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_7.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_8.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_9.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_10.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-3000-epoch-6\\sampler_11.bin\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-3000-epoch-6\\scaler.pt\n",
      "04/16/2024 20:03:35 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-3000-epoch-6\\random_states_0.pkl\n",
      "04/16/2024 20:03:35 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-1500-epoch-3] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:24<00:00,  1.53s/it]\n",
      "Train steps ... :  30%|       | 3000/10000 [1:40:13<2:58:14,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (3000 / 10000 | Eval Loss: 14.784587860107422 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  30%|       | 3025/10000 [1:40:49<2:48:51,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3025 / 10000 | Loss: 1.0440621376037598, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  30%|       | 3050/10000 [1:41:26<2:58:02,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3050 / 10000 | Loss: 1.1782186031341553, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  31%|       | 3075/10000 [1:42:05<2:58:06,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3075 / 10000 | Loss: 0.9574680924415588, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  31%|       | 3100/10000 [1:42:43<2:57:12,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3100 / 10000 | Loss: 1.0533753633499146, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  31%|      | 3125/10000 [1:43:22<2:54:34,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3125 / 10000 | Loss: 1.1054526567459106, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  32%|      | 3150/10000 [1:44:00<2:51:56,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3150 / 10000 | Loss: 1.088787317276001, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  32%|      | 3175/10000 [1:44:37<2:51:21,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3175 / 10000 | Loss: 1.0901107788085938, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  32%|      | 3200/10000 [1:45:15<2:50:49,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3200 / 10000 | Loss: 1.047781229019165, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  32%|      | 3225/10000 [1:45:53<2:50:10,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3225 / 10000 | Loss: 1.0538549423217773, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  32%|      | 3250/10000 [1:46:30<2:49:39,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3250 / 10000 | Loss: 0.9808436036109924, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  33%|      | 3275/10000 [1:47:08<2:48:53,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3275 / 10000 | Loss: 0.8413092494010925, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  33%|      | 3300/10000 [1:47:46<2:48:18,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3300 / 10000 | Loss: 0.9754841327667236, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  33%|      | 3325/10000 [1:48:23<2:47:46,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3325 / 10000 | Loss: 0.9546835422515869, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  34%|      | 3350/10000 [1:49:22<2:57:13,  1.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3350 / 10000 | Loss: 1.1865653991699219, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  34%|      | 3375/10000 [1:49:58<2:38:53,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3375 / 10000 | Loss: 0.850554883480072, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  34%|      | 3400/10000 [1:50:34<2:38:41,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3400 / 10000 | Loss: 0.8490664958953857, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  34%|      | 3425/10000 [1:51:11<2:50:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3425 / 10000 | Loss: 0.8785192966461182, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  34%|      | 3450/10000 [1:51:50<2:49:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3450 / 10000 | Loss: 0.9383096694946289, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  35%|      | 3475/10000 [1:52:29<2:47:09,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3475 / 10000 | Loss: 0.9714935421943665, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  35%|      | 3500/10000 [1:53:07<2:45:32,  1.53s/it]04/16/2024 20:19:53 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-3500-epoch-7\n",
      "04/16/2024 20:19:53 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3500 / 10000 | Loss: 0.9968298077583313, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 20:19:54 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-3500-epoch-7\\model.safetensors\n",
      "04/16/2024 20:19:54 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 20:20:01 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-3500-epoch-7\\model_1.safetensors\n",
      "04/16/2024 20:20:18 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-3500-epoch-7\\optimizer.bin\n",
      "04/16/2024 20:20:18 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-3500-epoch-7\\scheduler.bin\n",
      "04/16/2024 20:20:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler.bin\n",
      "04/16/2024 20:20:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_1.bin\n",
      "04/16/2024 20:20:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_2.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_3.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_4.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_5.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_6.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_7.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_8.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_9.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_10.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_11.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_12.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-3500-epoch-7\\sampler_13.bin\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-3500-epoch-7\\scaler.pt\n",
      "04/16/2024 20:20:19 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-3500-epoch-7\\random_states_0.pkl\n",
      "04/16/2024 20:20:19 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-2000-epoch-4] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:22<00:00,  1.51s/it]\n",
      "Train steps ... :  35%|      | 3500/10000 [1:56:55<2:45:32,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (3500 / 10000 | Eval Loss: 14.765580177307129 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  35%|      | 3525/10000 [1:57:31<2:36:56,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3525 / 10000 | Loss: 0.903105616569519, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  36%|      | 3550/10000 [1:58:09<2:43:41,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3550 / 10000 | Loss: 0.9290660619735718, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  36%|      | 3575/10000 [1:58:47<2:42:52,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3575 / 10000 | Loss: 0.7536381483078003, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  36%|      | 3600/10000 [1:59:25<2:42:03,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3600 / 10000 | Loss: 0.9158504009246826, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  36%|      | 3625/10000 [2:00:03<2:41:24,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3625 / 10000 | Loss: 0.9639959335327148, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  36%|      | 3650/10000 [2:00:41<2:40:53,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3650 / 10000 | Loss: 0.9533877968788147, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  37%|      | 3675/10000 [2:01:19<2:40:12,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3675 / 10000 | Loss: 0.920609176158905, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  37%|      | 3700/10000 [2:01:57<2:39:35,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3700 / 10000 | Loss: 1.0062103271484375, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  37%|      | 3725/10000 [2:02:35<2:38:55,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3725 / 10000 | Loss: 0.8131766319274902, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  38%|      | 3750/10000 [2:03:13<2:38:14,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3750 / 10000 | Loss: 0.8771234154701233, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  38%|      | 3775/10000 [2:03:51<2:37:34,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3775 / 10000 | Loss: 0.8857176303863525, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  38%|      | 3800/10000 [2:04:29<2:36:58,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3800 / 10000 | Loss: 0.925186038017273, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  38%|      | 3825/10000 [2:05:28<3:04:19,  1.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3825 / 10000 | Loss: 0.7774158120155334, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  38%|      | 3850/10000 [2:06:03<2:27:12,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3850 / 10000 | Loss: 0.8595274686813354, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  39%|      | 3875/10000 [2:06:39<2:27:11,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3875 / 10000 | Loss: 0.8709777593612671, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  39%|      | 3900/10000 [2:07:16<2:29:32,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3900 / 10000 | Loss: 0.7876337170600891, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  39%|      | 3925/10000 [2:07:52<2:28:52,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3925 / 10000 | Loss: 0.8119494915008545, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  40%|      | 3950/10000 [2:08:29<2:28:21,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3950 / 10000 | Loss: 0.8397389054298401, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  40%|      | 3975/10000 [2:09:06<2:27:41,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (3975 / 10000 | Loss: 0.8379313945770264, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  40%|      | 4000/10000 [2:09:43<2:27:04,  1.47s/it]04/16/2024 20:36:29 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-4000-epoch-8\n",
      "04/16/2024 20:36:29 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4000 / 10000 | Loss: 0.936780571937561, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 20:36:30 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-4000-epoch-8\\model.safetensors\n",
      "04/16/2024 20:36:30 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 20:36:36 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-4000-epoch-8\\model_1.safetensors\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-4000-epoch-8\\optimizer.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-4000-epoch-8\\scheduler.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_1.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_2.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_3.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_4.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_5.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_6.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_7.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_8.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_9.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_10.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_11.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_12.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_13.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_14.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-4000-epoch-8\\sampler_15.bin\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-4000-epoch-8\\scaler.pt\n",
      "04/16/2024 20:36:54 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-4000-epoch-8\\random_states_0.pkl\n",
      "04/16/2024 20:36:54 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-2500-epoch-5] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:21<00:00,  1.50s/it]\n",
      "Train steps ... :  40%|      | 4000/10000 [2:13:30<2:27:04,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (4000 / 10000 | Eval Loss: 14.16757869720459 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  40%|      | 4025/10000 [2:14:06<2:24:47,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4025 / 10000 | Loss: 0.8058812618255615, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  40%|      | 4050/10000 [2:14:42<2:26:26,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4050 / 10000 | Loss: 0.8528374433517456, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  41%|      | 4075/10000 [2:15:19<2:25:51,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4075 / 10000 | Loss: 0.7910724878311157, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  41%|      | 4100/10000 [2:15:56<2:25:20,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4100 / 10000 | Loss: 0.7747631669044495, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  41%|     | 4125/10000 [2:16:33<2:24:44,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4125 / 10000 | Loss: 0.8412415981292725, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  42%|     | 4150/10000 [2:17:10<2:24:08,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4150 / 10000 | Loss: 0.8473352193832397, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  42%|     | 4175/10000 [2:17:47<2:25:18,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4175 / 10000 | Loss: 0.8297067284584045, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  42%|     | 4200/10000 [2:18:24<2:23:17,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4200 / 10000 | Loss: 0.7950937747955322, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  42%|     | 4225/10000 [2:19:01<2:22:09,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4225 / 10000 | Loss: 0.7986474633216858, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  42%|     | 4250/10000 [2:19:38<2:22:02,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4250 / 10000 | Loss: 0.8603546619415283, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  43%|     | 4275/10000 [2:20:15<2:20:59,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4275 / 10000 | Loss: 0.9232803583145142, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  43%|     | 4300/10000 [2:21:14<3:27:53,  2.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4300 / 10000 | Loss: 0.7510573267936707, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  43%|     | 4325/10000 [2:21:49<2:15:47,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4325 / 10000 | Loss: 0.7617058753967285, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  44%|     | 4350/10000 [2:22:26<2:15:44,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4350 / 10000 | Loss: 0.7894378900527954, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  44%|     | 4375/10000 [2:23:03<2:21:40,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4375 / 10000 | Loss: 0.7702402472496033, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  44%|     | 4400/10000 [2:23:40<2:20:47,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4400 / 10000 | Loss: 0.7636687159538269, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  44%|     | 4425/10000 [2:24:18<2:20:03,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4425 / 10000 | Loss: 0.7352432012557983, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  44%|     | 4450/10000 [2:24:56<2:19:29,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4450 / 10000 | Loss: 0.7524254322052002, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  45%|     | 4475/10000 [2:25:33<2:18:45,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4475 / 10000 | Loss: 0.7861554622650146, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  45%|     | 4500/10000 [2:26:11<2:18:10,  1.51s/it]04/16/2024 20:52:57 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-4500-epoch-9\n",
      "04/16/2024 20:52:57 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4500 / 10000 | Loss: 0.7518160939216614, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 20:53:00 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-4500-epoch-9\\model.safetensors\n",
      "04/16/2024 20:53:00 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 20:53:07 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-4500-epoch-9\\model_1.safetensors\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-4500-epoch-9\\optimizer.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-4500-epoch-9\\scheduler.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_1.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_2.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_3.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_4.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_5.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_6.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_7.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_8.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_9.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_10.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_11.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_12.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_13.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_14.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_15.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_16.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-4500-epoch-9\\sampler_17.bin\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-4500-epoch-9\\scaler.pt\n",
      "04/16/2024 20:53:16 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-4500-epoch-9\\random_states_0.pkl\n",
      "04/16/2024 20:53:16 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-3000-epoch-6] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:24<00:00,  1.52s/it]\n",
      "Train steps ... :  45%|     | 4500/10000 [2:29:54<2:18:10,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (4500 / 10000 | Eval Loss: 14.356171607971191 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  45%|     | 4525/10000 [2:30:30<2:12:30,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4525 / 10000 | Loss: 0.7185930013656616, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  46%|     | 4550/10000 [2:31:06<2:13:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4550 / 10000 | Loss: 0.7366095781326294, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  46%|     | 4575/10000 [2:31:43<2:12:44,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4575 / 10000 | Loss: 0.6935442686080933, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  46%|     | 4600/10000 [2:32:20<2:12:08,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4600 / 10000 | Loss: 0.7947193384170532, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  46%|     | 4625/10000 [2:32:57<2:11:36,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4625 / 10000 | Loss: 0.7882838249206543, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  46%|     | 4650/10000 [2:33:34<2:15:50,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4650 / 10000 | Loss: 0.7526445984840393, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  47%|     | 4675/10000 [2:34:12<2:15:11,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4675 / 10000 | Loss: 0.6984368562698364, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  47%|     | 4700/10000 [2:34:50<2:14:25,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4700 / 10000 | Loss: 0.7653207182884216, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  47%|     | 4725/10000 [2:35:28<2:13:35,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4725 / 10000 | Loss: 0.7532860040664673, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  48%|     | 4750/10000 [2:36:06<2:12:59,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4750 / 10000 | Loss: 0.7651711702346802, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  48%|     | 4775/10000 [2:37:06<4:21:51,  3.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4775 / 10000 | Loss: 0.655874490737915, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  48%|     | 4800/10000 [2:37:41<2:03:49,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4800 / 10000 | Loss: 0.6444344520568848, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  48%|     | 4825/10000 [2:38:17<2:04:09,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4825 / 10000 | Loss: 0.7846433520317078, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  48%|     | 4850/10000 [2:38:54<2:11:43,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4850 / 10000 | Loss: 0.6665394306182861, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  49%|     | 4875/10000 [2:39:33<2:14:24,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4875 / 10000 | Loss: 0.6186251044273376, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  49%|     | 4900/10000 [2:40:12<2:08:34,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4900 / 10000 | Loss: 0.7342714667320251, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  49%|     | 4925/10000 [2:40:50<2:07:57,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4925 / 10000 | Loss: 0.6939190626144409, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  50%|     | 4950/10000 [2:41:27<2:07:17,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4950 / 10000 | Loss: 0.6485622525215149, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  50%|     | 4975/10000 [2:42:05<2:06:41,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (4975 / 10000 | Loss: 0.6616478562355042, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  50%|     | 5000/10000 [2:42:43<2:06:18,  1.52s/it]04/16/2024 21:09:29 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-5000-epoch-10\n",
      "04/16/2024 21:09:29 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5000 / 10000 | Loss: 0.6552691459655762, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 21:09:31 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-5000-epoch-10\\model.safetensors\n",
      "04/16/2024 21:09:31 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 21:09:32 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-5000-epoch-10\\model_1.safetensors\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-5000-epoch-10\\optimizer.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-5000-epoch-10\\scheduler.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_1.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_2.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_3.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_4.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_5.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_6.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_7.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_8.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_9.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_10.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_11.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_12.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_13.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_14.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_15.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_16.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_17.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_18.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-5000-epoch-10\\sampler_19.bin\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-5000-epoch-10\\scaler.pt\n",
      "04/16/2024 21:09:35 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-5000-epoch-10\\random_states_0.pkl\n",
      "04/16/2024 21:09:35 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-3500-epoch-7] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:25<00:00,  1.53s/it]\n",
      "Train steps ... :  50%|     | 5000/10000 [2:46:14<2:06:18,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (5000 / 10000 | Eval Loss: 15.19997787475586 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  50%|     | 5025/10000 [2:46:50<2:00:22,  1.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5025 / 10000 | Loss: 0.6529499888420105, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  50%|     | 5050/10000 [2:47:27<2:07:59,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5050 / 10000 | Loss: 0.7441715002059937, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  51%|     | 5075/10000 [2:48:06<2:08:39,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5075 / 10000 | Loss: 0.6655868291854858, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  51%|     | 5100/10000 [2:48:45<2:03:37,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5100 / 10000 | Loss: 0.7358751893043518, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  51%|    | 5125/10000 [2:49:23<2:02:33,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5125 / 10000 | Loss: 0.704825758934021, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  52%|    | 5150/10000 [2:50:01<2:01:58,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5150 / 10000 | Loss: 0.6557352542877197, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  52%|    | 5175/10000 [2:50:38<2:01:18,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5175 / 10000 | Loss: 0.69881671667099, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  52%|    | 5200/10000 [2:51:16<2:00:39,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5200 / 10000 | Loss: 0.612594485282898, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  52%|    | 5225/10000 [2:51:54<2:00:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5225 / 10000 | Loss: 0.683363676071167, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  52%|    | 5250/10000 [2:52:53<6:12:01,  4.70s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5250 / 10000 | Loss: 0.574798583984375, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  53%|    | 5275/10000 [2:53:29<1:52:28,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5275 / 10000 | Loss: 0.643438994884491, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  53%|    | 5300/10000 [2:54:05<1:52:54,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5300 / 10000 | Loss: 0.6108800172805786, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  53%|    | 5325/10000 [2:54:42<1:58:31,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5325 / 10000 | Loss: 0.6236873865127563, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  54%|    | 5350/10000 [2:55:20<1:58:09,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5350 / 10000 | Loss: 0.6362641453742981, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  54%|    | 5375/10000 [2:55:58<1:57:22,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5375 / 10000 | Loss: 0.6114572286605835, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  54%|    | 5400/10000 [2:56:36<1:56:44,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5400 / 10000 | Loss: 0.6526699066162109, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  54%|    | 5425/10000 [2:57:14<1:56:02,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5425 / 10000 | Loss: 0.6219083070755005, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  55%|    | 5450/10000 [2:57:52<1:55:28,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5450 / 10000 | Loss: 0.7340532541275024, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  55%|    | 5475/10000 [2:58:30<1:54:44,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5475 / 10000 | Loss: 0.6128401160240173, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  55%|    | 5500/10000 [2:59:08<1:54:05,  1.52s/it]04/16/2024 21:25:54 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-5500-epoch-11\n",
      "04/16/2024 21:25:54 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5500 / 10000 | Loss: 0.6964160203933716, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 21:25:55 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-5500-epoch-11\\model.safetensors\n",
      "04/16/2024 21:25:55 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 21:25:57 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-5500-epoch-11\\model_1.safetensors\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-5500-epoch-11\\optimizer.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-5500-epoch-11\\scheduler.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_1.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_2.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_3.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_4.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_5.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_6.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_7.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_8.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_9.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_10.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_11.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_12.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_13.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_14.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_15.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_16.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_17.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_18.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_19.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_20.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-5500-epoch-11\\sampler_21.bin\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-5500-epoch-11\\scaler.pt\n",
      "04/16/2024 21:25:59 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-5500-epoch-11\\random_states_0.pkl\n",
      "04/16/2024 21:25:59 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-4000-epoch-8] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:24<00:00,  1.52s/it]\n",
      "Train steps ... :  55%|    | 5500/10000 [3:02:38<1:54:05,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (5500 / 10000 | Eval Loss: 14.796135902404785 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  55%|    | 5525/10000 [3:03:14<1:48:19,  1.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5525 / 10000 | Loss: 0.6352732181549072, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  56%|    | 5550/10000 [3:03:50<1:50:57,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5550 / 10000 | Loss: 0.6271026134490967, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  56%|    | 5575/10000 [3:04:28<1:50:43,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5575 / 10000 | Loss: 0.6109451055526733, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  56%|    | 5600/10000 [3:05:05<1:49:54,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5600 / 10000 | Loss: 0.6455128788948059, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  56%|    | 5625/10000 [3:05:42<1:49:22,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5625 / 10000 | Loss: 0.662844717502594, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  56%|    | 5650/10000 [3:06:20<1:49:50,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5650 / 10000 | Loss: 0.6118952035903931, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  57%|    | 5675/10000 [3:06:58<1:46:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5675 / 10000 | Loss: 0.6494833827018738, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  57%|    | 5700/10000 [3:07:35<1:44:43,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5700 / 10000 | Loss: 0.6636185050010681, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  57%|    | 5725/10000 [3:08:33<9:33:36,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5725 / 10000 | Loss: 0.6414217948913574, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  57%|    | 5750/10000 [3:09:08<1:40:31,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5750 / 10000 | Loss: 0.6309968829154968, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  58%|    | 5775/10000 [3:09:44<1:40:47,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5775 / 10000 | Loss: 0.5911917686462402, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  58%|    | 5800/10000 [3:10:21<1:43:51,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5800 / 10000 | Loss: 0.5881272554397583, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  58%|    | 5825/10000 [3:10:58<1:43:25,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5825 / 10000 | Loss: 0.6264157891273499, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  58%|    | 5850/10000 [3:11:35<1:42:35,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5850 / 10000 | Loss: 0.6356358528137207, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  59%|    | 5875/10000 [3:12:12<1:42:29,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5875 / 10000 | Loss: 0.5492143630981445, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  59%|    | 5900/10000 [3:12:49<1:41:24,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5900 / 10000 | Loss: 0.5463218688964844, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  59%|    | 5925/10000 [3:13:26<1:40:58,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5925 / 10000 | Loss: 0.5819334387779236, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  60%|    | 5950/10000 [3:14:04<1:42:04,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5950 / 10000 | Loss: 0.642201840877533, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  60%|    | 5975/10000 [3:14:43<1:47:07,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (5975 / 10000 | Loss: 0.6114391684532166, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  60%|    | 6000/10000 [3:15:21<1:39:33,  1.49s/it]04/16/2024 21:42:08 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-6000-epoch-12\n",
      "04/16/2024 21:42:08 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6000 / 10000 | Loss: 0.6022511124610901, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 21:42:09 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-6000-epoch-12\\model.safetensors\n",
      "04/16/2024 21:42:09 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 21:42:11 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-6000-epoch-12\\model_1.safetensors\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-6000-epoch-12\\optimizer.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-6000-epoch-12\\scheduler.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_1.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_2.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_3.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_4.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_5.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_6.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_7.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_8.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_9.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_10.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_11.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_12.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_13.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_14.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_15.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_16.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_17.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_18.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_19.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_20.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_21.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_22.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-6000-epoch-12\\sampler_23.bin\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-6000-epoch-12\\scaler.pt\n",
      "04/16/2024 21:42:21 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-6000-epoch-12\\random_states_0.pkl\n",
      "04/16/2024 21:42:21 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-4500-epoch-9] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:29<00:00,  1.56s/it]\n",
      "Train steps ... :  60%|    | 6000/10000 [3:19:04<1:39:33,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (6000 / 10000 | Eval Loss: 14.663985252380371 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  60%|    | 6025/10000 [3:19:41<1:41:37,  1.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6025 / 10000 | Loss: 0.6088269948959351, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  60%|    | 6050/10000 [3:20:20<1:41:53,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6050 / 10000 | Loss: 0.5838919281959534, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  61%|    | 6075/10000 [3:20:58<1:39:42,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6075 / 10000 | Loss: 0.5462437272071838, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  61%|    | 6100/10000 [3:21:36<1:39:09,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6100 / 10000 | Loss: 0.6410101056098938, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  61%|   | 6125/10000 [3:22:15<1:39:23,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6125 / 10000 | Loss: 0.6393176317214966, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  62%|   | 6150/10000 [3:22:53<1:40:35,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6150 / 10000 | Loss: 0.579779326915741, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  62%|   | 6175/10000 [3:23:32<1:39:11,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6175 / 10000 | Loss: 0.5672785043716431, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  62%|   | 6200/10000 [3:24:11<1:39:37,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6200 / 10000 | Loss: 0.5750563144683838, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  62%|   | 6225/10000 [3:25:12<1:31:01,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6225 / 10000 | Loss: 0.5521154999732971, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  62%|   | 6250/10000 [3:25:48<1:29:40,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6250 / 10000 | Loss: 0.5345958471298218, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  63%|   | 6275/10000 [3:26:29<1:38:39,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6275 / 10000 | Loss: 0.5465453863143921, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  63%|   | 6300/10000 [3:27:08<1:37:26,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6300 / 10000 | Loss: 0.5394601225852966, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  63%|   | 6325/10000 [3:27:47<1:35:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6325 / 10000 | Loss: 0.541936993598938, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  64%|   | 6350/10000 [3:28:25<1:33:24,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6350 / 10000 | Loss: 0.5430051684379578, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  64%|   | 6375/10000 [3:29:04<1:32:42,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6375 / 10000 | Loss: 0.5589258670806885, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  64%|   | 6400/10000 [3:29:43<1:34:09,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6400 / 10000 | Loss: 0.5740880370140076, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  64%|   | 6425/10000 [3:30:22<1:31:11,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6425 / 10000 | Loss: 0.6033069491386414, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  64%|   | 6450/10000 [3:31:00<1:32:55,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6450 / 10000 | Loss: 0.5584529042243958, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  65%|   | 6475/10000 [3:31:39<1:31:09,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6475 / 10000 | Loss: 0.584836483001709, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  65%|   | 6500/10000 [3:32:18<1:29:52,  1.54s/it]04/16/2024 21:59:04 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-6500-epoch-13\n",
      "04/16/2024 21:59:04 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6500 / 10000 | Loss: 0.5697060227394104, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 21:59:06 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-6500-epoch-13\\model.safetensors\n",
      "04/16/2024 21:59:06 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 21:59:08 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-6500-epoch-13\\model_1.safetensors\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-6500-epoch-13\\optimizer.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-6500-epoch-13\\scheduler.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_1.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_2.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_3.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_4.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_5.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_6.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_7.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_8.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_9.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_10.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_11.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_12.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_13.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_14.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_15.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_16.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_17.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_18.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_19.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_20.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_21.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_22.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_23.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_24.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-6500-epoch-13\\sampler_25.bin\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-6500-epoch-13\\scaler.pt\n",
      "04/16/2024 21:59:09 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-6500-epoch-13\\random_states_0.pkl\n",
      "04/16/2024 21:59:09 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-5000-epoch-10] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:29<00:00,  1.56s/it]\n",
      "Train steps ... :  65%|   | 6500/10000 [3:35:53<1:29:52,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (6500 / 10000 | Eval Loss: 14.881711959838867 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  65%|   | 6525/10000 [3:36:28<1:26:23,  1.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6525 / 10000 | Loss: 0.592155933380127, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  66%|   | 6550/10000 [3:37:06<1:25:59,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6550 / 10000 | Loss: 0.5873292088508606, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  66%|   | 6575/10000 [3:37:43<1:26:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6575 / 10000 | Loss: 0.5787099599838257, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  66%|   | 6600/10000 [3:38:21<1:25:18,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6600 / 10000 | Loss: 0.5700242519378662, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  66%|   | 6625/10000 [3:38:59<1:24:44,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6625 / 10000 | Loss: 0.5789681673049927, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  66%|   | 6650/10000 [3:39:36<1:24:06,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6650 / 10000 | Loss: 0.5576483607292175, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  67%|   | 6675/10000 [3:40:14<1:23:22,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6675 / 10000 | Loss: 0.6013653874397278, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  67%|   | 6700/10000 [3:41:12<1:17:56,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6700 / 10000 | Loss: 0.5277141332626343, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  67%|   | 6725/10000 [3:41:48<1:18:13,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6725 / 10000 | Loss: 0.4868677258491516, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  68%|   | 6750/10000 [3:42:27<1:24:25,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6750 / 10000 | Loss: 0.5392006039619446, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  68%|   | 6775/10000 [3:43:06<1:23:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6775 / 10000 | Loss: 0.5115161538124084, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  68%|   | 6800/10000 [3:43:45<1:22:25,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6800 / 10000 | Loss: 0.5402982234954834, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  68%|   | 6825/10000 [3:44:23<1:21:58,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6825 / 10000 | Loss: 0.5418921709060669, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  68%|   | 6850/10000 [3:45:02<1:21:20,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6850 / 10000 | Loss: 0.5770591497421265, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  69%|   | 6875/10000 [3:45:41<1:20:35,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6875 / 10000 | Loss: 0.5391871929168701, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  69%|   | 6900/10000 [3:46:20<1:20:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6900 / 10000 | Loss: 0.5133776664733887, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  69%|   | 6925/10000 [3:46:58<1:19:14,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6925 / 10000 | Loss: 0.5230666995048523, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  70%|   | 6950/10000 [3:47:37<1:18:37,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6950 / 10000 | Loss: 0.5629994869232178, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  70%|   | 6975/10000 [3:48:17<1:20:39,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (6975 / 10000 | Loss: 0.5304768085479736, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  70%|   | 7000/10000 [3:48:56<1:19:47,  1.60s/it]04/16/2024 22:15:42 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-7000-epoch-14\n",
      "04/16/2024 22:15:42 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7000 / 10000 | Loss: 0.49639570713043213, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 22:15:43 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-7000-epoch-14\\model.safetensors\n",
      "04/16/2024 22:15:43 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 22:15:45 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-7000-epoch-14\\model_1.safetensors\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-7000-epoch-14\\optimizer.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-7000-epoch-14\\scheduler.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_1.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_2.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_3.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_4.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_5.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_6.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_7.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_8.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_9.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_10.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_11.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_12.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_13.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_14.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_15.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_16.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_17.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_18.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_19.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_20.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_21.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_22.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_23.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_24.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_25.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 26 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_26.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 27 saved in result_distiling_3\\checkpoint-7000-epoch-14\\sampler_27.bin\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-7000-epoch-14\\scaler.pt\n",
      "04/16/2024 22:15:46 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-7000-epoch-14\\random_states_0.pkl\n",
      "04/16/2024 22:15:46 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-5500-epoch-11] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:36<00:00,  1.61s/it]\n",
      "Train steps ... :  70%|   | 7000/10000 [3:52:36<1:19:47,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (7000 / 10000 | Eval Loss: 15.121490478515625 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  70%|   | 7025/10000 [3:53:12<1:16:08,  1.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7025 / 10000 | Loss: 0.5372777581214905, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  70%|   | 7050/10000 [3:53:51<1:18:04,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7050 / 10000 | Loss: 0.5330546498298645, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  71%|   | 7075/10000 [3:54:31<1:19:08,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7075 / 10000 | Loss: 0.5132157206535339, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  71%|   | 7100/10000 [3:55:14<1:23:50,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7100 / 10000 | Loss: 0.5386698246002197, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  71%|  | 7125/10000 [3:55:57<1:19:20,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7125 / 10000 | Loss: 0.5160777568817139, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  72%|  | 7150/10000 [3:56:39<1:19:32,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7150 / 10000 | Loss: 0.511840283870697, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  72%|  | 7175/10000 [3:57:42<1:13:44,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7175 / 10000 | Loss: 0.4980394244194031, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  72%|  | 7200/10000 [3:58:21<1:12:22,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7200 / 10000 | Loss: 0.5521352887153625, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  72%|  | 7225/10000 [3:59:00<1:14:01,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7225 / 10000 | Loss: 0.5023045539855957, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  72%|  | 7250/10000 [3:59:41<1:15:56,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7250 / 10000 | Loss: 0.5199245810508728, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  73%|  | 7275/10000 [4:00:21<1:13:07,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7275 / 10000 | Loss: 0.47919607162475586, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  73%|  | 7300/10000 [4:01:01<1:14:04,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7300 / 10000 | Loss: 0.4774186611175537, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  73%|  | 7325/10000 [4:01:44<1:10:45,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7325 / 10000 | Loss: 0.4914891719818115, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  74%|  | 7350/10000 [4:02:24<1:13:40,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7350 / 10000 | Loss: 0.5357663631439209, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  74%|  | 7375/10000 [4:03:05<1:08:21,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7375 / 10000 | Loss: 0.5066041350364685, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  74%|  | 7400/10000 [4:03:44<1:07:21,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7400 / 10000 | Loss: 0.5128340721130371, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  74%|  | 7425/10000 [4:04:24<1:11:13,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7425 / 10000 | Loss: 0.4762367606163025, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  74%|  | 7450/10000 [4:05:05<1:04:50,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7450 / 10000 | Loss: 0.5543513298034668, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  75%|  | 7475/10000 [4:05:45<1:09:01,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7475 / 10000 | Loss: 0.479919970035553, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  75%|  | 7500/10000 [4:06:24<1:03:15,  1.52s/it]04/16/2024 22:33:10 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-7500-epoch-15\n",
      "04/16/2024 22:33:10 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7500 / 10000 | Loss: 0.5268158316612244, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 22:33:11 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-7500-epoch-15\\model.safetensors\n",
      "04/16/2024 22:33:11 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-7500-epoch-15\\model_1.safetensors\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-7500-epoch-15\\optimizer.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-7500-epoch-15\\scheduler.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_1.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_2.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_3.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_4.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_5.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_6.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_7.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_8.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_9.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_10.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_11.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_12.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_13.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_14.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_15.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_16.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_17.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_18.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_19.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_20.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_21.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_22.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_23.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_24.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_25.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 26 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_26.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 27 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_27.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 28 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_28.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 29 saved in result_distiling_3\\checkpoint-7500-epoch-15\\sampler_29.bin\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-7500-epoch-15\\scaler.pt\n",
      "04/16/2024 22:33:13 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-7500-epoch-15\\random_states_0.pkl\n",
      "04/16/2024 22:33:13 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-6000-epoch-12] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:42<00:00,  1.66s/it]\n",
      "Train steps ... :  75%|  | 7500/10000 [4:10:10<1:03:15,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (7500 / 10000 | Eval Loss: 15.018406867980957 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  75%|  | 7525/10000 [4:10:49<1:06:17,  1.61s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7525 / 10000 | Loss: 0.4992452561855316, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  76%|  | 7550/10000 [4:11:32<1:10:24,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7550 / 10000 | Loss: 0.5220807790756226, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  76%|  | 7575/10000 [4:12:15<1:07:16,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7575 / 10000 | Loss: 0.49024367332458496, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  76%|  | 7600/10000 [4:12:57<1:06:04,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7600 / 10000 | Loss: 0.5279253721237183, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  76%|  | 7625/10000 [4:13:38<1:05:02,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7625 / 10000 | Loss: 0.4854644536972046, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  76%|  | 7650/10000 [4:14:42<1:01:17,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7650 / 10000 | Loss: 0.4892787039279938, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  77%|  | 7675/10000 [4:15:19<1:01:13,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7675 / 10000 | Loss: 0.502271294593811, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  77%|  | 7700/10000 [4:15:59<1:02:45,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7700 / 10000 | Loss: 0.4789867699146271, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  77%|  | 7725/10000 [4:16:41<1:04:22,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7725 / 10000 | Loss: 0.48890581727027893, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  78%|  | 7750/10000 [4:17:23<1:02:47,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7750 / 10000 | Loss: 0.4746769070625305, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  78%|  | 7775/10000 [4:18:04<1:00:45,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7775 / 10000 | Loss: 0.5439116358757019, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  78%|  | 7800/10000 [4:18:45<1:00:18,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7800 / 10000 | Loss: 0.5045899152755737, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  78%|  | 7825/10000 [4:19:27<59:34,  1.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7825 / 10000 | Loss: 0.498951256275177, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  78%|  | 7850/10000 [4:20:08<1:01:15,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7850 / 10000 | Loss: 0.4701685607433319, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  79%|  | 7875/10000 [4:20:50<57:52,  1.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7875 / 10000 | Loss: 0.4618570804595947, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  79%|  | 7900/10000 [4:21:31<56:57,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7900 / 10000 | Loss: 0.4763807952404022, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  79%|  | 7925/10000 [4:22:12<57:32,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7925 / 10000 | Loss: 0.48719286918640137, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  80%|  | 7950/10000 [4:22:53<55:46,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7950 / 10000 | Loss: 0.5045497417449951, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  80%|  | 7975/10000 [4:23:34<54:56,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (7975 / 10000 | Loss: 0.47851377725601196, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  80%|  | 8000/10000 [4:24:15<56:36,  1.70s/it]04/16/2024 22:51:01 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-8000-epoch-16\n",
      "04/16/2024 22:51:01 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8000 / 10000 | Loss: 0.47411203384399414, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 22:51:03 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-8000-epoch-16\\model.safetensors\n",
      "04/16/2024 22:51:03 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 22:51:04 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-8000-epoch-16\\model_1.safetensors\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-8000-epoch-16\\optimizer.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-8000-epoch-16\\scheduler.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_1.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_2.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_3.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_4.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_5.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_6.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_7.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_8.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_9.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_10.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_11.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_12.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_13.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_14.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_15.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_16.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_17.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_18.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_19.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_20.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_21.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_22.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_23.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_24.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_25.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 26 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_26.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 27 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_27.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 28 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_28.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 29 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_29.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 30 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_30.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 31 saved in result_distiling_3\\checkpoint-8000-epoch-16\\sampler_31.bin\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-8000-epoch-16\\scaler.pt\n",
      "04/16/2024 22:51:05 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-8000-epoch-16\\random_states_0.pkl\n",
      "04/16/2024 22:51:05 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-6500-epoch-13] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:42<00:00,  1.66s/it]\n",
      "Train steps ... :  80%|  | 8000/10000 [4:28:01<56:36,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (8000 / 10000 | Eval Loss: 14.90263843536377 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  80%|  | 8025/10000 [4:28:40<53:50,  1.64s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8025 / 10000 | Loss: 0.4853608012199402, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  80%|  | 8050/10000 [4:29:22<54:03,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8050 / 10000 | Loss: 0.48399800062179565, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  81%|  | 8075/10000 [4:30:03<54:29,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8075 / 10000 | Loss: 0.4982725977897644, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  81%|  | 8100/10000 [4:30:42<49:49,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8100 / 10000 | Loss: 0.459185391664505, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  81%| | 8125/10000 [4:31:45<47:00,  1.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8125 / 10000 | Loss: 0.4744066596031189, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  82%| | 8150/10000 [4:32:23<45:19,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8150 / 10000 | Loss: 0.4790208637714386, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  82%| | 8175/10000 [4:33:01<46:28,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8175 / 10000 | Loss: 0.4756321310997009, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  82%| | 8200/10000 [4:33:39<45:50,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8200 / 10000 | Loss: 0.4510052800178528, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  82%| | 8225/10000 [4:34:18<45:26,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8225 / 10000 | Loss: 0.4850976765155792, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  82%| | 8250/10000 [4:34:57<46:41,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8250 / 10000 | Loss: 0.42787420749664307, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  83%| | 8275/10000 [4:35:36<43:58,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8275 / 10000 | Loss: 0.4777015745639801, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  83%| | 8300/10000 [4:36:14<43:59,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8300 / 10000 | Loss: 0.4749031066894531, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  83%| | 8325/10000 [4:36:53<42:27,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8325 / 10000 | Loss: 0.4648522138595581, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  84%| | 8350/10000 [4:37:31<42:02,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8350 / 10000 | Loss: 0.49541619420051575, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  84%| | 8375/10000 [4:38:10<41:45,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8375 / 10000 | Loss: 0.470834881067276, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  84%| | 8400/10000 [4:38:49<41:04,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8400 / 10000 | Loss: 0.46029436588287354, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  84%| | 8425/10000 [4:39:27<40:25,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8425 / 10000 | Loss: 0.46370428800582886, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  84%| | 8450/10000 [4:40:05<39:53,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8450 / 10000 | Loss: 0.4383831322193146, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  85%| | 8475/10000 [4:40:44<38:57,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8475 / 10000 | Loss: 0.4669332802295685, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  85%| | 8500/10000 [4:41:22<38:35,  1.54s/it]04/16/2024 23:08:08 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-8500-epoch-17\n",
      "04/16/2024 23:08:08 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8500 / 10000 | Loss: 0.4368954300880432, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 23:08:10 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-8500-epoch-17\\model.safetensors\n",
      "04/16/2024 23:08:10 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 23:08:11 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-8500-epoch-17\\model_1.safetensors\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-8500-epoch-17\\optimizer.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-8500-epoch-17\\scheduler.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_1.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_2.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_3.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_4.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_5.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_6.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_7.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_8.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_9.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_10.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_11.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_12.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_13.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_14.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_15.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_16.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_17.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_18.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_19.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_20.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_21.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_22.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_23.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_24.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_25.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 26 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_26.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 27 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_27.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 28 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_28.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 29 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_29.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 30 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_30.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 31 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_31.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 32 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_32.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 33 saved in result_distiling_3\\checkpoint-8500-epoch-17\\sampler_33.bin\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-8500-epoch-17\\scaler.pt\n",
      "04/16/2024 23:08:12 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-8500-epoch-17\\random_states_0.pkl\n",
      "04/16/2024 23:08:12 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-7000-epoch-14] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:39<00:00,  1.64s/it]\n",
      "Train steps ... :  85%| | 8500/10000 [4:45:05<38:35,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (8500 / 10000 | Eval Loss: 15.205109596252441 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  85%| | 8525/10000 [4:45:45<40:36,  1.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8525 / 10000 | Loss: 0.4717879295349121, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  86%| | 8550/10000 [4:46:26<41:16,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8550 / 10000 | Loss: 0.4687042832374573, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  86%| | 8575/10000 [4:47:09<38:48,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8575 / 10000 | Loss: 0.47943153977394104, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  86%| | 8600/10000 [4:48:11<35:05,  1.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8600 / 10000 | Loss: 0.45151907205581665, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  86%| | 8625/10000 [4:48:47<33:45,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8625 / 10000 | Loss: 0.4477439522743225, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  86%| | 8650/10000 [4:49:25<35:16,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8650 / 10000 | Loss: 0.41922876238822937, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  87%| | 8675/10000 [4:50:06<35:38,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8675 / 10000 | Loss: 0.44635358452796936, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  87%| | 8700/10000 [4:50:47<34:53,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8700 / 10000 | Loss: 0.4398043751716614, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  87%| | 8725/10000 [4:51:27<33:56,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8725 / 10000 | Loss: 0.4261015057563782, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  88%| | 8750/10000 [4:52:07<33:54,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8750 / 10000 | Loss: 0.42458030581474304, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  88%| | 8775/10000 [4:52:48<32:50,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8775 / 10000 | Loss: 0.45708906650543213, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  88%| | 8800/10000 [4:53:28<31:55,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8800 / 10000 | Loss: 0.4386690557003021, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  88%| | 8825/10000 [4:54:09<33:40,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8825 / 10000 | Loss: 0.45566242933273315, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  88%| | 8850/10000 [4:54:51<31:49,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8850 / 10000 | Loss: 0.46285855770111084, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  89%| | 8875/10000 [4:55:33<31:32,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8875 / 10000 | Loss: 0.4451533854007721, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  89%| | 8900/10000 [4:56:15<30:27,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8900 / 10000 | Loss: 0.45420798659324646, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  89%| | 8925/10000 [4:56:57<31:14,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8925 / 10000 | Loss: 0.43087059259414673, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  90%| | 8950/10000 [4:57:40<30:38,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8950 / 10000 | Loss: 0.4304153025150299, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  90%| | 8975/10000 [4:58:24<29:37,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (8975 / 10000 | Loss: 0.4541173279285431, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  90%| | 9000/10000 [4:59:07<28:57,  1.74s/it]04/16/2024 23:25:54 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-9000-epoch-18\n",
      "04/16/2024 23:25:54 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9000 / 10000 | Loss: 0.4436376690864563, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 23:25:55 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-9000-epoch-18\\model.safetensors\n",
      "04/16/2024 23:25:55 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 23:25:56 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-9000-epoch-18\\model_1.safetensors\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-9000-epoch-18\\optimizer.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-9000-epoch-18\\scheduler.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_1.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_2.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_3.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_4.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_5.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_6.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_7.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_8.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_9.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_10.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_11.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_12.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_13.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_14.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_15.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_16.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_17.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_18.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_19.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_20.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_21.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_22.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_23.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_24.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_25.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 26 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_26.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 27 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_27.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 28 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_28.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 29 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_29.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 30 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_30.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 31 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_31.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 32 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_32.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 33 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_33.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 34 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_34.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 35 saved in result_distiling_3\\checkpoint-9000-epoch-18\\sampler_35.bin\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-9000-epoch-18\\scaler.pt\n",
      "04/16/2024 23:25:57 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-9000-epoch-18\\random_states_0.pkl\n",
      "04/16/2024 23:25:57 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-7500-epoch-15] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:42<00:00,  1.66s/it]\n",
      "Train steps ... :  90%| | 9000/10000 [5:02:54<28:57,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (9000 / 10000 | Eval Loss: 15.32075309753418 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  90%| | 9025/10000 [5:03:34<26:50,  1.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9025 / 10000 | Loss: 0.4470010995864868, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  90%| | 9050/10000 [5:04:15<26:15,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9050 / 10000 | Loss: 0.4728611409664154, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  91%| | 9075/10000 [5:05:19<24:53,  1.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9075 / 10000 | Loss: 0.4635727107524872, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  91%| | 9100/10000 [5:05:56<21:44,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9100 / 10000 | Loss: 0.43432971835136414, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  91%|| 9125/10000 [5:06:34<22:39,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9125 / 10000 | Loss: 0.4515876770019531, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  92%|| 9150/10000 [5:07:13<22:51,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9150 / 10000 | Loss: 0.4338645339012146, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  92%|| 9175/10000 [5:07:54<22:33,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9175 / 10000 | Loss: 0.46490374207496643, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  92%|| 9200/10000 [5:08:35<21:54,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9200 / 10000 | Loss: 0.43420055508613586, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  92%|| 9225/10000 [5:09:17<21:18,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9225 / 10000 | Loss: 0.44988077878952026, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  92%|| 9250/10000 [5:09:58<20:29,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9250 / 10000 | Loss: 0.4456271529197693, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  93%|| 9275/10000 [5:10:39<20:20,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9275 / 10000 | Loss: 0.41187748312950134, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  93%|| 9300/10000 [5:11:21<19:10,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9300 / 10000 | Loss: 0.44625547528266907, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  93%|| 9325/10000 [5:12:02<18:25,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9325 / 10000 | Loss: 0.47375011444091797, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  94%|| 9350/10000 [5:12:44<18:04,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9350 / 10000 | Loss: 0.4233546257019043, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  94%|| 9375/10000 [5:13:25<17:04,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9375 / 10000 | Loss: 0.47039133310317993, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  94%|| 9400/10000 [5:14:07<17:05,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9400 / 10000 | Loss: 0.4180648922920227, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  94%|| 9425/10000 [5:14:48<15:50,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9425 / 10000 | Loss: 0.439382940530777, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  94%|| 9450/10000 [5:15:30<15:07,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9450 / 10000 | Loss: 0.4507378339767456, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  95%|| 9475/10000 [5:16:11<14:41,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9475 / 10000 | Loss: 0.4127490818500519, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  95%|| 9500/10000 [5:16:53<13:41,  1.64s/it]04/16/2024 23:43:39 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-9500-epoch-19\n",
      "04/16/2024 23:43:39 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9500 / 10000 | Loss: 0.4523216187953949, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 23:43:40 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-9500-epoch-19\\model.safetensors\n",
      "04/16/2024 23:43:40 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/16/2024 23:43:42 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-9500-epoch-19\\model_1.safetensors\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-9500-epoch-19\\optimizer.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-9500-epoch-19\\scheduler.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_1.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_2.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_3.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_4.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_5.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_6.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_7.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_8.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_9.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_10.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_11.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_12.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_13.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_14.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_15.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_16.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_17.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_18.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_19.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_20.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_21.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_22.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_23.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_24.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_25.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 26 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_26.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 27 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_27.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 28 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_28.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 29 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_29.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 30 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_30.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 31 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_31.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 32 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_32.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 33 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_33.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 34 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_34.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 35 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_35.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 36 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_36.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 37 saved in result_distiling_3\\checkpoint-9500-epoch-19\\sampler_37.bin\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-9500-epoch-19\\scaler.pt\n",
      "04/16/2024 23:43:43 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-9500-epoch-19\\random_states_0.pkl\n",
      "04/16/2024 23:43:43 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-8000-epoch-16] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:45<00:00,  1.69s/it]\n",
      "Train steps ... :  95%|| 9500/10000 [5:20:43<13:41,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (9500 / 10000 | Eval Loss: 15.252725601196289 | )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  95%|| 9525/10000 [5:21:23<14:33,  1.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9525 / 10000 | Loss: 0.4327377676963806, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  96%|| 9550/10000 [5:22:33<13:51,  1.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9550 / 10000 | Loss: 0.4126783311367035, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  96%|| 9575/10000 [5:23:12<11:12,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9575 / 10000 | Loss: 0.45667096972465515, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  96%|| 9600/10000 [5:23:51<10:54,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9600 / 10000 | Loss: 0.4312096834182739, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  96%|| 9625/10000 [5:24:32<10:16,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9625 / 10000 | Loss: 0.4250440001487732, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  96%|| 9650/10000 [5:25:14<09:55,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9650 / 10000 | Loss: 0.4066798985004425, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  97%|| 9675/10000 [5:25:56<08:54,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9675 / 10000 | Loss: 0.4219701588153839, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  97%|| 9700/10000 [5:26:37<08:21,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9700 / 10000 | Loss: 0.42369747161865234, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  97%|| 9725/10000 [5:27:18<07:33,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9725 / 10000 | Loss: 0.41964632272720337, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  98%|| 9750/10000 [5:28:00<06:50,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9750 / 10000 | Loss: 0.41202059388160706, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  98%|| 9775/10000 [5:28:41<06:17,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9775 / 10000 | Loss: 0.42490047216415405, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  98%|| 9800/10000 [5:29:23<05:28,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9800 / 10000 | Loss: 0.4243955612182617, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  98%|| 9825/10000 [5:30:04<04:47,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9825 / 10000 | Loss: 0.42237237095832825, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  98%|| 9850/10000 [5:30:46<04:08,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9850 / 10000 | Loss: 0.40490052103996277, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  99%|| 9875/10000 [5:31:27<03:25,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9875 / 10000 | Loss: 0.4297671318054199, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  99%|| 9900/10000 [5:32:08<02:49,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9900 / 10000 | Loss: 0.4413171410560608, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... :  99%|| 9925/10000 [5:32:50<02:03,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9925 / 10000 | Loss: 0.4061076045036316, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... : 100%|| 9950/10000 [5:33:31<01:21,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9950 / 10000 | Loss: 0.4194033145904541, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... : 100%|| 9975/10000 [5:34:13<00:41,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (9975 / 10000 | Loss: 0.40384355187416077, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train steps ... : 100%|| 10000/10000 [5:34:55<00:00,  1.64s/it]04/17/2024 00:01:41 - INFO - accelerate.accelerator - Saving current state to ./result_distiling_3\\checkpoint-10000-epoch-20\n",
      "04/17/2024 00:01:41 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step... (10000 / 10000 | Loss: 0.405915766954422, Learning Rate: 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/17/2024 00:01:43 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-10000-epoch-20\\model.safetensors\n",
      "04/17/2024 00:01:43 - WARNING - accelerate.utils.other - Removed shared tensor {'proj_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "04/17/2024 00:01:44 - INFO - accelerate.checkpointing - Model weights saved in result_distiling_3\\checkpoint-10000-epoch-20\\model_1.safetensors\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Optimizer state saved in result_distiling_3\\checkpoint-10000-epoch-20\\optimizer.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Scheduler state saved in result_distiling_3\\checkpoint-10000-epoch-20\\scheduler.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_1.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 2 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_2.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 3 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_3.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 4 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_4.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 5 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_5.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 6 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_6.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 7 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_7.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 8 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_8.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 9 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_9.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 10 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_10.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 11 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_11.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 12 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_12.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 13 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_13.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 14 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_14.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 15 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_15.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 16 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_16.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 17 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_17.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 18 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_18.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 19 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_19.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 20 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_20.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 21 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_21.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 22 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_22.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 23 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_23.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 24 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_24.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 25 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_25.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 26 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_26.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 27 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_27.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 28 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_28.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 29 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_29.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 30 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_30.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 31 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_31.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 32 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_32.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 33 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_33.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 34 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_34.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 35 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_35.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 36 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_36.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 37 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_37.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 38 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_38.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 39 saved in result_distiling_3\\checkpoint-10000-epoch-20\\sampler_39.bin\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Gradient scaler state saved in result_distiling_3\\checkpoint-10000-epoch-20\\scaler.pt\n",
      "04/17/2024 00:01:45 - INFO - accelerate.checkpointing - Random states saved in result_distiling_3\\checkpoint-10000-epoch-20\\random_states_0.pkl\n",
      "04/17/2024 00:01:45 - INFO - run_distillation - Deleting older checkpoint [result_distiling_3\\checkpoint-8500-epoch-17] due to args.save_total_limit\n",
      "Configuration saved in ./result_distiling_3\\config.json\n",
      "Configuration saved in ./result_distiling_3\\generation_config.json\n",
      "Model weights saved in ./result_distiling_3\\pytorch_model.bin\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating eval...: 100%|| 134/134 [03:51<00:00,  1.73s/it]\n",
      "Train steps ... : 100%|| 10000/10000 [5:38:54<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for step (10000 / 10000 | Eval Loss: 15.371350288391113 | )\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34822150cc584a129656b9c6da198235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/ce_loss</td><td></td></tr><tr><td>eval/epoch</td><td></td></tr><tr><td>eval/kl_loss</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/time</td><td></td></tr><tr><td>train/ce_loss</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/kl_loss</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/time</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/ce_loss</td><td>8.76933</td></tr><tr><td>eval/epoch</td><td>20</td></tr><tr><td>eval/kl_loss</td><td>8.35588</td></tr><tr><td>eval/loss</td><td>15.37135</td></tr><tr><td>eval/time</td><td>231.84592</td></tr><tr><td>train/ce_loss</td><td>0.0669</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/kl_loss</td><td>0.35239</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.40592</td></tr><tr><td>train/time</td><td>16093.62246</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-pine-25</strong> at: <a href='https://wandb.ai/zekamrozek/distil-whisper/runs/okut9fs8' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper/runs/okut9fs8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240416_181148-okut9fs8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(epochs_trained, num_epochs):\n",
    "    vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(training_args.seed)\n",
    "    train_dataloader = DataLoader(\n",
    "        vectorized_datasets[\"train\"],\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        num_workers=dataloader_num_workers,\n",
    "        pin_memory=training_args.dataloader_pin_memory,\n",
    "    )\n",
    "    train_dataloader = accelerator.prepare(train_dataloader)\n",
    "    if hasattr(train_dataloader, \"dataset\") and isinstance(train_dataloader.dataset, IterableDataset):\n",
    "        train_dataloader.dataset.set_epoch(epoch)\n",
    "\n",
    "    if resume_step is not None:\n",
    "        # Skip the first N batches in the dataloader when resuming from a checkpoint\n",
    "        train_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "        resume_step = None\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        with accelerator.accumulate(student_model):\n",
    "            loss, train_metric = train_step(batch, temperature=training_args.temperature)\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(student_model.parameters(), training_args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Check if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            steps_trained_progress_bar.update(1)\n",
    "            cur_step += 1\n",
    "\n",
    "            if cur_step % training_args.logging_steps == 0:\n",
    "                steps_trained_progress_bar.write(\n",
    "                    f\"Step... ({cur_step} / {total_train_steps} | Loss:\"\n",
    "                    f\" {train_metric['loss']}, Learning Rate:\"\n",
    "                    f\" {lr_scheduler.get_last_lr()[0]})\"\n",
    "                )\n",
    "                log_metric(\n",
    "                    accelerator,\n",
    "                    metrics=train_metric,\n",
    "                    learning_rate=lr_scheduler.get_last_lr()[0],\n",
    "                    train_time=train_time + time.time() - train_start,\n",
    "                    step=cur_step,\n",
    "                    epoch=epoch,\n",
    "                    prefix=\"train\",\n",
    "                )\n",
    "\n",
    "            # save checkpoint and weights after each save_steps and at the end of training\n",
    "            if (cur_step % training_args.save_steps == 0) or cur_step == total_train_steps:\n",
    "                intermediate_dir = os.path.join(training_args.output_dir, f\"checkpoint-{cur_step}-epoch-{epoch}\")\n",
    "                accelerator.save_state(output_dir=intermediate_dir)\n",
    "                accelerator.wait_for_everyone()\n",
    "                if accelerator.is_main_process:\n",
    "                    rotate_checkpoints(training_args.save_total_limit, output_dir=training_args.output_dir)\n",
    "\n",
    "                    if cur_step == total_train_steps:\n",
    "                        student_model = accelerator.unwrap_model(student_model)\n",
    "                        student_model.save_pretrained(training_args.output_dir)\n",
    "\n",
    "                    if training_args.push_to_hub:\n",
    "                        repo.push_to_hub(\n",
    "                            commit_message=f\"Saving train state of step {cur_step}\",\n",
    "                            blocking=False,\n",
    "                        )\n",
    "\n",
    "            if training_args.do_eval and (cur_step % eval_steps == 0 or cur_step == total_train_steps):\n",
    "                train_time += time.time() - train_start\n",
    "                student_model.eval()\n",
    "                # ======================== Evaluating ==============================\n",
    "                for eval_split in ['eval']:\n",
    "                    eval_metrics = []\n",
    "                    eval_preds = []\n",
    "                    eval_labels = []\n",
    "                    eval_start = time.time()\n",
    "\n",
    "                    validation_dataloader = DataLoader(\n",
    "                        vectorized_datasets[eval_split],\n",
    "                        collate_fn=data_collator,\n",
    "                        batch_size=per_device_eval_batch_size,\n",
    "                        drop_last=False,\n",
    "                        num_workers=dataloader_num_workers,\n",
    "                        pin_memory=training_args.dataloader_pin_memory,\n",
    "                    )\n",
    "                    validation_dataloader = accelerator.prepare(validation_dataloader)\n",
    "\n",
    "                    for batch in tqdm(\n",
    "                        validation_dataloader,\n",
    "                        desc=f\"Evaluating {eval_split}...\",\n",
    "                        position=2,\n",
    "                        disable=not accelerator.is_local_main_process,\n",
    "                    ):\n",
    "                        # Model forward\n",
    "                        eval_metric = eval_step(batch)\n",
    "                        eval_metric = accelerator.gather_for_metrics(eval_metric)\n",
    "                        eval_metrics.append(eval_metric)\n",
    "\n",
    "                        # generation\n",
    "                        if training_args.predict_with_generate:\n",
    "                            generated_ids = generate_step(batch)\n",
    "                            # Gather all predictions and targets\n",
    "                            generated_ids, labels = accelerator.gather_for_metrics(\n",
    "                                (generated_ids, batch[\"labels\"])\n",
    "                            )\n",
    "                            eval_preds.extend(generated_ids)\n",
    "                            eval_labels.extend(labels)\n",
    "\n",
    "                    eval_time = time.time() - eval_start\n",
    "                    # normalize eval metrics\n",
    "                    eval_metrics = {\n",
    "                        key: torch.mean(torch.stack([d[key] for d in eval_metrics])) for key in eval_metrics[0]\n",
    "                    }\n",
    "\n",
    "                    # compute WER metric\n",
    "                    wer_desc = \"\"\n",
    "                    if training_args.predict_with_generate:\n",
    "                        wer_metric, pred_str, label_str, norm_pred_str, norm_label_str = compute_metrics(\n",
    "                            eval_preds, eval_labels\n",
    "                        )\n",
    "                        eval_metrics.update(wer_metric)\n",
    "                        wer_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in wer_metric.items()])\n",
    "                        log_pred(\n",
    "                            accelerator,\n",
    "                            pred_str,\n",
    "                            label_str,\n",
    "                            norm_pred_str,\n",
    "                            norm_label_str,\n",
    "                            step=cur_step,\n",
    "                            prefix=eval_split,\n",
    "                        )\n",
    "\n",
    "                    # Print metrics and update progress bar\n",
    "                    steps_trained_progress_bar.write(\n",
    "                        f\"Eval results for step ({cur_step} / {total_train_steps} | Eval Loss: {eval_metrics['loss']} |\"\n",
    "                        f\" {wer_desc})\"\n",
    "                    )\n",
    "\n",
    "                    log_metric(\n",
    "                        accelerator,\n",
    "                        metrics=eval_metrics,\n",
    "                        train_time=eval_time,\n",
    "                        step=cur_step,\n",
    "                        epoch=epoch,\n",
    "                        prefix=eval_split,\n",
    "                    )\n",
    "\n",
    "                # flush the train metrics\n",
    "                train_start = time.time()\n",
    "\n",
    "            # break condition\n",
    "            if cur_step == total_train_steps:\n",
    "                continue_training = False\n",
    "                break\n",
    "\n",
    "    if not continue_training:\n",
    "        break\n",
    "\n",
    "accelerator.end_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f733de3f-ea66-4fc2-bc61-caf1c6e33ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fcc8967-8758-4c6b-9437-95ab254e1575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.accelerator.Accelerator at 0x1a2ccc40be0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80cfd9-8875-415f-85cf-9a2acbac0bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991385ee-f919-4068-8bb1-3b99fdcdcb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f0bc0-fd48-4d57-bbb8-a0f87e9e9d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28687f92-bf90-42ba-b4d7-6d33519dcdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b32e4-6d68-4d14-89c2-e54d13dc2eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd9d1d-d95d-41f9-9d64-9314c6ca6377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9b8a8-fe9a-4803-8cf3-a220ca589cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320f351-fd9d-4a0b-ab62-fc18ac0b8b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
