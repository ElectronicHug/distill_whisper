{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Zhenya\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfFolder\n",
    "import os\n",
    "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "login(token=huggingface_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also adapt this script for your own speech recognition validation. Pointers for this are left as comments.\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import DatasetDict, IterableDatasetDict, load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    is_wandb_available,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.models.whisper.english_normalizer import EnglishTextNormalizer, BasicTextNormalizer\n",
    "from transformers.models.whisper.modeling_whisper import WhisperForCausalLM\n",
    "from transformers.utils import check_min_version, is_accelerate_available\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.34.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=2.14.6\", \"To fix: `pip install --upgrade datasets`\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_eval import DataTrainingArguments, write_metric, write_wandb_metric, write_wandb_pred, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(dataset_name=None, model_name_or_path='./models/distiled_student_model_moz_v1', subfolder='', model_variant=None, cache_dir=None, assistant_model_name_or_path=None, dtype='float16', use_pipeline=True, chunk_length_s=30.0, return_timestamps=True, language='uk', task='transcribe', attn_implementation='sdpa', batch_size=1, num_beams=1, temperature_fallback=True, logprob_threshold=-1.0, no_speech_threshold=0.6, compression_ratio_threshold=1.35, condition_on_prev_tokens=False, samples_per_dataset=None, dataset_config_name=None, dataset_split_name=None, dataset_cache_dir=None, overwrite_cache=False, preprocessing_num_workers=None, audio_column_name='audio', text_column_name=None, generation_max_length=256, log_predictions=True, preprocessing_only=False, wandb_project='distil-whisper-speed-benchmark', wandb_name=None, wandb_job_type='distil-whisper', wandb_dir=None, save_code_to_wandb=False, streaming=True, max_eval_samples=None, seed=42, use_fast_tokenizer=True, prompt_text=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = HfArgumentParser([DataTrainingArguments])\n",
    "# data_args = parser.parse_json_file(json_file=\"pipe_configs/eval_teacher_v0.json\")[0]\n",
    "data_args = parser.parse_json_file(json_file=\"pipe_configs/eval_v0.json\")[0]\n",
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "# 3. Set seed for reproducibility\n",
    "set_seed(data_args.seed)\n",
    "\n",
    "if data_args.use_pipeline and data_args.batch_size > 1:\n",
    "    raise ValueError(\"Make sure that `batch_size` is set to 1 when `use_pipeline=True`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/10/2024 11:13:17 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzekamrozek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Zhenya\\phd\\training\\local_pipeline\\wandb\\run-20240510_111318-jdcg3u2x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zekamrozek/distil-whisper-speed-benchmark/runs/jdcg3u2x' target=\"_blank\">brisk-glade-11</a></strong> to <a href='https://wandb.ai/zekamrozek/distil-whisper-speed-benchmark' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zekamrozek/distil-whisper-speed-benchmark' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper-speed-benchmark</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zekamrozek/distil-whisper-speed-benchmark/runs/jdcg3u2x' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper-speed-benchmark/runs/jdcg3u2x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "has_wandb = is_wandb_available()\n",
    "if has_wandb:\n",
    "    import wandb\n",
    "    import wandb as wandb_logger\n",
    "\n",
    "    # store generation HPs for runs\n",
    "    generation_arguments = {\n",
    "        \"torch_version\": str(torch.__version__),\n",
    "        \"transformers_version\": str(transformers.__version__),\n",
    "        \"attn_implementation\": data_args.attn_implementation,\n",
    "        \"model_name_or_path\": data_args.model_name_or_path,\n",
    "        \"subfolder\": data_args.subfolder,\n",
    "        \"assistant_model_name_or_path\": data_args.assistant_model_name_or_path,\n",
    "        \"seed\": data_args.seed,\n",
    "        \"batch_size\": data_args.batch_size,\n",
    "        \"num_beams\": data_args.num_beams,\n",
    "        \"return_timestamps\": data_args.return_timestamps,\n",
    "        \"condition_on_prev_tokens\": data_args.condition_on_prev_tokens,\n",
    "        \"temperature_fallback\": data_args.temperature_fallback,\n",
    "        \"logprob_threshold\": data_args.logprob_threshold,\n",
    "        \"no_speech_threshold\": data_args.no_speech_threshold,\n",
    "        \"use_pipeline\": data_args.use_pipeline,\n",
    "        \"chunk_length_s\": data_args.chunk_length_s,\n",
    "    }\n",
    "\n",
    "    # Set up wandb run\n",
    "    wandb_logger.init(\n",
    "        project=data_args.wandb_project,\n",
    "        name=data_args.wandb_name,\n",
    "        job_type=data_args.wandb_job_type,\n",
    "        dir=data_args.wandb_dir,\n",
    "        save_code=data_args.save_code_to_wandb,\n",
    "        config=generation_arguments,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Wandb logging requires wandb to be installed. Run `pip install wandb` to enable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "DEBUG_MODE = True\n",
    "# 3. Load dataset\n",
    "raw_datasets = DatasetDict()\n",
    "\n",
    "raw_datasets['mozila_test'] = datasets.load_from_disk(\n",
    "        f'datasets/mozila_uk/test',\n",
    "    )\n",
    "\n",
    "raw_datasets['mozila_test'] = raw_datasets['mozila_test'].rename_column('sentence', 'text')\n",
    "raw_datasets['mozila_test'] = raw_datasets['mozila_test'].remove_columns(set(raw_datasets['mozila_test'].column_names)-{'text', 'audio'})\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    raw_datasets['mozila_test'] = raw_datasets['mozila_test'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/distiled_student_model_moz_v1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\n",
    "    data_args.model_name_or_path,\n",
    "    subfolder=data_args.subfolder,\n",
    "    cache_dir=data_args.cache_dir,\n",
    "    use_fast=data_args.use_fast_tokenizer,\n",
    ")\n",
    "dtype = getattr(torch, data_args.dtype)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    data_args.model_name_or_path,\n",
    "    subfolder=data_args.subfolder,\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=data_args.attn_implementation,\n",
    "    low_cpu_mem_usage=is_accelerate_available(),\n",
    "    cache_dir=data_args.cache_dir,\n",
    "    variant=data_args.model_variant,\n",
    ")\n",
    "model.to(\"cuda:0\", dtype=dtype)\n",
    "\n",
    "teacher_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"models/local_whisper_medium\",\n",
    "    subfolder=data_args.subfolder,\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=data_args.attn_implementation,\n",
    "    low_cpu_mem_usage=is_accelerate_available(),\n",
    "    cache_dir=data_args.cache_dir,\n",
    "    variant=data_args.model_variant,\n",
    ")\n",
    "teacher_model.to(\"cuda:0\", dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args.use_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = None\n",
    "if data_args.use_pipeline:\n",
    "    model_pipeline = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=dtype,\n",
    "        device=model.device,\n",
    "        chunk_length_s=data_args.chunk_length_s,\n",
    "    )\n",
    "    model_pipeline_forward = model_pipeline._forward\n",
    "\n",
    "    teacher_model_pipeline = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=teacher_model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=dtype,\n",
    "        device=model.device,\n",
    "        chunk_length_s=data_args.chunk_length_s,\n",
    "    )\n",
    "    teacher_model_pipeline_forward = teacher_model_pipeline._forward\n",
    "\n",
    "assistant_model = None\n",
    "if data_args.assistant_model_name_or_path is not None:\n",
    "    logger.info(\"Loading assistant model...\")\n",
    "\n",
    "    if data_args.assistant_model_name_or_path.startswith(\"openai\"):\n",
    "        assistant_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            data_args.assistant_model_name_or_path,\n",
    "            torch_dtype=dtype,\n",
    "            attn_implementation=data_args.attn_implementation,\n",
    "            low_cpu_mem_usage=is_accelerate_available(),\n",
    "            cache_dir=data_args.cache_dir,\n",
    "        )\n",
    "    else:\n",
    "        assistant_model = WhisperForCausalLM.from_pretrained(\n",
    "            data_args.assistant_model_name_or_path,\n",
    "            torch_dtype=dtype,\n",
    "            attn_implementation=data_args.attn_implementation,\n",
    "            low_cpu_mem_usage=is_accelerate_available(),\n",
    "            cache_dir=data_args.cache_dir,\n",
    "        )\n",
    "\n",
    "    assistant_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.cast_column(\n",
    "    data_args.audio_column_name,\n",
    "    datasets.features.Audio(sampling_rate=processor.feature_extractor.sampling_rate),\n",
    ")\n",
    "\n",
    "# 7. Preprocessing the datasets.\n",
    "# We need to read the audio files as arrays and tokenize the targets.\n",
    "audio_column_name = data_args.audio_column_name\n",
    "normalizer = (\n",
    "    BasicTextNormalizer() if data_args.language is not None\n",
    "    else EnglishTextNormalizer(processor.tokenizer.english_spelling_normalizer)\n",
    ")\n",
    "sampling_rate = processor.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f60b03a1dec4ea8b70ef480d6c09529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # process audio\n",
    "    audio = [sample[\"array\"].astype(np.float32) for sample in batch[audio_column_name]]\n",
    "\n",
    "    if model_pipeline is None:\n",
    "        inputs = processor.feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=False,\n",
    "            padding=\"longest\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        if inputs.input_features.shape[-1] < 3000:\n",
    "            inputs = processor.feature_extractor(\n",
    "                audio,\n",
    "                sampling_rate=sampling_rate,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "        batch[\"input_features\"] = inputs.input_features.to(dtype)\n",
    "        batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    else:\n",
    "        batch[\"input_features\"] = audio\n",
    "\n",
    "    # process audio length\n",
    "    batch[\"length_in_s\"] = [len(sample) / sampling_rate for sample in audio]\n",
    "    # process targets\n",
    "    batch[\"reference\"] = batch[\"text\"]\n",
    "    return batch\n",
    "\n",
    "vectorized_datasets = IterableDatasetDict()\n",
    "\n",
    "for split in raw_datasets:\n",
    "    raw_datasets_features = list(raw_datasets[split].features.keys())\n",
    "\n",
    "    vectorized_datasets[split] = raw_datasets[split].map(\n",
    "        function=prepare_dataset,\n",
    "        remove_columns=raw_datasets_features,\n",
    "        batch_size=data_args.batch_size,\n",
    "        batched=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred_str, label_str):\n",
    "    # normalize everything and re-compute the WER\n",
    "    norm_pred_str = [normalizer(pred) for pred in pred_str]\n",
    "    norm_label_str = [normalizer(label) for label in label_str]\n",
    "\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero normalized references:\n",
    "    norm_pred_str = [norm_pred_str[i] for i in range(len(norm_pred_str)) if len(norm_label_str[i]) > 0]\n",
    "    norm_label_str = [norm_label_str[i] for i in range(len(norm_label_str)) if len(norm_label_str[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=norm_pred_str, references=norm_label_str)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": data_args.generation_max_length,\n",
    "    \"return_timestamps\": data_args.return_timestamps,\n",
    "    \"num_beams\": data_args.num_beams,\n",
    "    \"top_k\": 0,\n",
    "}\n",
    "\n",
    "if hasattr(model.generation_config, \"is_multilingual\") and model.generation_config.is_multilingual:\n",
    "    gen_kwargs[\"language\"] = data_args.language\n",
    "    gen_kwargs[\"task\"] = data_args.task\n",
    "elif data_args.language is not None:\n",
    "    raise ValueError(\n",
    "        \"Setting language token for an English-only checkpoint is not permitted. The language argument should \"\n",
    "        \"only be set for multilingual checkpoints.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if assistant_model is not None:\n",
    "    gen_kwargs[\"assistant_model\"] = assistant_model\n",
    "\n",
    "if data_args.prompt_text is not None:\n",
    "    gen_kwargs[\"prompt_ids\"] = processor.get_prompt_ids(data_args.prompt_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "long_form_gen_kwargs = {\n",
    "    \"condition_on_prev_tokens\": data_args.condition_on_prev_tokens,\n",
    "    \"compression_ratio_threshold\": data_args.compression_ratio_threshold,\n",
    "    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) if data_args.temperature_fallback else 0,\n",
    "    \"logprob_threshold\": data_args.logprob_threshold,\n",
    "    \"no_speech_threshold\": data_args.no_speech_threshold,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e59b7d03d94d9586385565c9be92ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zhenya\\anaconda3\\envs\\environment\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def benchmark(batch, use_teacher=False):\n",
    "    if model_pipeline is None:\n",
    "        inputs = torch.stack(batch[\"input_features\"], dim=0).cuda()\n",
    "        attention_mask = torch.stack(batch[\"attention_mask\"], dim=0).cuda()\n",
    "        # automatically use long-form args if required\n",
    "        inner_batch_size, num_mels, seq_len = inputs.shape\n",
    "        if seq_len == 3000:\n",
    "            batch_gen_kwargs = gen_kwargs\n",
    "        else:\n",
    "            batch_gen_kwargs = {**gen_kwargs, **long_form_gen_kwargs}\n",
    "\n",
    "        set_seed(data_args.seed)\n",
    "        start_time = time.time()\n",
    "        output_ids = model.generate(inputs, attention_mask=attention_mask, **batch_gen_kwargs)\n",
    "        batch[\"time\"] = inner_batch_size * [(time.time() - start_time) / inner_batch_size]\n",
    "\n",
    "        batch[\"transcription\"] = processor.batch_decode(\n",
    "            output_ids, skip_special_tokens=True, decode_with_timestamps=data_args.return_timestamps\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        inputs = batch[\"input_features\"]\n",
    "        # Time forward: let's make sure that only forward is timed and not pre- and post-processing\n",
    "        time_result = []\n",
    "\n",
    "        def _forward_time(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = model_pipeline_forward(*args, **kwargs)\n",
    "            end_time = time.time() - start_time\n",
    "            time_result.append(end_time)\n",
    "            return result\n",
    "\n",
    "        model_pipeline._forward = _forward_time\n",
    "        teacher_model_pipeline._forward = _forward_time\n",
    "\n",
    "        # print(.shape)\n",
    "        inputs = np.array(inputs)\n",
    "        if use_teacher:\n",
    "            result = model_pipeline(inputs, generate_kwargs=gen_kwargs)\n",
    "        else:\n",
    "            result = teacher_model_pipeline(inputs, generate_kwargs=gen_kwargs)\n",
    "        # print(result)\n",
    "        result = result[\"text\"]\n",
    "        batch[\"transcription\"] = [result]\n",
    "        batch[\"time\"] = [sum(time_result)]\n",
    "\n",
    "    batch[\"num_words\"] = [len(r.split()) for r in batch[\"reference\"]]\n",
    "    return batch\n",
    "\n",
    "result_datasets = DatasetDict()\n",
    "\n",
    "for split in vectorized_datasets:\n",
    "    result_datasets[split] = vectorized_datasets[split].map(\n",
    "        function=benchmark,\n",
    "        remove_columns=[\"input_features\"],\n",
    "        batch_size=data_args.batch_size,\n",
    "        batched=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model_pipeline.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' Мі хотів стягти її з вагончику'],\n",
       " [\" Відтоді є встих об'їхати увесь світ.\"],\n",
       " [' Пробачте, і тут лериків.'],\n",
       " [' Приїхав дідрозказав бабі, що так і так.'],\n",
       " [' І тут наперед, приготуйтеся.'],\n",
       " [' Ін підійшов до дверей і пустуков.'],\n",
       " [' Але я хочу їхати коло вас.'],\n",
       " [' Однак не чужих робила, та і на вас буду.'],\n",
       " [' Не завжди так складається, як сподівається.'],\n",
       " [' Взяти ноги на плечі.']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_datasets['mozila_test']['transcription'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Він хотів стягти її з вагончика.',\n",
       " \"Відтоді я встиг об'їхати увесь світ.\",\n",
       " 'Пробачте: і тут лірика.',\n",
       " 'Приїхав дід, розказав бабі, що так і так.',\n",
       " 'І тут наперед приготуйтеся',\n",
       " 'Він підійшов до дверей і постукав.',\n",
       " 'Але я хочу їхати коло вас.',\n",
       " 'Однак на чужих робила, то й на вас буду.',\n",
       " 'Не завжди так складається, як сподівається.',\n",
       " 'Взяти ноги на плечі.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_datasets['mozila_test']['reference'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' Мі хотів стягти її з вагончику'],\n",
       " [\" Відтоді є встих об'їхати увесь світ.\"],\n",
       " [' Пробачте, і тут лериків.'],\n",
       " [' Приїхав дідрозказав бабі, що так і так.'],\n",
       " [' І тут наперед, приготуйтеся.'],\n",
       " [' Ін підійшов до дверей і пустуков.'],\n",
       " [' Але я хочу їхати коло вас.'],\n",
       " [' Однак не чужих робила, та і на вас буду.'],\n",
       " [' Не завжди так складається, як сподівається.'],\n",
       " [' Взяти ноги на плечі.']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_datasets['mozila_test']['transcription'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/10/2024 11:20:14 - INFO - __main__ - ***** Running Evaluation *****\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   torch_version: 2.2.2\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   transformers_version: 4.40.1\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   attn_implementation: sdpa\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   model_name_or_path: ./models/distiled_student_model_moz_v1\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   subfolder: \n",
      "05/10/2024 11:20:14 - INFO - __main__ -   assistant_model_name_or_path: None\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   seed: 42\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   batch_size: 1\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   num_beams: 1\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   return_timestamps: True\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   condition_on_prev_tokens: False\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   temperature_fallback: True\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   logprob_threshold: -1.0\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   no_speech_threshold: 0.6\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   use_pipeline: True\n",
      "05/10/2024 11:20:14 - INFO - __main__ -   chunk_length_s: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start benchmarking mozila_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Samples: 100it [00:00, 14288.21it/s]\n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval wer: 31.728045325779036 | Eval rtf: 35.65086328417897 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "stats_dataset = DatasetDict()\n",
    "\n",
    "all_stats = {\"rtf\": 0, \"wer\": 0}\n",
    "rtf_stats = {\n",
    "    \"times_audio_total\": 0,\n",
    "    \"times_transcription_total\": 0,\n",
    "}\n",
    "\n",
    "logger.info(\"***** Running Evaluation *****\")\n",
    "for key in generation_arguments:\n",
    "    logger.info(f\"  {key}: {generation_arguments[key]}\")\n",
    "\n",
    "datasets_evaluated_progress_bar = tqdm(result_datasets, desc=\"Datasets\", position=0)\n",
    "for split in datasets_evaluated_progress_bar:\n",
    "    transcriptions = []\n",
    "    references = []\n",
    "    stats = {}\n",
    "    times_audio_total = 0\n",
    "    times_transcription_total = 0\n",
    "\n",
    "    datasets_evaluated_progress_bar.write(f\"Start benchmarking {split}...\")\n",
    "    result_iter = iter(result_datasets[split])\n",
    "    for result in tqdm(result_iter, desc=\"Samples\", position=1):\n",
    "        times_audio_total += result[\"length_in_s\"]\n",
    "        # print(result[\"time\"])\n",
    "        times_transcription_total += result[\"time\"][0]\n",
    "        # ensure prompt is removed from the transcription (awaiting fix in Transformers)\n",
    "        if data_args.prompt_text is not None:\n",
    "            result[\"transcription\"] = result[\"transcription\"].replace(data_args.prompt_text, \"\")\n",
    "        transcriptions.append(result[\"transcription\"])\n",
    "        references.append(result[\"reference\"])\n",
    "\n",
    "    norm_transcriptions = [normalizer(pred[0]) for pred in transcriptions]\n",
    "    norm_references = [normalizer(label) for label in references]\n",
    "\n",
    "    transcriptions = [transcriptions[i] for i in range(len(transcriptions)) if len(norm_references[i]) > 0]\n",
    "    references = [references[i] for i in range(len(references)) if len(norm_references[i]) > 0]\n",
    "\n",
    "    norm_transcriptions = [\n",
    "        norm_transcriptions[i] for i in range(len(norm_transcriptions)) if len(norm_references[i]) > 0\n",
    "    ]\n",
    "    norm_references = [norm_references[i] for i in range(len(norm_references)) if len(norm_references[i]) > 0]\n",
    "\n",
    "    stats[\"wer\"] = compute_metrics(norm_transcriptions, norm_references)\n",
    "\n",
    "    wer_per_sample = []\n",
    "    for pred, ref in zip(norm_transcriptions, norm_references):\n",
    "        wer_per_sample.append(compute_metrics([pred], [ref]))\n",
    "\n",
    "    stats[\"rtf\"] = times_audio_total / times_transcription_total\n",
    "    stats_dataset[split] = stats\n",
    "\n",
    "    wer_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in stats.items()])\n",
    "    datasets_evaluated_progress_bar.write(wer_desc)\n",
    "\n",
    "    write_wandb_metric(wandb_logger, stats, prefix=split)\n",
    "\n",
    "    if data_args.log_predictions:\n",
    "        write_wandb_pred(\n",
    "            wandb_logger,\n",
    "            transcriptions,\n",
    "            references,\n",
    "            norm_transcriptions,\n",
    "            norm_references,\n",
    "            wer_per_sample,\n",
    "            prefix=split,\n",
    "        )\n",
    "\n",
    "    rtf_stats[\"times_audio_total\"] += times_audio_total\n",
    "    rtf_stats[\"times_transcription_total\"] += times_transcription_total\n",
    "    all_stats[\"wer\"] += stats[\"wer\"]\n",
    "\n",
    "all_stats[\"wer\"] = all_stats[\"wer\"] / len(result_datasets)\n",
    "# technically this is the reciprocal of the RTF, but it makes the scale easier to read on wandb\n",
    "all_stats[\"rtf\"] = rtf_stats[\"times_audio_total\"] / rtf_stats[\"times_transcription_total\"]\n",
    "\n",
    "stats_dataset[\"all\"] = all_stats\n",
    "\n",
    "write_wandb_metric(wandb_logger, all_stats, prefix=\"all\")\n",
    "\n",
    "benchmark_artifact = wandb.Artifact(\"Benchmark\", type=\"datasets\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    for split in stats_dataset:\n",
    "        file_name = os.path.join(temp_dir, f\"{'_'.join(split.split('/'))}.json\")\n",
    "\n",
    "        with open(file_name, \"w\") as json_file:\n",
    "            json.dump(stats_dataset[split], json_file)\n",
    "\n",
    "        benchmark_artifact.add_file(file_name, split)\n",
    "\n",
    "    wandb_logger.log_artifact(benchmark_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval wer: 31.728045325779036 | Eval rtf: 34.67377667224213 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start benchmarking mozila_test...\n",
    "# Samples: 9791it [00:00, 17022.25it/s]\n",
    "# Datasets:   0%|          | 0/1 [01:49<?, ?it/s]\n",
    "# Eval wer: 29.481835266854127 | Eval rtf: 43.302215022844024 |\n",
    "# Datasets: 100%|██████████| 1/1 [01:51<00:00, 111.21s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
