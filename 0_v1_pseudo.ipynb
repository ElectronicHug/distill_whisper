{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Zhenya\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfFolder\n",
    "import os\n",
    "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "login(token=huggingface_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    IterableDatasetDict,\n",
    "    load_dataset,\n",
    ")\n",
    "from huggingface_hub import HfFolder, create_repo, get_full_repo_name, snapshot_download, upload_folder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    WhisperTokenizerFast,\n",
    ")\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer, EnglishTextNormalizer\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.34.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=2.14.6\", \"To fix: `pip install --upgrade datasets`\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "DEBUG_MODE = True\n",
    "raw_datasets = DatasetDict()\n",
    "# sampling_rate = 16_000\n",
    "# 3. Load dataset\n",
    "all_eval_datasets_list = []\n",
    "\n",
    "spits_to_load = ['train', 'validation','test']\n",
    "\n",
    "# 3. Load dataset\n",
    "raw_datasets = DatasetDict()\n",
    "token = HfFolder().get_token()\n",
    "\n",
    "for split in spits_to_load:\n",
    "    print(split)\n",
    "    raw_datasets[split] = datasets.load_from_disk(\n",
    "            f'datasets/mozila_uk/{split}',\n",
    "        )\n",
    "\n",
    "    if DEBUG_MODE:\n",
    "        raw_datasets[split] = raw_datasets[split].select(range(100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_pseudo_labelling import ModelArguments, DataTrainingArguments, shift_tokens_right, DataCollatorSpeechSeq2SeqWithPadding, log_metric, log_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport src.utils\n",
    "%aimport src.load_model\n",
    "\n",
    "from src.utils import prepare_accelerator\n",
    "from src.load_model import load_config_feature_ext_tokenizer, load_processor, load_whisper_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_json_file(json_file=\"pipe_configs/pseudo_v0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzekamrozek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Zhenya\\phd\\training\\local_pipeline\\wandb\\run-20240508_115827-pthsxbqd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zekamrozek/distil-whisper-labelling/runs/pthsxbqd' target=\"_blank\">restful-brook-22</a></strong> to <a href='https://wandb.ai/zekamrozek/distil-whisper-labelling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zekamrozek/distil-whisper-labelling' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper-labelling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zekamrozek/distil-whisper-labelling/runs/pthsxbqd' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper-labelling/runs/pthsxbqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2024 11:58:35 - WARNING - src.utils - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "accelerator, model_dtype, logger = prepare_accelerator(input_dtype=model_args.dtype, project_name=data_args.wandb_project, training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zhenya\\anaconda3\\envs\\environment\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config, feature_extractor, tokenizer = load_config_feature_ext_tokenizer(\n",
    "    model_name_or_path=\"openai/whisper-medium\", model_args=model_args)\n",
    "\n",
    "processor = load_processor(processor_path=\"openai/whisper-medium\", model_args=model_args)\n",
    "\n",
    "model = load_whisper_model(model_args.model_name_or_path, model_args, dtype=model_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperConfig {\n",
       "  \"_name_or_path\": \"openai/whisper-medium\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"apply_spec_augment\": false,\n",
       "  \"architectures\": [\n",
       "    \"WhisperForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"begin_suppress_tokens\": [\n",
       "    220,\n",
       "    50257\n",
       "  ],\n",
       "  \"bos_token_id\": 50257,\n",
       "  \"classifier_proj_size\": 256,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 24,\n",
       "  \"decoder_start_token_id\": 50258,\n",
       "  \"dropout\": 0.0,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 24,\n",
       "  \"eos_token_id\": 50257,\n",
       "  \"forced_decoder_ids\": [\n",
       "    [\n",
       "      1,\n",
       "      50259\n",
       "    ],\n",
       "    [\n",
       "      2,\n",
       "      50359\n",
       "    ],\n",
       "    [\n",
       "      3,\n",
       "      50363\n",
       "    ]\n",
       "  ],\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"mask_feature_length\": 10,\n",
       "  \"mask_feature_min_masks\": 0,\n",
       "  \"mask_feature_prob\": 0.0,\n",
       "  \"mask_time_length\": 10,\n",
       "  \"mask_time_min_masks\": 2,\n",
       "  \"mask_time_prob\": 0.05,\n",
       "  \"max_length\": 448,\n",
       "  \"max_source_positions\": 1500,\n",
       "  \"max_target_positions\": 448,\n",
       "  \"median_filter_width\": 7,\n",
       "  \"model_type\": \"whisper\",\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_mel_bins\": 80,\n",
       "  \"pad_token_id\": 50257,\n",
       "  \"scale_embedding\": false,\n",
       "  \"suppress_tokens\": [\n",
       "    1,\n",
       "    2,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    14,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    31,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    359,\n",
       "    503,\n",
       "    522,\n",
       "    542,\n",
       "    873,\n",
       "    893,\n",
       "    902,\n",
       "    918,\n",
       "    922,\n",
       "    931,\n",
       "    1350,\n",
       "    1853,\n",
       "    1982,\n",
       "    2460,\n",
       "    2627,\n",
       "    3246,\n",
       "    3253,\n",
       "    3268,\n",
       "    3536,\n",
       "    3846,\n",
       "    3961,\n",
       "    4183,\n",
       "    4667,\n",
       "    6585,\n",
       "    6647,\n",
       "    7273,\n",
       "    9061,\n",
       "    9383,\n",
       "    10428,\n",
       "    10929,\n",
       "    11938,\n",
       "    12033,\n",
       "    12331,\n",
       "    12562,\n",
       "    13793,\n",
       "    14157,\n",
       "    14635,\n",
       "    15265,\n",
       "    15618,\n",
       "    16553,\n",
       "    16604,\n",
       "    18362,\n",
       "    18956,\n",
       "    20075,\n",
       "    21675,\n",
       "    22520,\n",
       "    26130,\n",
       "    26161,\n",
       "    26435,\n",
       "    28279,\n",
       "    29464,\n",
       "    31650,\n",
       "    32302,\n",
       "    32470,\n",
       "    36865,\n",
       "    42863,\n",
       "    47425,\n",
       "    49870,\n",
       "    50254,\n",
       "    50258,\n",
       "    50358,\n",
       "    50359,\n",
       "    50360,\n",
       "    50361,\n",
       "    50362\n",
       "  ],\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.40.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_weighted_layer_sum\": false,\n",
       "  \"vocab_size\": 51865\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "return_timestamps = data_args.return_timestamps\n",
    "if hasattr(model.generation_config, \"is_multilingual\") and model.generation_config.is_multilingual:\n",
    "    # We need to set the language and task ids for multilingual checkpoints\n",
    "    tokenizer.set_prefix_tokens(\n",
    "        language=data_args.language, task=data_args.task, predict_timestamps=return_timestamps\n",
    "    )\n",
    "elif data_args.language is not None:\n",
    "    raise ValueError(\n",
    "        \"Setting language token for an English-only checkpoint is not permitted. The language argument should \"\n",
    "        \"only be set for multilingual checkpoints.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_normilazer(language, tokenizer=None):\n",
    "\n",
    "    if language is not None:\n",
    "        normalizer = BasicTextNormalizer()\n",
    "    else:\n",
    "        normalizer = EnglishTextNormalizer(tokenizer.english_spelling_normalizer)\n",
    "    return normalizer\n",
    "\n",
    "normalizer = prepare_normilazer(language=data_args.language, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.cast_column(\n",
    "    data_args.audio_column_name,\n",
    "    datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(raw_datasets['train'][0]['audio']['array'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = int(data_args.max_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "max_label_length = (\n",
    "    data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length\n",
    ")\n",
    "audio_column_name = data_args.audio_column_name\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "\n",
    "preprocessing_batch_size = data_args.preprocessing_batch_size\n",
    "num_workers = data_args.preprocessing_num_workers\n",
    "dataloader_num_workers = training_args.dataloader_num_workers\n",
    "\n",
    "text_column_name = data_args.text_column_name\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "id_column_name = data_args.id_column_name\n",
    "speaker_id_column_name = data_args.speaker_id_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_position = 1\n",
    "decoder_prev_token_id = tokenizer.convert_tokens_to_ids(\"<|startofprev|>\")\n",
    "decoder_eot_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if speaker_id_column_name is not None:\n",
    "    raw_datasets = raw_datasets.sort(speaker_id_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_dataset(batch):\n",
    "    audio = [sample[\"array\"] for sample in batch[audio_column_name]]\n",
    "    input_lengths = [len(sample) for sample in audio]\n",
    "\n",
    "    text = batch[text_column_name]\n",
    "    speaker_id = batch[speaker_id_column_name] if speaker_id_column_name else len(text) * [None]\n",
    "\n",
    "    concatenated_audio = []\n",
    "    concatenated_text = []\n",
    "    concatenated_speaker = []\n",
    "    condition_on_prev = []\n",
    "    audio_sample = audio[0]\n",
    "    text_sample = text[0]\n",
    "\n",
    "    for idx in range(1, len(audio)):\n",
    "        prev_speaker = speaker_id[idx - 1]\n",
    "        speaker = speaker_id[idx]\n",
    "\n",
    "        if len(audio_sample) + input_lengths[idx] < max_input_length:\n",
    "            if speaker == prev_speaker:\n",
    "                # we have no information about whether the segments follow on sequentially\n",
    "                # so we just ensure the same speaker as we concatenate across files\n",
    "                audio_sample = np.append(audio_sample, audio[idx])\n",
    "                # extra spaces in the text transcription don't matter, since we only use it for the WER computation\n",
    "                text_sample += \" \" + text[idx]\n",
    "            else:\n",
    "                # speakers do not follow sequentially, save the audio and start looping again\n",
    "                concatenated_audio.append(audio_sample)\n",
    "                concatenated_text.append(text_sample)\n",
    "                concatenated_speaker.append(speaker)\n",
    "                condition_on_prev.append(0)\n",
    "                audio_sample = audio[idx]\n",
    "                text_sample = text[idx]\n",
    "\n",
    "        else:\n",
    "            # concatenated audio exceeds max length, save the audio and start looping again\n",
    "            concatenated_audio.append(audio_sample)\n",
    "            concatenated_text.append(text_sample)\n",
    "            concatenated_speaker.append(speaker)\n",
    "            condition_on_prev.append(1)\n",
    "            audio_sample = audio[idx]\n",
    "            text_sample = text[idx]\n",
    "\n",
    "    batch[audio_column_name] = [{\"array\": array, \"sampling_rate\": sampling_rate} for array in concatenated_audio]\n",
    "    batch[text_column_name] = concatenated_text\n",
    "    batch[id_column_name] = concatenated_speaker\n",
    "    batch[\"condition_on_prev\"] = condition_on_prev\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_features = list(next(iter(raw_datasets.values())).features.keys())\n",
    "if data_args.concatenate_audio and not data_args.streaming:\n",
    "    with accelerator.main_process_first():\n",
    "        raw_datasets = raw_datasets.map(\n",
    "            concatenate_dataset,\n",
    "            batched=True,\n",
    "            batch_size=preprocessing_batch_size,\n",
    "            num_proc=1,\n",
    "            remove_columns=set(raw_datasets_features)\n",
    "            - {audio_column_name, text_column_name, id_column_name, \"condition_on_prev\"},\n",
    "            desc=\"Concatenating dataset...\",\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name =\"mozila-uk\"\n",
    "\n",
    "raw_datasets = raw_datasets.cast_column(\n",
    "        audio_column_name, datasets.features.Audio(sampling_rate=sampling_rate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess_ids(speaker_ids, indices):\n",
    "    speaker_ids_formatted = []\n",
    "    for speaker, idx in zip(speaker_ids, indices):\n",
    "        formatted_idx = f\"{dataset_name}-{speaker}-{idx}\" if speaker is not None else f\"{dataset_name}-{idx}\"\n",
    "        speaker_ids_formatted.append(formatted_idx)\n",
    "    return {id_column_name: speaker_ids_formatted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with accelerator.main_process_first():\n",
    "    raw_datasets = raw_datasets.map(\n",
    "        postprocess_ids,\n",
    "        input_columns=[id_column_name],\n",
    "        with_indices=True,\n",
    "        desc=\"Setting sample idxs...\",\n",
    "        batched=True,\n",
    "        batch_size=preprocessing_batch_size,\n",
    "        num_proc=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids_dataset = IterableDatasetDict() if data_args.streaming else DatasetDict()\n",
    "for split in raw_datasets:\n",
    "    file_ids_dataset[split] = raw_datasets[split][id_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # process audio\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    # process audio length\n",
    "    batch[model_input_name] = inputs.get(model_input_name)[0]\n",
    "\n",
    "    # process targets\n",
    "    input_str = batch[text_column_name]\n",
    "    batch[\"labels\"] = tokenizer(input_str, max_length=max_label_length, truncation=True).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d762adb5004d64bfb0946e674ceede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess dataset:   0%|          | 0/1658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_features = list(next(iter(raw_datasets.values())).features.keys())\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    vectorized_datasets = raw_datasets.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=raw_datasets_features,\n",
    "        num_proc=1,\n",
    "        desc=\"preprocess dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(training_args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels, file_ids):\n",
    "    # replace padded labels by the padding token\n",
    "    for idx in range(len(labels)):\n",
    "        labels[idx][labels[idx] == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=False, decode_with_timestamps=return_timestamps)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # normalize everything and re-compute the WER\n",
    "    norm_pred_str = [normalizer(pred) for pred in pred_str]\n",
    "    norm_label_str = [normalizer(label) for label in label_str]\n",
    "    # for logging, we need the pred/labels to match the norm_pred/norm_labels, so discard any filtered samples here\n",
    "    pred_str = [pred_str[i] for i in range(len(norm_pred_str)) if len(norm_label_str[i]) > 0]\n",
    "    label_str = [label_str[i] for i in range(len(norm_label_str)) if len(norm_label_str[i]) > 0]\n",
    "    file_ids = [file_ids[i] for i in range(len(file_ids)) if len(norm_label_str[i]) > 0]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero normalized references:\n",
    "    norm_pred_str = [norm_pred_str[i] for i in range(len(norm_pred_str)) if len(norm_label_str[i]) > 0]\n",
    "    norm_label_str = [norm_label_str[i] for i in range(len(norm_label_str)) if len(norm_label_str[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=norm_pred_str, references=norm_label_str)\n",
    "\n",
    "    return {\"wer\": wer}, pred_str, label_str, norm_pred_str, norm_label_str, file_ids\n",
    "\n",
    "def filter_eot_tokens(preds):\n",
    "    for idx in range(len(preds)):\n",
    "        # remove the EOT tokens to get the 'true' token length\n",
    "        token_ids = [token for token in preds[idx] if token != decoder_eot_token_id]\n",
    "        token_ids = token_ids + [decoder_eot_token_id]\n",
    "        preds[idx] = token_ids\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,  # <|startoftranscript|>\n",
    "    input_padding=\"longest\",\n",
    "    target_padding=\"max_length\",\n",
    "    max_target_length=max_label_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": max_label_length,\n",
    "    \"num_beams\": 1,\n",
    "    \"return_timestamps\": data_args.return_timestamps,\n",
    "    \"language\": data_args.language,\n",
    "    \"task\": data_args.task,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.forced_decoder_ids = None\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# 15. Prepare everything with accelerate\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step_with_save(split=\"eval\", decoder_eot_token_id=decoder_eot_token_id, decoder_prev_token_id=decoder_prev_token_id, timestamp_position=timestamp_position):\n",
    "    # ======================== Evaluating ==============================\n",
    "    eval_preds = []\n",
    "    eval_labels = []\n",
    "    eval_ids = []\n",
    "    pred_str = []\n",
    "    eval_start = time.time()\n",
    "\n",
    "    eval_loader = DataLoader(\n",
    "        vectorized_datasets[split],\n",
    "        batch_size=per_device_eval_batch_size,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=dataloader_num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    file_loader = DataLoader(\n",
    "        file_ids_dataset[split],\n",
    "        batch_size=per_device_eval_batch_size * accelerator.num_processes,\n",
    "        num_workers=dataloader_num_workers,\n",
    "    )\n",
    "\n",
    "    eval_loader = accelerator.prepare(eval_loader)\n",
    "    batches = tqdm(eval_loader, desc=f\"Evaluating {split}...\", disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    # make the split name pretty for librispeech etc\n",
    "    split = split.replace(\".\", \"-\").split(\"/\")[-1]\n",
    "    output_csv = os.path.join(output_dir, f\"{split}-transcription.csv\")\n",
    "\n",
    "    for step, (batch, file_ids) in enumerate(zip(batches, file_loader)):\n",
    "        # Generate predictions and pad to max generated length\n",
    "        generate_fn = model.module.generate if accelerator.num_processes > 1 else model.generate\n",
    "        generated_ids = generate_fn(batch[\"input_features\"].to(dtype=model_dtype), **gen_kwargs)\n",
    "        generated_ids = accelerator.pad_across_processes(generated_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "        # Gather all predictions and targets\n",
    "        generated_ids, labels = accelerator.gather_for_metrics((generated_ids, batch[\"labels\"]))\n",
    "        eval_preds.extend(generated_ids.cpu().numpy())\n",
    "        eval_labels.extend(labels.cpu().numpy())\n",
    "        eval_ids.extend(file_ids)\n",
    "\n",
    "        if step % training_args.logging_steps == 0 and step > 0:\n",
    "            batches.write(f\"Saving transcriptions for split {split} step {step}\")\n",
    "            accelerator.wait_for_everyone()\n",
    "            pred_ids = eval_preds[-(len(eval_preds) - len(pred_str)) :]\n",
    "            pred_ids = filter_eot_tokens(pred_ids)\n",
    "            pred_str.extend(\n",
    "                tokenizer.batch_decode(\n",
    "                    pred_ids, skip_special_tokens=False, decode_with_timestamps=return_timestamps\n",
    "                )\n",
    "            )\n",
    "            csv_data = [[eval_ids[i], pred_str[i]] for i in range(len(eval_preds))]\n",
    "\n",
    "            with open(output_csv, \"w\", encoding=\"UTF8\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                # write multiple rows\n",
    "                writer.writerow([\"file_id\", \"whisper_transcript\"])\n",
    "                writer.writerows(csv_data)\n",
    "\n",
    "            # if training_args.push_to_hub and accelerator.is_main_process:\n",
    "            #     upload_folder(\n",
    "            #         folder_path=output_dir,\n",
    "            #         repo_id=repo_name,\n",
    "            #         repo_type=\"dataset\",\n",
    "            #         commit_message=f\"Saving transcriptions for split {split} step {step}.\",\n",
    "            #     )\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    eval_time = time.time() - eval_start\n",
    "\n",
    "    # compute WER metric for eval sets\n",
    "    wer_desc = \"\"\n",
    "    if \"validation\" in split or \"test\" in split:\n",
    "        eval_preds = filter_eot_tokens(eval_preds)\n",
    "        wer_metric, pred_str, label_str, norm_pred_str, norm_label_str, eval_ids = compute_metrics(\n",
    "            eval_preds, eval_labels, eval_ids\n",
    "        )\n",
    "        wer_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in wer_metric.items()])\n",
    "        # Save metrics + predictions\n",
    "        log_metric(\n",
    "            accelerator,\n",
    "            metrics=wer_metric,\n",
    "            train_time=eval_time,\n",
    "            prefix=split,\n",
    "        )\n",
    "        log_pred(\n",
    "            accelerator,\n",
    "            pred_str,\n",
    "            label_str,\n",
    "            norm_pred_str,\n",
    "            norm_label_str,\n",
    "            prefix=split,\n",
    "        )\n",
    "    else:\n",
    "        pred_ids = eval_preds[-(len(eval_preds) - len(pred_str)) :]\n",
    "        pred_ids = filter_eot_tokens(pred_ids)\n",
    "        pred_str.extend(\n",
    "            tokenizer.batch_decode(pred_ids, skip_special_tokens=False, decode_with_timestamps=return_timestamps)\n",
    "        )\n",
    "\n",
    "    batches.write(f\"Saving final transcriptions for split {split}.\")\n",
    "    csv_data = [[eval_ids[i], eval_preds[i]] for i in range(len(eval_preds))]\n",
    "    with open(output_csv, \"w\", encoding=\"UTF8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        # write multiple rows\n",
    "        writer.writerow([\"file_id\", \"whisper_transcript\"])\n",
    "        writer.writerows(csv_data)\n",
    "\n",
    "    # Print metrics\n",
    "    logger.info(wer_desc)\n",
    "\n",
    "    if not data_args.streaming:\n",
    "        raw_datasets[split] = raw_datasets[split].add_column(\"whisper_transcript\", pred_str)\n",
    "        raw_datasets[split] = raw_datasets[split].add_column(\"eval_preds\", eval_preds)\n",
    "\n",
    "        def add_concatenated_text(eval_preds, condition_on_prev):\n",
    "            concatenated_prev = [None]\n",
    "            for token_ids, condition in zip(eval_preds[:-1], condition_on_prev[1:]):\n",
    "                if condition is False:\n",
    "                    concatenated_prev.append(None)\n",
    "                else:\n",
    "                    prompt_ids = [token for token in token_ids if token != decoder_eot_token_id]\n",
    "                    prompt_ids = [decoder_prev_token_id] + prompt_ids[timestamp_position:]\n",
    "                    concatenated_prev.append(prompt_ids)\n",
    "            return {\"condition_on_prev\": concatenated_prev}\n",
    "\n",
    "        with accelerator.main_process_first():\n",
    "            raw_datasets[split] = raw_datasets[split].map(\n",
    "                add_concatenated_text,\n",
    "                input_columns=[\"eval_preds\", \"condition_on_prev\"],\n",
    "                remove_columns=[\"eval_preds\"],\n",
    "                desc=\"Setting condition on prev...\",\n",
    "                batched=True,\n",
    "                batch_size=preprocessing_batch_size,\n",
    "                num_proc=num_workers,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/07/2024 19:24:41 - INFO - src.utils - ***** Running Labelling *****\n",
      "05/07/2024 19:24:41 - INFO - src.utils -   Instantaneous batch size per device = 35\n",
      "05/07/2024 19:24:41 - INFO - src.utils -   Total eval batch size (w. parallel & distributed) = 35\n",
      "05/07/2024 19:24:41 - INFO - src.utils -   Predict labels with timestamps = True\n",
      "Evaluating train...:   0%|          | 0/101 [00:00<?, ?it/s]c:\\Users\\Zhenya\\anaconda3\\envs\\environment\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:694: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Evaluating train...: 100%|██████████| 101/101 [15:40<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final transcriptions for split train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/07/2024 19:40:28 - INFO - src.utils - \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aeca8b3cbe4caf8331477355bf5dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Setting condition on prev... (num_proc=4):   0%|          | 0/3529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating validation...: 100%|██████████| 48/48 [07:34<00:00,  9.46s/it]\n",
      "05/07/2024 19:48:13 - INFO - src.utils - Eval wer: 16.204646768812918 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final transcriptions for split validation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1902e400447942b19b16f160f526b2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Setting condition on prev... (num_proc=4):   0%|          | 0/1658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test...: 100%|██████████| 50/50 [2:45:08<00:00, 198.18s/it]    \n",
      "05/07/2024 22:33:37 - INFO - src.utils - Eval wer: 17.418362864255382 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final transcriptions for split test.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d6c2da5bdd49e3b14424358e2b2c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/7 shards):   0%|          | 0/3529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0b433597c94cfaafb344c9e8d9e0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/1658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd672641768f439aa59cef50cf05de72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/1734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6277e033903048d1b70a240d65b149ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='66.085 MB of 66.085 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/time</td><td>▁</td></tr><tr><td>test/wer</td><td>▁</td></tr><tr><td>validation/time</td><td>▁</td></tr><tr><td>validation/wer</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/time</td><td>9909.19393</td></tr><tr><td>test/wer</td><td>17.41836</td></tr><tr><td>validation/time</td><td>454.54624</td></tr><tr><td>validation/wer</td><td>16.20465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-firefly-21</strong> at: <a href='https://wandb.ai/zekamrozek/distil-whisper-labelling/runs/7v3tbjct' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper-labelling/runs/7v3tbjct</a><br/> View project at: <a href='https://wandb.ai/zekamrozek/distil-whisper-labelling' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper-labelling</a><br/>Synced 6 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240507_192353-7v3tbjct\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"***** Running Labelling *****\")\n",
    "logger.info(\"  Instantaneous batch size per device =\" f\" {training_args.per_device_eval_batch_size}\")\n",
    "logger.info(\n",
    "    f\"  Total eval batch size (w. parallel & distributed) = {training_args.per_device_eval_batch_size * accelerator.num_processes}\"\n",
    ")\n",
    "logger.info(f\"  Predict labels with timestamps = {return_timestamps}\")\n",
    "\n",
    "\n",
    "for split in spits_to_load:\n",
    "    eval_step_with_save(split=split)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "raw_datasets.save_to_disk(output_dir, num_proc=num_workers)\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/labeled_mozila_3'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_eot_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
