{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d2e02-792c-44ac-8d8c-b2e63f91292d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager,store).\n",
      "Your token has been saved to C:\\Users\\Zhenya\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfFolder\n",
    "import os\n",
    "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "login(token=huggingface_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765d97b6-fbe9-4855-a0ac-835a21c23c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    IterableDataset,\n",
    "    IterableDatasetDict,\n",
    "    concatenate_datasets,\n",
    "    interleave_datasets,\n",
    "    load_dataset,\n",
    ")\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AddedToken,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    WhisperTokenizerFast,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer, EnglishTextNormalizer\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.34.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=2.14.6\", \"To fix: `pip install --upgrade datasets`\")\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926b7b8b-ed85-49ec-81c0-9ade2a150550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from run_distillation import (ModelArguments, DataTrainingArguments, DistillationTrainingArguments,\n",
    "    DataCollatorSpeechSeq2SeqWithPadding, log_metric, log_pred, convert_dataset_str_to_list,\n",
    "    load_multiple_datasets, get_layers_to_supervise, sorted_checkpoints, rotate_checkpoints,\n",
    "    get_last_checkpoint, get_parameter_names)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Specify that the init_accelerator module should be reloaded\n",
    "%aimport src.init_accelerator\n",
    "%aimport src.load_models\n",
    "%aimport src.prepare_dataset\n",
    "%aimport src.transcript_csv_utils\n",
    "\n",
    "\n",
    "from src.transcript_csv_utils import save_transcripts_to_csv, load_transcripts_from_csv, display_transcrips_manual\n",
    "from src.init_accelerator import prepare_accelerator, create_rep_and_dir\n",
    "from src.load_models import load_config_feature_ext_tokenizer, load_processor, load_whisper_model\n",
    "from src.prepare_dataset import prepare_vectorized_dataset, prepare_normilazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe07f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "# sampling_rate = 16_000\n",
    "# 3. Load dataset\n",
    "all_train_datasets_list = []\n",
    "all_eval_datasets_list = []\n",
    "\n",
    "DEBUG_MODE = True\n",
    "\n",
    "\n",
    "mozila_dataset_config = {\n",
    "    'path': 'dataset_saved/labeled_uk_2',\n",
    "    'use_cols':['path', 'audio', 'whisper_transcript_decoded', 'whisper_transcript', 'text', 'label']\n",
    "    }\n",
    "\n",
    "def prepare_mozilla_uk_dataset(mozila_dataset_config, split, debug_mode=False):\n",
    "    split_dataset = datasets.load_from_disk(f\"{mozila_dataset_config['path']}/{split}\") \n",
    "    split_dataset = split_dataset.remove_columns(\n",
    "        set(split_dataset.features.keys()) - set(mozila_dataset_config['use_cols'])\n",
    "        )\n",
    "    \n",
    "    if debug_mode:\n",
    "        split_dataset = split_dataset.select(range(100))\n",
    "\n",
    "    split_dataset = split_dataset.rename_column('label', 'labels')\n",
    "    return split_dataset\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    print(split)\n",
    "    all_train_datasets_list.append(\n",
    "        prepare_mozilla_uk_dataset(mozila_dataset_config, split, DEBUG_MODE)\n",
    "        )\n",
    "    \n",
    "for split in ['test']:\n",
    "    print(split)\n",
    "    all_eval_datasets_list.append(\n",
    "        prepare_mozilla_uk_dataset(mozila_dataset_config, split, DEBUG_MODE)\n",
    "        )\n",
    "    \n",
    "# Place for next dataset    \n",
    "\n",
    "raw_datasets['train'] = concatenate_datasets(all_train_datasets_list)\n",
    "raw_datasets['eval'] = concatenate_datasets(all_eval_datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364ef027-253e-4b56-a685-0915746b5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parse input arguments\n",
    "# We keep distinct sets of args, for cleaner separation of model/data/training related args\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, DistillationTrainingArguments))\n",
    "\n",
    "list_args = [\n",
    "    '--model_name_or_path=./models_dir/student_moz_uk',\n",
    " \n",
    "    '--teacher_model_name_or_path=./local_whisper_medium',\n",
    "    '--eval_steps=500',\n",
    "    '--save_steps=500',\n",
    "    '--warmup_steps=250',\n",
    "    '--learning_rate=0.00001',\n",
    "    '--lr_scheduler_type=constant_with_warmup',\n",
    "    '--logging_steps=25',\n",
    "    '--save_total_limit=1',\n",
    "    '--max_steps=10000',\n",
    "\n",
    "    '--per_device_train_batch_size=24',\n",
    "    '--per_device_eval_batch_size=24',\n",
    "    '--dataloader_num_workers=4',\n",
    "    '--preprocessing_num_workers=4',\n",
    "    '--ddp_timeout=7200',\n",
    "    '--dtype=float16',\n",
    "    '--do_train=True',\n",
    "    '--do_eval=True',\n",
    "    '--gradient_checkpointing=True',\n",
    "    '--streaming=False',\n",
    "    '--cache_dir=./model_cache/',\n",
    "\n",
    "    '--overwrite_output_dir=False',\n",
    "    '--output_dir=./result_distiling_new_uk_data_3',\n",
    "    '--freeze_encoder=True',\n",
    "    '--language=uk',\n",
    "    # '--=',\n",
    "]\n",
    "\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(list_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173692f2-afac-4f39-ac97-97e7a4c1b590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzekamrozek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\distiling_whisper_local\\wandb\\run-20240425_022929-dgb0dwjo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zekamrozek/distil-whisper/runs/dgb0dwjo' target=\"_blank\">comic-violet-57</a></strong> to <a href='https://wandb.ai/zekamrozek/distil-whisper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zekamrozek/distil-whisper' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zekamrozek/distil-whisper/runs/dgb0dwjo' target=\"_blank\">https://wandb.ai/zekamrozek/distil-whisper/runs/dgb0dwjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2024 02:29:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/25/2024 02:29:37 - INFO - __main__ - Training/evaluation parameters DistillationTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=7200,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "dtype=float16,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=500.0,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_encoder=True,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "kl_weight=1.0,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./result_distiling_new_uk_data_3\\runs\\Apr25_02-29-26_DESKTOP-9H5C6TS,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=25,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=constant_with_warmup,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10000,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./result_distiling_new_uk_data_3,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=24,\n",
      "per_device_train_batch_size=24,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./result_distiling_new_uk_data_3,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "temperature=2.0,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=250,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "accelerator, model_dtype= prepare_accelerator(input_dtype=training_args.dtype,\n",
    "                                                training_args=training_args, data_args=data_args, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef778b1-6b8e-4869-9d5e-826742fef631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5335a96-b4d0-4198-827f-cba2ef4d94ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if training_args.output_dir is not None:\n",
    "    os.makedirs(training_args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eecf625-6143-4d42-ae37-7968aeef3664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-medium\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 24,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 24,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading configuration file preprocessor_config.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\vocab.json\n",
      "loading file tokenizer.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\tokenizer.json\n",
      "loading file merges.txt from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\merges.txt\n",
      "loading file normalizer.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\normalizer.json\n",
      "loading file added_tokens.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file preprocessor_config.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\vocab.json\n",
      "loading file tokenizer.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\tokenizer.json\n",
      "loading file merges.txt from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\merges.txt\n",
      "loading file normalizer.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\normalizer.json\n",
      "loading file added_tokens.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at ./model_cache/models--openai--whisper-medium\\snapshots\\abdf7c39ab9d0397620ccaea8974cc764cd0953e\\tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config, feature_extractor, tokenizer = load_config_feature_ext_tokenizer(\n",
    "    model_name_or_path=\"openai/whisper-medium\", model_args=model_args)\n",
    "\n",
    "processor = load_processor(processor_path=\"openai/whisper-medium\", model_args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e94fae2c-324c-462f-bf2a-0703cedb9fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./local_whisper_medium\\config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-medium\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 24,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 24,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file ./local_whisper_medium\\model.safetensors\n",
      "Instantiating WhisperForConditionalGeneration model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"max_length\": 448,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ]\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at ./local_whisper_medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./local_whisper_medium\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"alignment_heads\": [\n",
      "    [\n",
      "      13,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      16,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      20,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      23,\n",
      "      4\n",
      "    ]\n",
      "  ],\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ]\n",
      "  ],\n",
      "  \"is_multilingual\": true,\n",
      "  \"lang_to_id\": {\n",
      "    \"<|af|>\": 50327,\n",
      "    \"<|am|>\": 50334,\n",
      "    \"<|ar|>\": 50272,\n",
      "    \"<|as|>\": 50350,\n",
      "    \"<|az|>\": 50304,\n",
      "    \"<|ba|>\": 50355,\n",
      "    \"<|be|>\": 50330,\n",
      "    \"<|bg|>\": 50292,\n",
      "    \"<|bn|>\": 50302,\n",
      "    \"<|bo|>\": 50347,\n",
      "    \"<|br|>\": 50309,\n",
      "    \"<|bs|>\": 50315,\n",
      "    \"<|ca|>\": 50270,\n",
      "    \"<|cs|>\": 50283,\n",
      "    \"<|cy|>\": 50297,\n",
      "    \"<|da|>\": 50285,\n",
      "    \"<|de|>\": 50261,\n",
      "    \"<|el|>\": 50281,\n",
      "    \"<|en|>\": 50259,\n",
      "    \"<|es|>\": 50262,\n",
      "    \"<|et|>\": 50307,\n",
      "    \"<|eu|>\": 50310,\n",
      "    \"<|fa|>\": 50300,\n",
      "    \"<|fi|>\": 50277,\n",
      "    \"<|fo|>\": 50338,\n",
      "    \"<|fr|>\": 50265,\n",
      "    \"<|gl|>\": 50319,\n",
      "    \"<|gu|>\": 50333,\n",
      "    \"<|haw|>\": 50352,\n",
      "    \"<|ha|>\": 50354,\n",
      "    \"<|he|>\": 50279,\n",
      "    \"<|hi|>\": 50276,\n",
      "    \"<|hr|>\": 50291,\n",
      "    \"<|ht|>\": 50339,\n",
      "    \"<|hu|>\": 50286,\n",
      "    \"<|hy|>\": 50312,\n",
      "    \"<|id|>\": 50275,\n",
      "    \"<|is|>\": 50311,\n",
      "    \"<|it|>\": 50274,\n",
      "    \"<|ja|>\": 50266,\n",
      "    \"<|jw|>\": 50356,\n",
      "    \"<|ka|>\": 50329,\n",
      "    \"<|kk|>\": 50316,\n",
      "    \"<|km|>\": 50323,\n",
      "    \"<|kn|>\": 50306,\n",
      "    \"<|ko|>\": 50264,\n",
      "    \"<|la|>\": 50294,\n",
      "    \"<|lb|>\": 50345,\n",
      "    \"<|ln|>\": 50353,\n",
      "    \"<|lo|>\": 50336,\n",
      "    \"<|lt|>\": 50293,\n",
      "    \"<|lv|>\": 50301,\n",
      "    \"<|mg|>\": 50349,\n",
      "    \"<|mi|>\": 50295,\n",
      "    \"<|mk|>\": 50308,\n",
      "    \"<|ml|>\": 50296,\n",
      "    \"<|mn|>\": 50314,\n",
      "    \"<|mr|>\": 50320,\n",
      "    \"<|ms|>\": 50282,\n",
      "    \"<|mt|>\": 50343,\n",
      "    \"<|my|>\": 50346,\n",
      "    \"<|ne|>\": 50313,\n",
      "    \"<|nl|>\": 50271,\n",
      "    \"<|nn|>\": 50342,\n",
      "    \"<|no|>\": 50288,\n",
      "    \"<|oc|>\": 50328,\n",
      "    \"<|pa|>\": 50321,\n",
      "    \"<|pl|>\": 50269,\n",
      "    \"<|ps|>\": 50340,\n",
      "    \"<|pt|>\": 50267,\n",
      "    \"<|ro|>\": 50284,\n",
      "    \"<|ru|>\": 50263,\n",
      "    \"<|sa|>\": 50344,\n",
      "    \"<|sd|>\": 50332,\n",
      "    \"<|si|>\": 50322,\n",
      "    \"<|sk|>\": 50298,\n",
      "    \"<|sl|>\": 50305,\n",
      "    \"<|sn|>\": 50324,\n",
      "    \"<|so|>\": 50326,\n",
      "    \"<|sq|>\": 50317,\n",
      "    \"<|sr|>\": 50303,\n",
      "    \"<|su|>\": 50357,\n",
      "    \"<|sv|>\": 50273,\n",
      "    \"<|sw|>\": 50318,\n",
      "    \"<|ta|>\": 50287,\n",
      "    \"<|te|>\": 50299,\n",
      "    \"<|tg|>\": 50331,\n",
      "    \"<|th|>\": 50289,\n",
      "    \"<|tk|>\": 50341,\n",
      "    \"<|tl|>\": 50348,\n",
      "    \"<|tr|>\": 50268,\n",
      "    \"<|tt|>\": 50351,\n",
      "    \"<|uk|>\": 50280,\n",
      "    \"<|ur|>\": 50290,\n",
      "    \"<|uz|>\": 50337,\n",
      "    \"<|vi|>\": 50278,\n",
      "    \"<|yi|>\": 50335,\n",
      "    \"<|yo|>\": 50325,\n",
      "    \"<|zh|>\": 50260\n",
      "  },\n",
      "  \"max_initial_timestamp_index\": 50,\n",
      "  \"max_length\": 448,\n",
      "  \"no_timestamps_token_id\": 50363,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"prev_sot_token_id\": 50361,\n",
      "  \"return_timestamps\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"task_to_id\": {\n",
      "    \"transcribe\": 50359,\n",
      "    \"translate\": 50358\n",
      "  }\n",
      "}\n",
      "\n",
      "loading weights file ./models_dir/student_moz_uk\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"max_length\": 448,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ]\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at ./models_dir/student_moz_uk.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./models_dir/student_moz_uk\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"alignment_heads\": [\n",
      "    [\n",
      "      13,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      16,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      20,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      23,\n",
      "      4\n",
      "    ]\n",
      "  ],\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ]\n",
      "  ],\n",
      "  \"is_multilingual\": true,\n",
      "  \"lang_to_id\": {\n",
      "    \"<|af|>\": 50327,\n",
      "    \"<|am|>\": 50334,\n",
      "    \"<|ar|>\": 50272,\n",
      "    \"<|as|>\": 50350,\n",
      "    \"<|az|>\": 50304,\n",
      "    \"<|ba|>\": 50355,\n",
      "    \"<|be|>\": 50330,\n",
      "    \"<|bg|>\": 50292,\n",
      "    \"<|bn|>\": 50302,\n",
      "    \"<|bo|>\": 50347,\n",
      "    \"<|br|>\": 50309,\n",
      "    \"<|bs|>\": 50315,\n",
      "    \"<|ca|>\": 50270,\n",
      "    \"<|cs|>\": 50283,\n",
      "    \"<|cy|>\": 50297,\n",
      "    \"<|da|>\": 50285,\n",
      "    \"<|de|>\": 50261,\n",
      "    \"<|el|>\": 50281,\n",
      "    \"<|en|>\": 50259,\n",
      "    \"<|es|>\": 50262,\n",
      "    \"<|et|>\": 50307,\n",
      "    \"<|eu|>\": 50310,\n",
      "    \"<|fa|>\": 50300,\n",
      "    \"<|fi|>\": 50277,\n",
      "    \"<|fo|>\": 50338,\n",
      "    \"<|fr|>\": 50265,\n",
      "    \"<|gl|>\": 50319,\n",
      "    \"<|gu|>\": 50333,\n",
      "    \"<|haw|>\": 50352,\n",
      "    \"<|ha|>\": 50354,\n",
      "    \"<|he|>\": 50279,\n",
      "    \"<|hi|>\": 50276,\n",
      "    \"<|hr|>\": 50291,\n",
      "    \"<|ht|>\": 50339,\n",
      "    \"<|hu|>\": 50286,\n",
      "    \"<|hy|>\": 50312,\n",
      "    \"<|id|>\": 50275,\n",
      "    \"<|is|>\": 50311,\n",
      "    \"<|it|>\": 50274,\n",
      "    \"<|ja|>\": 50266,\n",
      "    \"<|jw|>\": 50356,\n",
      "    \"<|ka|>\": 50329,\n",
      "    \"<|kk|>\": 50316,\n",
      "    \"<|km|>\": 50323,\n",
      "    \"<|kn|>\": 50306,\n",
      "    \"<|ko|>\": 50264,\n",
      "    \"<|la|>\": 50294,\n",
      "    \"<|lb|>\": 50345,\n",
      "    \"<|ln|>\": 50353,\n",
      "    \"<|lo|>\": 50336,\n",
      "    \"<|lt|>\": 50293,\n",
      "    \"<|lv|>\": 50301,\n",
      "    \"<|mg|>\": 50349,\n",
      "    \"<|mi|>\": 50295,\n",
      "    \"<|mk|>\": 50308,\n",
      "    \"<|ml|>\": 50296,\n",
      "    \"<|mn|>\": 50314,\n",
      "    \"<|mr|>\": 50320,\n",
      "    \"<|ms|>\": 50282,\n",
      "    \"<|mt|>\": 50343,\n",
      "    \"<|my|>\": 50346,\n",
      "    \"<|ne|>\": 50313,\n",
      "    \"<|nl|>\": 50271,\n",
      "    \"<|nn|>\": 50342,\n",
      "    \"<|no|>\": 50288,\n",
      "    \"<|oc|>\": 50328,\n",
      "    \"<|pa|>\": 50321,\n",
      "    \"<|pl|>\": 50269,\n",
      "    \"<|ps|>\": 50340,\n",
      "    \"<|pt|>\": 50267,\n",
      "    \"<|ro|>\": 50284,\n",
      "    \"<|ru|>\": 50263,\n",
      "    \"<|sa|>\": 50344,\n",
      "    \"<|sd|>\": 50332,\n",
      "    \"<|si|>\": 50322,\n",
      "    \"<|sk|>\": 50298,\n",
      "    \"<|sl|>\": 50305,\n",
      "    \"<|sn|>\": 50324,\n",
      "    \"<|so|>\": 50326,\n",
      "    \"<|sq|>\": 50317,\n",
      "    \"<|sr|>\": 50303,\n",
      "    \"<|su|>\": 50357,\n",
      "    \"<|sv|>\": 50273,\n",
      "    \"<|sw|>\": 50318,\n",
      "    \"<|ta|>\": 50287,\n",
      "    \"<|te|>\": 50299,\n",
      "    \"<|tg|>\": 50331,\n",
      "    \"<|th|>\": 50289,\n",
      "    \"<|tk|>\": 50341,\n",
      "    \"<|tl|>\": 50348,\n",
      "    \"<|tr|>\": 50268,\n",
      "    \"<|tt|>\": 50351,\n",
      "    \"<|uk|>\": 50280,\n",
      "    \"<|ur|>\": 50290,\n",
      "    \"<|uz|>\": 50337,\n",
      "    \"<|vi|>\": 50278,\n",
      "    \"<|yi|>\": 50335,\n",
      "    \"<|yo|>\": 50325,\n",
      "    \"<|zh|>\": 50260\n",
      "  },\n",
      "  \"max_initial_timestamp_index\": 50,\n",
      "  \"max_length\": 448,\n",
      "  \"no_timestamps_token_id\": 50363,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"prev_sot_token_id\": 50361,\n",
      "  \"return_timestamps\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"task_to_id\": {\n",
      "    \"transcribe\": 50359,\n",
      "    \"translate\": 50358\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# teacher_model = load_whisper_model(model_args.teacher_model_name_or_path, model_args, dtype=model_dtype)\n",
    "# student_model = load_whisper_model(model_args.model_name_or_path, model_args, dtype=model_dtype)\n",
    "\n",
    "teacher_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_args.teacher_model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    token=model_args.token,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=model_dtype,\n",
    ")\n",
    "\n",
    "student_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    subfolder=model_args.subfolder,\n",
    "    token=model_args.token,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# teacher_model.save_pretrained('./local_whisper_medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e0f764-abca-497e-a0c0-b5a54d823a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if student_model.config.decoder_start_token_id is None or teacher_model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\n",
    "        f\"Make sure that `config.decoder_start_token_id` is correctly defined for both the \"\n",
    "        f\"student and teacher model. Got {student_model.config.decoder_start_token_id} for the \"\n",
    "        f\"student and {teacher_model.config.decoder_start_token_id} for the teacher.\"\n",
    "    )\n",
    "\n",
    "share_hidden_states = training_args.freeze_encoder and student_model.config.d_model == teacher_model.config.d_model\n",
    "\n",
    "# enable gradient checkpointing if necessary\n",
    "if training_args.gradient_checkpointing:\n",
    "    student_model.gradient_checkpointing_enable()\n",
    "\n",
    "# freeze student encoder if necessary\n",
    "if training_args.freeze_encoder:\n",
    "    student_model.freeze_encoder()\n",
    "    student_model.model.encoder.gradient_checkpointing = False\n",
    "\n",
    "# if share_hidden_states:\n",
    "#     # tie the weights for the teacher encoder if we're freezing the student and it's the same as the teacher\n",
    "#     teacher_model.model.encoder = student_model.model.encoder\n",
    "\n",
    "is_multilingual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429b70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extractor saved in ./result_distiling_new_uk_data_3\\preprocessor_config.json\n",
      "tokenizer config file saved in ./result_distiling_new_uk_data_3\\tokenizer_config.json\n",
      "Special tokens file saved in ./result_distiling_new_uk_data_3\\special_tokens_map.json\n",
      "Configuration saved in ./result_distiling_new_uk_data_3\\config.json\n",
      "Configuration saved in ./result_distiling_new_uk_data_3\\generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# 8. Create a single speech processor - make sure all processes wait until data is saved\n",
    "if accelerator.is_main_process:\n",
    "    feature_extractor.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    # save the config and generation config as well\n",
    "    config.save_pretrained(training_args.output_dir)\n",
    "    student_model.generation_config.save_pretrained(training_args.output_dir)\n",
    "\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a33683d-4976-45ab-a540-c7681eb9774b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./result_distiling_new_uk_data_3\\preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file tokenizer.json\n",
      "loading file merges.txt\n",
      "loading file normalizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "410b5932-8f58-47af-b4de-f13737e8d700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 9. Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,\n",
    "# so we just need to set the correct target sampling rate.\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "raw_datasets = raw_datasets.cast_column(\n",
    "    data_args.audio_column_name,\n",
    "    datasets.features.Audio(sampling_rate=sampling_rate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "902539a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = prepare_normilazer(language=data_args.language, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "589600c8-9014-41ca-8a2a-fc7516d51122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Preprocessing the datasets: we need to read the audio files as arrays and tokenize the targets.\n",
    "# 10.1: Define the pre-processing constants\n",
    "max_input_length = int(data_args.max_duration_in_seconds * sampling_rate)\n",
    "min_input_length = int(data_args.min_duration_in_seconds * sampling_rate)\n",
    "max_label_length = (\n",
    "    data_args.max_label_length if data_args.max_label_length is not None else student_model.config.max_length\n",
    ")\n",
    "\n",
    "timestamp_probability = data_args.timestamp_probability\n",
    "condition_on_prev_probability = data_args.condition_on_prev_probability\n",
    "return_timestamps = data_args.return_timestamps if timestamp_probability > 0 else False\n",
    "\n",
    "timestamp_ids = tokenizer.timestamp_ids()\n",
    "timestamp_begin = tokenizer.all_special_ids[-1]\n",
    "timestamp_position = 3 if is_multilingual else 1\n",
    "\n",
    "decoder_start_token_id = student_model.config.decoder_start_token_id  # <|startoftranscript|>\n",
    "decoder_prev_token_id = tokenizer.all_special_ids[-3]  # <|startofprev|>\n",
    "decoder_eot_token_id = tokenizer.eos_token_id\n",
    "\n",
    "language = data_args.language\n",
    "task = data_args.task\n",
    "\n",
    "num_workers = data_args.preprocessing_num_workers\n",
    "dataloader_num_workers = training_args.dataloader_num_workers\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b6dae6c-60dc-44fe-ba50-75f9d296bc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wer_threshold = 10#data_args.wer_threshold\n",
    "\n",
    "# 10.3: filter training data based on WER threshold -> this is KEY to good distillation performance\n",
    "def is_wer_in_range(ground_truth, whisper_transcript,\n",
    "                     tokenizer=tokenizer, normalizer=normalizer, wer_threshold=wer_threshold,\n",
    "                     metric=metric):\n",
    "    norm_ground_truth = normalizer(ground_truth)\n",
    "    if (\n",
    "        isinstance(whisper_transcript, str)\n",
    "        and whisper_transcript.startswith(\"[\")\n",
    "        and whisper_transcript.endswith(\"]\")\n",
    "    ):\n",
    "        whisper_transcript = re.findall(r\"\\d+\", whisper_transcript)\n",
    "        whisper_transcript = [int(token) for token in whisper_transcript]\n",
    "    if isinstance(whisper_transcript, list):\n",
    "        whisper_transcript = tokenizer.decode(whisper_transcript, skip_special_tokens=True)\n",
    "    if len(norm_ground_truth) > 0 and whisper_transcript is not None:\n",
    "        norm_whisper_transcript = normalizer(whisper_transcript)\n",
    "        wer = 100 * metric.compute(predictions=[norm_whisper_transcript], references=[norm_ground_truth])\n",
    "        return wer < wer_threshold\n",
    "    else:\n",
    "        # filter automatically since we can't know the WER\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee37ac3-514c-40c7-ae03-eeb89774b113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 100\n",
      "train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620084d42a9b4fff98cfc064786f5c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by wer:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c16c4db1f3a44c7b014436ba561ada1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by wer:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 35\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'].num_rows, raw_datasets['eval'].num_rows)\n",
    "\n",
    "for split_for_wer_filter in [\n",
    "    'train', 'eval'\n",
    "]:\n",
    "    print(split_for_wer_filter\n",
    "    )\n",
    "    filter_by_wer_threshold = partial(\n",
    "        raw_datasets[split_for_wer_filter].filter,\n",
    "        function=is_wer_in_range,\n",
    "        input_columns=[\"text\", \"whisper_transcript\"],\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "    if wer_threshold is not None:\n",
    "        raw_datasets[split_for_wer_filter] = filter_by_wer_threshold(num_proc=1, desc=\"filtering train dataset by wer\")\n",
    "    \n",
    "print(raw_datasets['train'].num_rows, raw_datasets['eval'].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8ac6073-4540-4556-aeaa-69d9fcac072a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10.4: pre-process training/evaluation datasets\n",
    "def has_timestamp_tokens(input_str):\n",
    "    \"\"\"\n",
    "    Identify whether the input string contains timestamp tokens, of the form <|0.00|>, by searching for\n",
    "    pairs of left and right-angle brackets.\n",
    "    \"\"\"\n",
    "    return bool(re.search(\"\\<[^\\>]*\\>\", input_str))\n",
    "\n",
    "def prepare_train_dataset(batch):\n",
    "    \"\"\"\n",
    "    Pre-process the raw dataset in a three stage process:\n",
    "        1. Convert the audio arrays to log-mel spectrogram inputs\n",
    "        2. Possibly filter the timestamp tokens from the token ids (depending on the timestamp probability)\n",
    "        3. Possibly add prompt tokens if conditioning on previous text (depending on the conditioning probability)\n",
    "    TODO(SG): see whether we can 'pack' the audio inputs closer to 30 second chunks\n",
    "    \"\"\"\n",
    "    # process audio input\n",
    "    audio = [sample[\"array\"] for sample in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(audio, sampling_rate=sampling_rate)\n",
    "    batch[\"input_features\"] = inputs.input_features\n",
    "    batch[\"input_length\"] = [len(sample) for sample in audio]\n",
    "\n",
    "    # process text targets - for training these are the Whisper-generated pseudo-labels\n",
    "    input_str_batched = batch[\"whisper_transcript\"]\n",
    "\n",
    "    all_token_ids = []\n",
    "    all_token_ids_unprompted = []\n",
    "    for input_str in input_str_batched:\n",
    "        if isinstance(input_str, list):\n",
    "            # pseudo-labelled transcriptions have been retained as token ids (`decode_token_ids=False`)\n",
    "            token_ids = input_str\n",
    "        elif input_str[0].startswith(\"[\") and input_str[0].endswith(\"]\"):\n",
    "            token_ids = re.findall(r\"\\d+\", input_str)\n",
    "            token_ids = [int(token) for token in token_ids]\n",
    "        else:\n",
    "            token_ids = None\n",
    "\n",
    "        if token_ids is not None:\n",
    "            # remove the EOT tokens to get the 'true' token length\n",
    "            token_ids = [token for token in token_ids if token != decoder_eot_token_id]\n",
    "            token_ids = token_ids + [decoder_eot_token_id]\n",
    "            # check whether we have timestamps in the PLs and filter if required\n",
    "            has_timestamps = len(set(token_ids) & set(timestamp_ids)) > 0\n",
    "            if has_timestamps:\n",
    "                # sample from binomial distribution to get probability of training on timestamps\n",
    "                predict_timestamps = bool(np.random.binomial(1, timestamp_probability))\n",
    "                if not predict_timestamps:\n",
    "                    # filter timestamps and insert the <|notimestamps|> task token\n",
    "                    token_ids = [token for token in token_ids if token < timestamp_begin]\n",
    "                    token_ids.insert(timestamp_position, timestamp_begin)\n",
    "        else:\n",
    "            # pseudo-labelled transcriptions have been decoded to text (`decode_token_ids=True`)\n",
    "            has_timestamps = has_timestamp_tokens(input_str)\n",
    "\n",
    "            if has_timestamps:\n",
    "                predict_timestamps = bool(np.random.binomial(1, timestamp_probability))\n",
    "                if not predict_timestamps:\n",
    "                    # filter timestamp token ids if not part of the prediction task\n",
    "                    input_str = tokenizer._filter_timestamp_ids(input_str)\n",
    "            else:\n",
    "                predict_timestamps = False\n",
    "\n",
    "            tokenizer.set_prefix_tokens(language=language, task=task, predict_timestamps=predict_timestamps)\n",
    "            token_ids = tokenizer(input_str).input_ids\n",
    "\n",
    "        all_token_ids_unprompted.append(token_ids)\n",
    "        # check whether to condition on previous text - we do this with probability condition_on_prev_probability\n",
    "        condition_on_prev = bool(np.random.binomial(1, condition_on_prev_probability))\n",
    "        if condition_on_prev and len(all_token_ids_unprompted) > 1:\n",
    "            # prompt ids are the penultimate token ids in the batch\n",
    "            prompt_ids = all_token_ids_unprompted[-2]\n",
    "            # strip timestamp tokens from prompt\n",
    "            prompt_ids = [token for token in prompt_ids if token < timestamp_begin]\n",
    "            if len(prompt_ids) > 0:\n",
    "                # remove the standard task tokens and add the special <|startofprev|> token\n",
    "                prompt_ids = [decoder_prev_token_id] + prompt_ids[timestamp_position:-1]\n",
    "            if len(prompt_ids + token_ids) < max_label_length:\n",
    "                token_ids = prompt_ids + token_ids\n",
    "        all_token_ids.append(token_ids)\n",
    "\n",
    "    batch[\"labels\"] = all_token_ids\n",
    "    return batch\n",
    "\n",
    "# def prepare_eval_dataset(batch):\n",
    "#     # process audio input\n",
    "#     sample = batch[\"audio\"]\n",
    "#     inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "#     batch[\"input_features\"] = inputs.input_features[0]\n",
    "#     batch[\"input_length\"] = len(sample[\"array\"])\n",
    "\n",
    "#     # process targets - for evaluation these are the ground-truth transcriptions\n",
    "#     input_str = batch[\"text\"]\n",
    "#     batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be2d1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_length = (\n",
    "    data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length\n",
    ")\n",
    "audio_column_name = data_args.audio_column_name\n",
    "num_workers = data_args.preprocessing_num_workers\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "id_column_name = 'path'#data_args.id_column_name\n",
    "\n",
    "def prepare_dataset(batch, audio_column_name=audio_column_name,\n",
    "  model_input_name=model_input_name, \n",
    "  id_column_name=id_column_name, feature_extractor=feature_extractor, tokenizer=tokenizer):\n",
    "\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    batch[\"input_length\"] = len(sample[\"array\"])\n",
    "\n",
    "    batch[model_input_name] = inputs.get(model_input_name)[0]\n",
    "\n",
    "    batch[\"file_id\"] = tokenizer(batch[id_column_name], add_special_tokens=False).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b913a62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5135a49a7a634c6287a0753eed552fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess train dataset:   0%|          | 0/84 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60afacdb2c44ebdbf2e22b397d44da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess eval dataset (num_proc=4):   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_datasets = DatasetDict()\n",
    "\n",
    "# map_fn_eval = partial(\n",
    "#     raw_datasets['train'].map, function=prepare_dataset,\n",
    "# )\n",
    "\n",
    "# vectorized_datasets['train'] = map_fn_eval(num_proc=4, desc=\"preprocess eval dataset\")\n",
    "# raw_datasets_train_features = list(raw_datasets[\"train\"].features.keys())\n",
    "map_fn_train = partial(\n",
    "    raw_datasets['train'].map,\n",
    "    function=prepare_train_dataset,\n",
    "    # remove_columns=raw_datasets_train_features,\n",
    "    batched=True,\n",
    "    batch_size=max(training_args.per_device_train_batch_size // 4, 4),  # TODO(SG) make data prep bs configurable\n",
    ")\n",
    "vectorized_datasets[\"train\"] = map_fn_train(num_proc=1, desc=\"preprocess train dataset\")\n",
    "\n",
    "map_fn_eval = partial(\n",
    "    raw_datasets['eval'].map, function=prepare_dataset,\n",
    ")\n",
    "\n",
    "vectorized_datasets['eval'] = map_fn_eval(num_proc=4, desc=\"preprocess eval dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed788686-1409-4a04-a868-23200ed145db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb255172f9ff4010b9d44e88df655409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by audio length:   0%|          | 0/84 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f20938d44b4c6cb77ec5235a5a066d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset by audio length:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443b4f420c66409eb3ff43cd2fa5be7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset:   0%|          | 0/84 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1016f6644c84040885862d3011b96f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtering train dataset:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 35\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'].num_rows, raw_datasets['eval'].num_rows)\n",
    "\n",
    "# 10.5: Filter training data with inputs longer than `max_input_length`\n",
    "def is_audio_in_length_range(length):\n",
    "    return min_input_length < length < max_input_length\n",
    "\n",
    "filter_by_audio_fn = partial(\n",
    "    vectorized_datasets.filter, function=is_audio_in_length_range, input_columns=[\"input_length\"]\n",
    ")\n",
    "vectorized_datasets = filter_by_audio_fn(num_proc=1, desc=\"filtering train dataset by audio length\")\n",
    "\n",
    "# 10.6: Filter training data with labels longer than `max_label_length`\n",
    "def is_labels_in_length_range(labels):\n",
    "    return 0 < len(labels) <= max_label_length\n",
    "\n",
    "filter_by_labels_fn = partial(\n",
    "    vectorized_datasets.filter, function=is_labels_in_length_range, input_columns=[\"labels\"]\n",
    ")\n",
    "vectorized_datasets = filter_by_labels_fn(num_proc=1, desc=\"filtering train dataset\")\n",
    "\n",
    "print(raw_datasets['train'].num_rows, raw_datasets['eval'].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c7beaf6-5e5a-423c-8d83-a7a30cbac35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2024 02:30:00 - INFO - __main__ - Data preprocessing finished. Files cached at {'train': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled_uk_2\\\\train\\\\cache-eb6c28bf705b9981.arrow'}, {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled_uk_2\\\\train\\\\cache-5677e89877c4f812.arrow'}], 'eval': [{'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled_uk_2\\\\test\\\\cache-3e2ca0520983045f_00000_of_00004.arrow'}, {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled_uk_2\\\\test\\\\cache-3e2ca0520983045f_00001_of_00004.arrow'}, {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled_uk_2\\\\test\\\\cache-3e2ca0520983045f_00002_of_00004.arrow'}, {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled_uk_2\\\\test\\\\cache-3e2ca0520983045f_00003_of_00004.arrow'}, {'filename': 'f:\\\\distiling_whisper_local\\\\dataset_saved\\\\labeled_uk_2\\\\test\\\\cache-1c885a0073648e9f.arrow'}]}.\n"
     ]
    }
   ],
   "source": [
    "cache = {k: v.cache_files for k, v in vectorized_datasets.items()}\n",
    "logger.info(f\"Data preprocessing finished. Files cached at {cache}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614234ed-402c-4847-8406-fc2f1e4b8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define Evaluation Metrics\n",
    "def compute_metrics(preds, labels):\n",
    "    # replace padded labels by the padding token\n",
    "    for idx in range(len(labels)):\n",
    "        labels[idx][labels[idx] == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True, decode_with_timestamps=return_timestamps)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # normalize everything and re-compute the WER\n",
    "    norm_pred_str = [normalizer(pred) for pred in pred_str]\n",
    "    norm_label_str = [normalizer(label) for label in label_str]\n",
    "    # for logging, we need the pred/labels to match the norm_pred/norm_labels, so discard any filtered samples here\n",
    "    pred_str = [pred_str[i] for i in range(len(norm_pred_str)) if len(norm_label_str[i]) > 0]\n",
    "    label_str = [label_str[i] for i in range(len(norm_label_str)) if len(norm_label_str[i]) > 0]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero normalized references:\n",
    "    norm_pred_str = [norm_pred_str[i] for i in range(len(norm_pred_str)) if len(norm_label_str[i]) > 0]\n",
    "    norm_label_str = [norm_label_str[i] for i in range(len(norm_label_str)) if len(norm_label_str[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=norm_pred_str, references=norm_label_str)\n",
    "    return {\"wer\": wer, \"wer_ortho\": wer_ortho}, pred_str, label_str, norm_pred_str, norm_label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1854cb9b-8362-43c8-b824-a67e12386493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2024 02:30:00 - INFO - __main__ - max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# 12. Define Training Schedule\n",
    "# Store some constants\n",
    "per_device_train_batch_size = int(training_args.per_device_train_batch_size)\n",
    "train_batch_size = per_device_train_batch_size * accelerator.num_processes\n",
    "gradient_accumulation_steps = int(training_args.gradient_accumulation_steps)\n",
    "per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
    "\n",
    "if not data_args.streaming and training_args.max_steps < 0:\n",
    "    num_epochs = int(training_args.num_train_epochs)\n",
    "    steps_per_epoch = len(vectorized_datasets[\"train\"]) // (train_batch_size * gradient_accumulation_steps)\n",
    "    total_train_steps = steps_per_epoch * num_epochs\n",
    "elif training_args.max_steps > 0:\n",
    "    logger.info(\"max_steps is given, it will override any value given in num_train_epochs\")\n",
    "    total_train_steps = int(training_args.max_steps)\n",
    "    # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n",
    "    num_epochs = sys.maxsize\n",
    "    steps_per_epoch = total_train_steps\n",
    "else:\n",
    "    raise ValueError(\"max_steps must be specified when training with a streaming (iterable) dataset\")\n",
    "\n",
    "if training_args.eval_steps is None:\n",
    "    logger.info(\n",
    "        f\"eval_steps is not set, evaluating at the end of {'each epoch' if not data_args.streaming else 'training'}\"\n",
    "    )\n",
    "    eval_steps = steps_per_epoch\n",
    "else:\n",
    "    eval_steps = training_args.eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6f8ecd3-10b4-4f70-a8cc-44ac875ecc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Define optimizer, LR scheduler, collator\n",
    "decay_parameters = get_parameter_names(\n",
    "    student_model,\n",
    "    [nn.LayerNorm],\n",
    "    forbidden_module=[student_model.model.encoder] if training_args.freeze_encoder else None,\n",
    ")\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [param for name, param in student_model.named_parameters() if name in decay_parameters],\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [param for name, param in student_model.named_parameters() if name not in decay_parameters],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=optimizer_grouped_parameters,\n",
    "    lr=training_args.learning_rate,\n",
    "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "    eps=training_args.adam_epsilon,\n",
    ")\n",
    "\n",
    "# LR scheduler gets stepped by `num_processes` each time -> account for this in warmup / total steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=training_args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=total_train_steps * accelerator.num_processes,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=decoder_start_token_id,\n",
    "    decoder_prev_token_id=decoder_prev_token_id,\n",
    "    input_padding=\"longest\",\n",
    "    target_padding=\"max_length\",\n",
    "    max_target_length=max_label_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79087ec-db1e-47c6-939f-87384fb39fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Define generation arguments - we need to do this before we wrap the models in DDP\n",
    "# so that we can still access the configs\n",
    "num_beams = (\n",
    "    training_args.generation_num_beams\n",
    "    if training_args.generation_num_beams is not None\n",
    "    else getattr(student_model.generation_config, \"num_beams\", 1)\n",
    ")\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"max_length\": max_label_length,\n",
    "    \"num_beams\": 1,\n",
    "    \"return_timestamps\": data_args.return_timestamps,\n",
    "    \"language\": data_args.language,\n",
    "    \"task\": data_args.task,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bea554d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./local_whisper_medium\\config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-medium\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 24,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 24,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file ./local_whisper_medium\\model.safetensors\n",
      "Instantiating WhisperForConditionalGeneration model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"max_length\": 448,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ]\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at ./local_whisper_medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./local_whisper_medium\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"alignment_heads\": [\n",
      "    [\n",
      "      13,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      15,\n",
      "      15\n",
      "    ],\n",
      "    [\n",
      "      16,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      20,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      23,\n",
      "      4\n",
      "    ]\n",
      "  ],\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ]\n",
      "  ],\n",
      "  \"is_multilingual\": true,\n",
      "  \"lang_to_id\": {\n",
      "    \"<|af|>\": 50327,\n",
      "    \"<|am|>\": 50334,\n",
      "    \"<|ar|>\": 50272,\n",
      "    \"<|as|>\": 50350,\n",
      "    \"<|az|>\": 50304,\n",
      "    \"<|ba|>\": 50355,\n",
      "    \"<|be|>\": 50330,\n",
      "    \"<|bg|>\": 50292,\n",
      "    \"<|bn|>\": 50302,\n",
      "    \"<|bo|>\": 50347,\n",
      "    \"<|br|>\": 50309,\n",
      "    \"<|bs|>\": 50315,\n",
      "    \"<|ca|>\": 50270,\n",
      "    \"<|cs|>\": 50283,\n",
      "    \"<|cy|>\": 50297,\n",
      "    \"<|da|>\": 50285,\n",
      "    \"<|de|>\": 50261,\n",
      "    \"<|el|>\": 50281,\n",
      "    \"<|en|>\": 50259,\n",
      "    \"<|es|>\": 50262,\n",
      "    \"<|et|>\": 50307,\n",
      "    \"<|eu|>\": 50310,\n",
      "    \"<|fa|>\": 50300,\n",
      "    \"<|fi|>\": 50277,\n",
      "    \"<|fo|>\": 50338,\n",
      "    \"<|fr|>\": 50265,\n",
      "    \"<|gl|>\": 50319,\n",
      "    \"<|gu|>\": 50333,\n",
      "    \"<|haw|>\": 50352,\n",
      "    \"<|ha|>\": 50354,\n",
      "    \"<|he|>\": 50279,\n",
      "    \"<|hi|>\": 50276,\n",
      "    \"<|hr|>\": 50291,\n",
      "    \"<|ht|>\": 50339,\n",
      "    \"<|hu|>\": 50286,\n",
      "    \"<|hy|>\": 50312,\n",
      "    \"<|id|>\": 50275,\n",
      "    \"<|is|>\": 50311,\n",
      "    \"<|it|>\": 50274,\n",
      "    \"<|ja|>\": 50266,\n",
      "    \"<|jw|>\": 50356,\n",
      "    \"<|ka|>\": 50329,\n",
      "    \"<|kk|>\": 50316,\n",
      "    \"<|km|>\": 50323,\n",
      "    \"<|kn|>\": 50306,\n",
      "    \"<|ko|>\": 50264,\n",
      "    \"<|la|>\": 50294,\n",
      "    \"<|lb|>\": 50345,\n",
      "    \"<|ln|>\": 50353,\n",
      "    \"<|lo|>\": 50336,\n",
      "    \"<|lt|>\": 50293,\n",
      "    \"<|lv|>\": 50301,\n",
      "    \"<|mg|>\": 50349,\n",
      "    \"<|mi|>\": 50295,\n",
      "    \"<|mk|>\": 50308,\n",
      "    \"<|ml|>\": 50296,\n",
      "    \"<|mn|>\": 50314,\n",
      "    \"<|mr|>\": 50320,\n",
      "    \"<|ms|>\": 50282,\n",
      "    \"<|mt|>\": 50343,\n",
      "    \"<|my|>\": 50346,\n",
      "    \"<|ne|>\": 50313,\n",
      "    \"<|nl|>\": 50271,\n",
      "    \"<|nn|>\": 50342,\n",
      "    \"<|no|>\": 50288,\n",
      "    \"<|oc|>\": 50328,\n",
      "    \"<|pa|>\": 50321,\n",
      "    \"<|pl|>\": 50269,\n",
      "    \"<|ps|>\": 50340,\n",
      "    \"<|pt|>\": 50267,\n",
      "    \"<|ro|>\": 50284,\n",
      "    \"<|ru|>\": 50263,\n",
      "    \"<|sa|>\": 50344,\n",
      "    \"<|sd|>\": 50332,\n",
      "    \"<|si|>\": 50322,\n",
      "    \"<|sk|>\": 50298,\n",
      "    \"<|sl|>\": 50305,\n",
      "    \"<|sn|>\": 50324,\n",
      "    \"<|so|>\": 50326,\n",
      "    \"<|sq|>\": 50317,\n",
      "    \"<|sr|>\": 50303,\n",
      "    \"<|su|>\": 50357,\n",
      "    \"<|sv|>\": 50273,\n",
      "    \"<|sw|>\": 50318,\n",
      "    \"<|ta|>\": 50287,\n",
      "    \"<|te|>\": 50299,\n",
      "    \"<|tg|>\": 50331,\n",
      "    \"<|th|>\": 50289,\n",
      "    \"<|tk|>\": 50341,\n",
      "    \"<|tl|>\": 50348,\n",
      "    \"<|tr|>\": 50268,\n",
      "    \"<|tt|>\": 50351,\n",
      "    \"<|uk|>\": 50280,\n",
      "    \"<|ur|>\": 50290,\n",
      "    \"<|uz|>\": 50337,\n",
      "    \"<|vi|>\": 50278,\n",
      "    \"<|yi|>\": 50335,\n",
      "    \"<|yo|>\": 50325,\n",
      "    \"<|zh|>\": 50260\n",
      "  },\n",
      "  \"max_initial_timestamp_index\": 50,\n",
      "  \"max_length\": 448,\n",
      "  \"no_timestamps_token_id\": 50363,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"prev_sot_token_id\": 50361,\n",
      "  \"return_timestamps\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50358,\n",
      "    50359,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"task_to_id\": {\n",
      "    \"transcribe\": 50359,\n",
      "    \"translate\": 50358\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      2\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mteacher_model_name_or_path,\n\u001b[0;32m      3\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mcache_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 15. Prepare everything with accelerate\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1213\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[1;34m(self, device_placement, *args)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_megatron_lm(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1214\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_megatron_lm(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m-> 1214\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[0;32m   1215\u001b[0m     )\n\u001b[0;32m   1216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1094\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[1;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m-> 1094\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[0;32m   1096\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1334\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[1;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit precision with CPU or disk offload.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m         )\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_placement \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(model):\n\u001b[1;32m-> 1334\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_mode:\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1337\u001b[0m         DistributedType\u001b[38;5;241m.\u001b[39mMULTI_GPU,\n\u001b[0;32m   1338\u001b[0m         DistributedType\u001b[38;5;241m.\u001b[39mMULTI_NPU,\n\u001b[0;32m   1339\u001b[0m         DistributedType\u001b[38;5;241m.\u001b[39mMULTI_XPU,\n\u001b[0;32m   1340\u001b[0m     ):\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\transformers\\modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m         )\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 810 (3 times)]\u001b[0m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "teacher_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_args.teacher_model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    token=model_args.token,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 15. Prepare everything with accelerate\n",
    "teacher_model = accelerator.prepare(\n",
    "    student_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a943be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model_cache/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4d9917a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./local_whisper_medium'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.teacher_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91f94ce5-d385-486f-a0dd-cf7898e4c9b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 15. Prepare everything with accelerate\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1213\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[1;34m(self, device_placement, *args)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_megatron_lm(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1214\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_megatron_lm(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m-> 1214\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[0;32m   1215\u001b[0m     )\n\u001b[0;32m   1216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1094\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[1;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m-> 1094\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[0;32m   1096\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:1334\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[1;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit precision with CPU or disk offload.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m         )\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_placement \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(model):\n\u001b[1;32m-> 1334\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_mode:\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1337\u001b[0m         DistributedType\u001b[38;5;241m.\u001b[39mMULTI_GPU,\n\u001b[0;32m   1338\u001b[0m         DistributedType\u001b[38;5;241m.\u001b[39mMULTI_NPU,\n\u001b[0;32m   1339\u001b[0m         DistributedType\u001b[38;5;241m.\u001b[39mMULTI_XPU,\n\u001b[0;32m   1340\u001b[0m     ):\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\transformers\\modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m         )\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 810 (3 times)]\u001b[0m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027d324-ace5-4ca4-a886-2cbce2c42d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kl_divergence(target_distribution, log_predicted_distribution, labels):\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"none\")\n",
    "    divergence = kl_loss(log_predicted_distribution, target_distribution)\n",
    "    # ignore padded tokens from divergence, i.e. where labels are not set to -100\n",
    "    padding_mask = labels >= 0\n",
    "    padding_mask = padding_mask.unsqueeze(-1)\n",
    "    divergence = divergence * padding_mask\n",
    "    # take the average over the mini-batch\n",
    "    divergence = divergence.sum() / padding_mask.sum()\n",
    "    return divergence\n",
    "\n",
    "# Define gradient update step fn\n",
    "def train_step(\n",
    "    batch,\n",
    "    temperature=2.0,\n",
    "):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    student_outputs = student_model(**batch)\n",
    "    with torch.no_grad():\n",
    "        if share_hidden_states:\n",
    "            # if the student and teacher share the same frozen encoder then we don't have to recompute the\n",
    "            # encoder hidden-states for the teacher model, we can just re-use from the student\n",
    "            encoder_outputs = BaseModelOutput(student_outputs.encoder_last_hidden_state)\n",
    "            teacher_outputs = teacher_model(encoder_outputs=encoder_outputs, labels=batch[\"labels\"])\n",
    "        else:\n",
    "            # do the full forward pass for the teacher model (encoder + decoder)\n",
    "            teacher_outputs = teacher_model(**batch)\n",
    "\n",
    "    # CE (data) loss\n",
    "    ce_loss = student_outputs.loss\n",
    "    # rescale distribution by temperature to ensure gradients scale correctly\n",
    "    teacher_distribution = nn.functional.softmax(teacher_outputs.logits / temperature, dim=-1)\n",
    "    # log softmax of student predictions for numerical stability\n",
    "    student_distribution = nn.functional.log_softmax(student_outputs.logits / temperature, dim=-1)\n",
    "    # KL-divergence loss (scaled by temperature)\n",
    "    kl_loss = kl_divergence(teacher_distribution, student_distribution, batch[\"labels\"]) * temperature**2\n",
    "\n",
    "    # use Distil-Whisper formulation (fix weight of CE loss and tune KL weight)\n",
    "    loss = 0.8 * ce_loss + training_args.kl_weight * kl_loss\n",
    "    metrics = {\"loss\": loss, \"ce_loss\": ce_loss, \"kl_loss\": kl_loss}\n",
    "    return loss, metrics\n",
    "\n",
    "# Define eval fn\n",
    "def eval_step(batch):\n",
    "    student_model.eval()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student_model(**batch)\n",
    "        if share_hidden_states:\n",
    "            encoder_outputs = BaseModelOutput(student_outputs.encoder_last_hidden_state)\n",
    "            teacher_outputs = teacher_model(encoder_outputs=encoder_outputs, labels=batch[\"labels\"])\n",
    "        else:\n",
    "            teacher_outputs = teacher_model(**batch)\n",
    "\n",
    "    # CE (data) loss\n",
    "    ce_loss = student_outputs.loss\n",
    "\n",
    "    # log softmax / softmax for numerical stability\n",
    "    student_distribution = nn.functional.log_softmax(student_outputs.logits, dim=-1)\n",
    "    teacher_distribution = nn.functional.softmax(teacher_outputs.logits, dim=-1)\n",
    "    # temperature is always 1 for eval\n",
    "    kl_loss = kl_divergence(teacher_distribution, student_distribution, batch[\"labels\"])\n",
    "\n",
    "    # use Distil-Whisper formulation (fix weight of CE loss and tune KL weight)\n",
    "    loss = 0.8 * ce_loss + training_args.kl_weight * kl_loss\n",
    "    metrics = {\"loss\": loss, \"ce_loss\": ce_loss, \"kl_loss\": kl_loss}\n",
    "    return metrics\n",
    "\n",
    "def generate_step(batch):\n",
    "    student_model.eval()\n",
    "    output_ids = accelerator.unwrap_model(student_model).generate(batch[\"input_features\"], **gen_kwargs)\n",
    "    output_ids = accelerator.pad_across_processes(output_ids, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07bf3c-27b4-4f0a-a856-8da6cdb3a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2024 02:06:30 - INFO - __main__ - ***** Running training *****\n",
      "04/25/2024 02:06:30 - INFO - __main__ -   Num examples = 240000\n",
      "04/25/2024 02:06:30 - INFO - __main__ -   Instantaneous batch size per device = 24\n",
      "04/25/2024 02:06:30 - INFO - __main__ -   Gradient accumulation steps = 1\n",
      "04/25/2024 02:06:30 - INFO - __main__ -   Total train batch size (w. parallel & distributed) = 24\n",
      "04/25/2024 02:06:30 - INFO - __main__ -   Total optimization steps = 10000\n",
      "Train steps ... :   0%|          | 0/10000 [03:15<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {total_train_steps * train_batch_size * gradient_accumulation_steps}\")\n",
    "logger.info(\"  Instantaneous batch size per device =\" f\" {training_args.per_device_train_batch_size}\")\n",
    "logger.info(\"  Gradient accumulation steps =\" f\" {gradient_accumulation_steps}\")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel & distributed) = {train_batch_size * gradient_accumulation_steps}\"\n",
    ")\n",
    "logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
    "\n",
    "# ======================== Training ================================\n",
    "train_time = 0\n",
    "train_start = time.time()\n",
    "steps_trained_progress_bar = tqdm(\n",
    "    range(total_train_steps), desc=\"Train steps ... \", position=0, disable=not accelerator.is_local_main_process\n",
    ")\n",
    "continue_training = True\n",
    "epochs_trained = 0\n",
    "cur_step = 0\n",
    "\n",
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "\n",
    "if checkpoint is not None:\n",
    "    accelerator.load_state(checkpoint)\n",
    "    # Find num steps and epoch from saved state string pattern\n",
    "    pattern = r\"checkpoint-(\\d+)-epoch-(\\d+)\"\n",
    "    match = re.search(pattern, checkpoint)\n",
    "    cur_step = int(match.group(1))\n",
    "    epochs_trained = int(match.group(2))\n",
    "\n",
    "    logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "    logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "    logger.info(f\"  Continuing training from global step {cur_step}\")\n",
    "\n",
    "    steps_trained_progress_bar.update(cur_step)\n",
    "\n",
    "    for epoch in range(0, epochs_trained):\n",
    "        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(training_args.seed)\n",
    "\n",
    "    if not data_args.streaming and training_args.max_steps < 0:\n",
    "        # we know exactly the number of steps per epoch, so can skip through the required number of batches\n",
    "        resume_step = (cur_step - epochs_trained * steps_per_epoch) * gradient_accumulation_steps\n",
    "    else:\n",
    "        # Currently we don't know how many steps we've taken in the current epoch\n",
    "        # So we just shuffle the dataset one extra time and start from a fresh epoch\n",
    "        # This is \"good enough\" for our purposes but not fully correct\n",
    "        resume_step = None\n",
    "        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(training_args.seed)\n",
    "else:\n",
    "    resume_step = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02eb65-bc78-4408-8c30-3dc561d7d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n\u001b[1;32m---> 24\u001b[0m     \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:2040\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[1;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   2037\u001b[0m     \u001b[38;5;66;03m# `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\u001b[39;00m\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;66;03m# We cannot return the gradient norm because DeepSpeed does it.\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\accelerate\\accelerator.py:2003\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m   2001\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m xm\u001b[38;5;241m.\u001b[39m_fetch_gradients(opt)\n\u001b[0;32m   2002\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(gradients, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_processes)\n\u001b[1;32m-> 2003\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:307\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    304\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    305\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 307\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\disenv\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:229\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[1;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[1;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs_trained, num_epochs):\n",
    "    vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(training_args.seed)\n",
    "    train_dataloader = DataLoader(\n",
    "        vectorized_datasets[\"train\"],\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        num_workers=dataloader_num_workers,\n",
    "        pin_memory=training_args.dataloader_pin_memory,\n",
    "    )\n",
    "    train_dataloader = accelerator.prepare(train_dataloader)\n",
    "    if hasattr(train_dataloader, \"dataset\") and isinstance(train_dataloader.dataset, IterableDataset):\n",
    "        train_dataloader.dataset.set_epoch(epoch)\n",
    "\n",
    "    if resume_step is not None:\n",
    "        # Skip the first N batches in the dataloader when resuming from a checkpoint\n",
    "        train_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "        resume_step = None\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        with accelerator.accumulate(student_model):\n",
    "            loss, train_metric = train_step(batch, temperature=training_args.temperature)\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(student_model.parameters(), training_args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Check if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            steps_trained_progress_bar.update(1)\n",
    "            cur_step += 1\n",
    "\n",
    "            if cur_step % training_args.logging_steps == 0:\n",
    "                steps_trained_progress_bar.write(\n",
    "                    f\"Step... ({cur_step} / {total_train_steps} | Loss:\"\n",
    "                    f\" {train_metric['loss']}, Learning Rate:\"\n",
    "                    f\" {lr_scheduler.get_last_lr()[0]})\"\n",
    "                )\n",
    "                log_metric(\n",
    "                    accelerator,\n",
    "                    metrics=train_metric,\n",
    "                    learning_rate=lr_scheduler.get_last_lr()[0],\n",
    "                    train_time=train_time + time.time() - train_start,\n",
    "                    step=cur_step,\n",
    "                    epoch=epoch,\n",
    "                    prefix=\"train\",\n",
    "                )\n",
    "\n",
    "            # save checkpoint and weights after each save_steps and at the end of training\n",
    "            if (cur_step % training_args.save_steps == 0) or cur_step == total_train_steps:\n",
    "                intermediate_dir = os.path.join(training_args.output_dir, f\"checkpoint-{cur_step}-epoch-{epoch}\")\n",
    "                accelerator.save_state(output_dir=intermediate_dir)\n",
    "                accelerator.wait_for_everyone()\n",
    "                if accelerator.is_main_process:\n",
    "                    rotate_checkpoints(training_args.save_total_limit, output_dir=training_args.output_dir)\n",
    "\n",
    "                    if cur_step == total_train_steps:\n",
    "                        student_model = accelerator.unwrap_model(student_model)\n",
    "                        student_model.save_pretrained(training_args.output_dir)\n",
    "\n",
    "                    if training_args.push_to_hub:\n",
    "                        repo.push_to_hub(\n",
    "                            commit_message=f\"Saving train state of step {cur_step}\",\n",
    "                            blocking=False,\n",
    "                        )\n",
    "\n",
    "            if training_args.do_eval and (cur_step % eval_steps == 0 or cur_step == total_train_steps):\n",
    "                train_time += time.time() - train_start\n",
    "                student_model.eval()\n",
    "                # ======================== Evaluating ==============================\n",
    "                for eval_split in ['eval']:\n",
    "                    eval_metrics = []\n",
    "                    eval_preds = []\n",
    "                    eval_labels = []\n",
    "                    eval_start = time.time()\n",
    "\n",
    "                    validation_dataloader = DataLoader(\n",
    "                        vectorized_datasets[eval_split],\n",
    "                        collate_fn=data_collator,\n",
    "                        batch_size=per_device_eval_batch_size,\n",
    "                        drop_last=False,\n",
    "                        num_workers=dataloader_num_workers,\n",
    "                        pin_memory=training_args.dataloader_pin_memory,\n",
    "                    )\n",
    "                    validation_dataloader = accelerator.prepare(validation_dataloader)\n",
    "\n",
    "                    for batch in tqdm(\n",
    "                        validation_dataloader,\n",
    "                        desc=f\"Evaluating {eval_split}...\",\n",
    "                        position=2,\n",
    "                        disable=not accelerator.is_local_main_process,\n",
    "                    ):\n",
    "                        # Model forward\n",
    "                        eval_metric = eval_step(batch)\n",
    "                        eval_metric = accelerator.gather_for_metrics(eval_metric)\n",
    "                        eval_metrics.append(eval_metric)\n",
    "\n",
    "                        # generation\n",
    "                        if training_args.predict_with_generate:\n",
    "                            generated_ids = generate_step(batch)\n",
    "                            # Gather all predictions and targets\n",
    "                            generated_ids, labels = accelerator.gather_for_metrics(\n",
    "                                (generated_ids, batch[\"labels\"])\n",
    "                            )\n",
    "                            eval_preds.extend(generated_ids)\n",
    "                            eval_labels.extend(labels)\n",
    "\n",
    "                    eval_time = time.time() - eval_start\n",
    "                    # normalize eval metrics\n",
    "                    eval_metrics = {\n",
    "                        key: torch.mean(torch.stack([d[key] for d in eval_metrics])) for key in eval_metrics[0]\n",
    "                    }\n",
    "\n",
    "                    # compute WER metric\n",
    "                    wer_desc = \"\"\n",
    "                    if training_args.predict_with_generate:\n",
    "                        wer_metric, pred_str, label_str, norm_pred_str, norm_label_str = compute_metrics(\n",
    "                            eval_preds, eval_labels\n",
    "                        )\n",
    "                        eval_metrics.update(wer_metric)\n",
    "                        wer_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in wer_metric.items()])\n",
    "                        log_pred(\n",
    "                            accelerator,\n",
    "                            pred_str,\n",
    "                            label_str,\n",
    "                            norm_pred_str,\n",
    "                            norm_label_str,\n",
    "                            step=cur_step,\n",
    "                            prefix=eval_split,\n",
    "                        )\n",
    "\n",
    "                    # Print metrics and update progress bar\n",
    "                    steps_trained_progress_bar.write(\n",
    "                        f\"Eval results for step ({cur_step} / {total_train_steps} | Eval Loss: {eval_metrics['loss']} |\"\n",
    "                        f\" {wer_desc})\"\n",
    "                    )\n",
    "\n",
    "                    log_metric(\n",
    "                        accelerator,\n",
    "                        metrics=eval_metrics,\n",
    "                        train_time=eval_time,\n",
    "                        step=cur_step,\n",
    "                        epoch=epoch,\n",
    "                        prefix=eval_split,\n",
    "                    )\n",
    "\n",
    "                # flush the train metrics\n",
    "                train_start = time.time()\n",
    "\n",
    "            # break condition\n",
    "            if cur_step == total_train_steps:\n",
    "                continue_training = False\n",
    "                break\n",
    "\n",
    "    if not continue_training:\n",
    "        break\n",
    "\n",
    "accelerator.end_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc8967-8758-4c6b-9437-95ab254e1575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.accelerator.Accelerator at 0x1a2ccc40be0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80cfd9-8875-415f-85cf-9a2acbac0bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991385ee-f919-4068-8bb1-3b99fdcdcb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f0bc0-fd48-4d57-bbb8-a0f87e9e9d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28687f92-bf90-42ba-b4d7-6d33519dcdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b32e4-6d68-4d14-89c2-e54d13dc2eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd9d1d-d95d-41f9-9d64-9314c6ca6377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9b8a8-fe9a-4803-8cf3-a220ca589cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320f351-fd9d-4a0b-ab62-fc18ac0b8b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
